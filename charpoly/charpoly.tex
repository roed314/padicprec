\documentclass{sig-alternate-05-2015}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\hyphenation{regarding}

\usepackage{amsmath,amssymb}
\usepackage{amsthmnoproof}
\usepackage{amsrefs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\com}{Com}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lcm}{lcm}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\newcommand{\OK}{\mathcal{O}_K}

\newcommand{\calU}{\mathcal{U}}

\newcommand{\softO}{O\tilde{~}}

\newcommand{\inv}{\text{\rm inv}}
\newcommand{\app}{\text{\rm app}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\CopyrightYear{2016}
\setcopyright{acmcopyright}
\conferenceinfo{ISSAC '16,}{July 19-22, 2016, Waterloo, ON, Canada}
\isbn{978-1-4503-4380-0/16/07}\acmPrice{\$15.00}
\doi{http://dx.doi.org/10.1145/2930889.2930898}

\permission{Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only.}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\title{Characteristic polynomials of p-adic matrices}

\numberofauthors{3}
\author{
\alignauthor Xavier Caruso\\
  \affaddr{Universit\'e Rennes 1}\\
  \affaddr{\textsf{xavier.caruso@normalesup.org}}
\alignauthor David Roe\\
  \affaddr{University of Pittsburg}\\
  \affaddr{\textsf{roed@pitt.edu}}
\alignauthor Tristan Vaccon\\
  \affaddr{Universit\'e de Limoges}\\
  \affaddr{\textsf{tristan.vaccon@unilim.fr}}
}

\maketitle

\begin{abstract}
We analyze the precision of the characteristic polynomial $\chi(A)$ of
an $n \times n$ $p$-adic matrix $A$ using differential precision methods
developed previously.  When $A$ is integral with precision $O(p^k)$,
we give a criterion (checkable in time $\softO(n^\omega)$) for
$\chi(A)$ to have precision exactly $O(p^k)$.  We also give a $\softO(n^3)$
algorithm for determining the optimal precision when the criterion is not
satisfied, and give examples when the precision is larger than $O(p^k)$.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010148.10010149.10010150</concept_id>
<concept_desc>Computing methodologies~Algebraic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\vspace{-1mm}
\ccsdesc[500]{Computing methodologies~Algebraic algorithms}
\printccsdesc

\vspace{-1.5mm}
\keywords{Algorithms, $p$-adic precision, Newton polygon, factorization}

%\vspace{1mm}
% \noindent
% {\bf Categories and Subject Descripto\RS:} \\
%\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
%  Manipulation -- \emph{Algebraic Algorithms}
%
% \vspace{1mm}
% \noindent
% {\bf General Terms:} Algorithms, Theory
%
% \vspace{1mm}
% \noindent
% {\bf Keywords:} $p$-adic precision, linear algebra, ultrametric analysis
%\medskip

\section{Introduction}

\todo{Explain here why computing characteristic polynomial is so important.}

The characteristic polynomial is a fundamental invariant of a matrix: its roots
give the eigenvalues, and the trace and determinant can be extracted from its coefficients.
In fact, the best known division-free algorithm for computing determinants
over arbitrary rings \cite{kartofen:92a} does so using the characteristic polynomial.
Over $p$-adic fields, computing the characteristic polynomial is a key ingredient in
algorithms for counting points of varieties over finite fields. \todo{list some point counting references.}
\todo{Application to characteristic polynomials of elements of p-adic field extensions?}

\todo{Explain also why taking care of precision is important.}

When computing with $p$-adic matrices, the lack of infinite memory implies
that the entries may only be approximated at some finite precision $O(p^m)$.  As a
consequence, in designing algorithms for such matrices one must analyze
not only the running time of the algorithm but also the accuracy of the result.

Let $M \in M_n(\Qp)$ be known at precision $O(p^m)$.
The simplest approach for computing the characteristic polynomial of $M$
is to compute $\det(X-M)$ either using recursive row expansion or various division free
algorithms \cites{\cite{seifullin:02a} \cite{kartofen:92a}}.  There are two issues
with these methods.  First, they are slower than alternatives that allow division,
requiring $O(n!)$, $O(n^4)$ and $\softO(n^{2 + \omega/2})$ operations.  Second,
while the lack of division implies that the result is accurate modulo $p^m$ as
long as $M \in M_n(\Zp)$, they still do not yield the optimal precision.

A faster approach over a field is to compute the Frobenius normal form of $M$,
which is achievable in running time $\softO(n^\omega)$ \cite{storjohann:01a}.
However, the fact that it uses division frequently leads to catastrophic losses of division.
In many examples, no precision remains at the end of the calculation.

Instead 

\todo{Example of $M$ and $M + 1$.}



\subsection*{Previous contributions.} Many ad-hoc for Kedlaya-like point counting algorithms.  Our previous papers.  What else?

\subsection*{The contribution of this paper.} % Or \textbf

\subsection*{Organization of the article.}

\subsection*{Notation} Throughout the paper, $K$ will refer to a complete,
discrete valuation field, $\val : K \twoheadrightarrow \Z \cup \{+\infty\}$ to its valuation,
$\OK$ its ring of integers and $\pi$ a uniformizer. \todo{Soft-Oh}

\section{Theoretical study}

\subsection{The theory of p-adic precision}

We recall some of the definitions and results of \cite{caruso-roe-vaccon:14a}
as a foundation for our discussion of the precision for the characteristic polynomial
of a matrix.  We will be concerned with two $K$-manifolds in what follows:
the space $M_n(K)$ of $n \times n$ matrices with entries in $K$ and the space
$K_n[X]$ of monic degree $n$ polynomials over $K$.  Given a matrix $M \in M_n(K)$,
the most general kind of precision structure we may attach to $M$ is a
\emph{lattice} $H$ in the tangent space at $M$.  However, representing an
arbitrary lattice requires $n^2$ basis vectors, each with $n^2$ entries.  We therefore
frequently work with certain classes of lattices, either \emph{jagged} lattices
where we specify a precision for each matrix entry or \emph{flat} lattices where
every entry is known to a fixed precision $O(p^m)$.  Similarly, precision for
monic polynomials can be specified by giving a lattice in the tangent space
at $f(X) \in K_n[X]$, or restricted to jagged or flat precision in the interest
of simplicity.

Let $\chi : M_n(K) \to K_n[X]$ be the characteristic polynomial map.
Our analysis of the precision behavior of $\chi$ rests upon
the computation of its derivative $d\chi$, using \cite{caruso-roe-vaccon:14a}*{Lem. 3.4}.
For a matrix $M \in M_n(K)$, we identify the tangent space $V$
at $M$ with $M_n(K)$ itself, and the tangent space $W$ at $\chi(M)$ with
the space $K_{<n}[X]$ of polynomials of degree less than $n$.
Let $\com(M)$ denote the comatrix of $M$ (when $M \in \GL_n(K)$,
we have $\com(M) = \det(M) M^{-1}$) and $d\chi_M$ the differential at $M$.  Recall
\citelist{\cite{caruso-roe-vaccon:14a}*{Appendix B} \cite{caruso-roe-vaccon:15a}*{\S 3.3}}
that $d\chi_M$ is given by
\begin{equation} \label{eq:dchi}
d\chi_M: dM \mapsto \tr(\com(X-M) \cdot dM).
\end{equation}

\begin{prop}
\label{prop:surjectivity}
For $M \in M_n(K)$, the following conditions are equivalent:
\begin{enumerate}[(i)]
\renewcommand{\itemsep}{0pt}
\item \label{pt:surj} the differential $d\chi_M$ is surjective
\item \label{pt:cyc} the matrix $M$ has a cyclic vector (\emph{i.e.} $M$ is similar
to a companion matrix)
\item \label{pt:1d} the eigenspaces of $M$ over the algebraic
closure $\bar{K}$ of $K$ all have dimension $1$
%\item the Jordan blocks of $M$ all have distinct generalized eigenvalues
\end{enumerate}
\end{prop}

\begin{proof}
The equivalence of \eqref{pt:cyc} and \eqref{pt:1d} is standard; see
\cite{hoffman-kunze:LinearAlgebra}*{\S 7.1} for example.  We now show
that \eqref{pt:surj} and \eqref{pt:cyc} are equivalent.

For any $A \in \GL_n(K)$, the image of $d\chi$ at $M$ will be the same
as the image of $d\chi$ at $AMA^{-1}$, so we may assume that
$M$ is a companion matrix.  For a companion matrix, the bottom row of
$\com(X-M)$ consists of $1, X, X^2, \dots, X^{n-1}$ so $d\chi_M$ is surjective.

To show the inverse, suppose that $M$ has a repeated eigenvalue $\lambda$ over $\bar{K}$.
After conjugating into Jordan normal form over $\bar{K}$, the entries of $\com(X-M)$
will also be block diagonal, and divisible within each block by the product of $(X-\mu)$
for each other eigenvalue $\mu$.  Since $\lambda$ occurs twice as an eigenvalue,
$X - \lambda$ will divide every entry of $\com(X-M)$ and $d\chi_M$ will not be surjective.
\end{proof}

We also have an analogue of Proposition \ref{prop:surjectivity} for integral matrices.

\begin{prop}
\label{prop:intsurj}
For $M \in M_n(\OK)$, the following conditions are equivalent:
\begin{enumerate}[(i)]
\renewcommand{\itemsep}{0pt}
\item \label{pt:surjp} the image of $M_n(\OK)$ under $d\chi_M$ is $\OK[X] \cap K_{<n}[X]$.
\item \label{pt:cycp} the reduction of $M$ modulo $\pi$ has a cyclic vector
\end{enumerate}
\end{prop}
\begin{proof}
The condition \eqref{pt:surjp} is equivalent to the surjectivity of $d\chi_M$ modulo $\pi$.  The equivalence with
\eqref{pt:cycp} follows the same argument as Proposition \ref{prop:surjectivity}, but over $\OK / \pi\OK$.
\end{proof}

Write $B^-_V(r)$ (resp. $B_V(r)$) for the open (resp. closed) ball of radius $r$ in $V$.

\begin{prop} \label{prop:mainlem}
Suppose that $M \in M_n(K)$ satisfies one of the conditions in Proposition \ref{prop:surjectivity}, and
let $\alpha = \max(\lvert \det(M)^{-1} \rvert, 1)$.  \todo{What if $\det(M) = 0$?}. Then, for all $\rho \in (0, 1]$ and all
$r \in (0, \alpha \cdot \rho^{-1})$, any lattice $H$ such that $B_V^-(\rho r) \subset H \subset B_V(r)$
satisfies:
\begin{equation}
\chi(M + H) = \chi(M) + d\chi_M(H).
\end{equation}
\end{prop}

\begin{proof}

Recall \cite{caruso-roe-vaccon:15a}*{Def. 3.3} that the \emph{precision polygon}
of $M$ is the lower convex hull of the Newton polygons of the entries of $\com(X-M)$.
Since the precision polygon lies below the Newton polygon of $M$ \cite{caruso-roe-vaccon:15a}*{Prop. 3.4},
and since the smallest entry of the Newton polygon occurs either with the leading
coefficient or the constant term, $B_W(1) \subset d\chi(M)(B_V(\alpha))$.

Since the coefficients of $\chi(M)$ are given by polynomials in the entries of $M$
with integral coefficients, \cite{caruso-roe-vaccon:15a}*{Prop. 2.2} implies
the conclusion.
\end{proof}

The relationship between precision and the images of lattices under $d\chi_M$ allows us to
apply Proposition \ref{prop:intsurj} to determine when the precision of the characteristic polynomial
is the minimum possible.

\begin{cor}
Suppose that $M \in \GL_n(\OK)$ is known with precision $O(p^m)$.
Then the characteristic polynomial of $M$ has precision larger than $O(p^m)$
if and only if the reduction of $M$ modulo $\pi$ does not have a cyclic vector.
\end{cor}

Note that this criterion is checkable using $\softO(n^\omega)$ operations in the residue field,
the running time of matrix multiplication.

\todo{Image modulo $p$ and elementary divisors. 
Consequences: (1)~computation in $\softO(n^\omega)$ and (2)~gain criterium.}  

\subsection{Stability under multiplication by $X$}

By definition, the codomain of $d \chi_M$ is $K_{< n}[X]$. 
However, when $M$ is given, $K_{< n}[X]$ is canonically isomorphic
to $K[X]/\chi_M(X)$ as a $K$-vector space. For our purpose, it will 
often be convenient to view $d \chi_M$ as an $K$-linear mapping
$M_n(K) \to K[X]/\chi_M(X)$.

\begin{prop}
Let $A$ be the subring of $K[X]$ consisting of polynomials $P$ for
which $P(M) \in M_n(\OK)$, and $V = d \chi_M \big(M_n(\OK)\big)$
as a submodule of $K[X]/\chi_M(X)$.  Then $V$ is stable under 
multiplication by $A$.
\end{prop}
\begin{proof}
Let $C = \com(X-M)$ and $P \in A$.  By \eqref{eq:dchi}, $V$ is given by
the $\OK$-span of the entries of $C$.   Using the fact that the product of matrix
with its comatrix is the determinant, $(X - M) \cdot C = \chi(M)$ and thus
$P(X) \cdot C \equiv P(M) \cdot C \pmod{\chi_M(X)}$.  The span of the entries
of the left hand side is precisely $P(X) \cdot V$, while the span of the entries
of the right hand side is contained within $V$ since $P(M) \in M_n(\OK)$.
\end{proof}

\begin{cor}
If $M \in M_n(\OK)$, then $d \chi_M \big(M_n(\OK)\big)$ 
is stable under multiplication by $X$ and hence is a module over $\OK[X]$.
\end{cor}

We can also give a criterion on $M$ that allows an easy description of the image of flat
precision lattices under $d\chi(M)$.
\begin{prop}
Suppose that $M \in M_n(\OK)$, and that the reduction $\bar{M}$ of $M$ modulo $\pi$ has
a cyclic vector.  Then $d\chi_M\big(M_n(\OK)\big) = \OK[X] / \chi(M)$.
\end{prop}
\begin{proof}
It suffices to check that $d\chi_M$ is surjective modulo $\pi$.
Surjectivity of differential
can be seen: write matrix in Jordan normal form.  Comatrix(X - M) surjective iff when you write it
in Frobenius normal form it has only one block.
\end{proof}


\begin{comment}
\begin{prop} \label{prop:factor_com}
Let $M \in M_n (O_K)$ be such that $discr(\chi (M)) \neq 0.$
We assume that $A=\com (X-M)$ is such that $gcd(A_{1,1}, \chi (M))=1$
(in $K[X]$).
Then for $U=col(A,1)$ and $V=A_{1,1}^{-1} row(A,1) \mod \chi (M),$
both in $O_K[X]^n,$ we have: 
\[ \com (X-M)= U \cdot {}^t V \mod \chi (M).\]
\todo{I think that the correct condition is $d \chi_M$ surjective (which
is a bit weaker).}
\end{prop}


\begin{proof}
We first prove that for all $i,j \in \llbracket 1, n \rrbracket,$ we have
\begin{equation}A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod \chi (M). \label{eqn:2-minors-com} \end{equation}
Let $\lambda$ be a root of $\chi (M)$ in an algebraic closure of $K.$
Then $A(\lambda)$ is of rank one.
Indeed, $A(\lambda) \times (\lambda-M) =0.$
Since $\chi (M)$ is square-free, then $\lambda-M$ is of rank $n-1$
and then $A(\lambda)$ is of rank at most one.
Nevertheless, we can remark that
 $gcd(A_{1,1}, \chi (M))=1,$ $A_{1,1} \neq 0 \mod (X-\lambda)$
and thus, $A(\lambda)$ is of rank exactly one.
As a consequence, all $2 \times 2$-minors of $A(\lambda)$ are zero.
It means that \[A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod (X-\lambda). \]
By the Chinese Remainder Theorem, our first result is then proved.

Now, let us take $U=col(A,1)$ the first column of $A$ and
$V=A_{1,1}^{-1} row(A,1) \mod \chi (M)$ (as $A_{1,1}$ is
invertible $\mod \chi (M)).$
Then thanks to \eqref{eqn:2-minors-com}, $U$ and $V$ satisfy the desired equality. 
\end{proof}

\begin{rem}
If $M \in M_n (O_K)$ is such that for some $U,V \in O_{K}[X]^n$
we have $\com (X-M)= U \cdot {}^t V \mod \chi (M)$ and if 
$P\in GL_n (O_K)$ then
\[\com (X-PMP^{-1})=(PU) \cdot ( {}^tV P^{-1}) \mod \chi(M).\]
In other words, factorizability of $\com(X-M)$ is stable
by change of basis.
\end{rem}
\end{comment}

\subsection{Compact form of $d \chi_M$}

Let $C$ be the companion matrix associated to $\chi_M$:
\begin{equation}
\label{eq:companion}
N = \left( \begin{matrix}
0 & 1 & 0 & \cdots & 0 \\
\vdots & \ddots & 1 & \ddots & \vdots \\
\vdots & & \ddots & \ddots & 0 \\
0 & \cdots & \cdots & 0 & 1 \\
-a_0 & -a_1 & \cdots & \cdots & -a_{n-1}
\end{matrix} \right)
\end{equation}
with $\chi_M = a_0 + a_1 X + \cdots + a_{n-1} X^{n-1} + X^n$.
By Proposition~\ref{prop:surjectivity}, there exists a matrix 
$P \in \GL_n(K)$ such that $M = P N P^{-1}$. Applying the same
result to the transpose of $M$, we find that there exists another
invertible matrix $Q \in \GL_n(K)$ such that $M^t = Q N Q^{-1}$.

\begin{prop}
\label{prop:shortcom}
We keep the previous notations and assumptions.
Let $V$ be the row vector $(1, X, \ldots, X^{n-1})$. Then
\begin{equation}
\label{eq:shortcom}
\com(X{-}M) = \alpha \cdot P V^t \cdot V Q^t
\mod \chi_M
\end{equation}
for some $\alpha \in K[X]$.
\end{prop}

\begin{proof}
Write $C = \com(X{-}M)$. From $(X{-}M) \cdot C \equiv 0 
\pmod{\chi_M}$, we deduce $(X{-}N) \cdot P^{-1} C \equiv 0 \pmod{\chi_M}$. 
Therefore each column of $P^{-1} C$ lies in the right kernel of $X{-}N$
modulo $\chi_M$. On the other hand, a direct computation shows that
every column vector $W$ lying in the right kernel of $X{-}N$ modulo 
$\chi_M$ can be written as $W = w \cdot V^t$ for some $w \in 
K[X]/\chi_M$. We deduce that $C \equiv P \cdot V^t B \pmod{\chi_M}$
for some row vector $B$.
Applying the same reasoning with $M^t$, we find that $B$ can be
written $B = \alpha V Q^t$ for some $\alpha \in K[X]/\chi_M$ and
we are done.
\end{proof}

Proposition~\ref{prop:shortcom} shows that $\com(X{-}M)$ can be encoded 
by the datum of the quadruple $(\alpha, P, Q, \chi_M)$ whose total size 
stays within $O(n^2)$ : the polynomials $\alpha$ and $\chi_M$ are 
determined by $2n$ coefficients while we need $2n^2$ coefficients to 
write down the matrices $P$ and $Q$. 
We shall see moreover in \S \ref{sec:eigenvalues} that interesting
informations can be read off on this short form $(\alpha, P, Q, 
\chi_M)$.

\section{Differential\\via Hessenberg form}
\label{sec:diffHess}

In this section, we combine the computation of an Hessenberg form
of a matrix and the computation of the inverse through the SNF
over a complete discrete valuation field
to compute $\com (X-M)$ and $d \chi$
in a way such that if $M \in M_n(O_K),$
only division by invertible elements of $O_K$
occur.
%The computation of an Hessenberg form provides an $O(n^3)$
%algorithm to compute the characteristic polynomial.

\subsection{Hessenberg form}

We begin with the study of the computation of
an approximate Hessenberg form.

\begin{deftn}
$M \in M_n (k)$ is said to be an Hessenberg matrix if
\[\forall i \in \llbracket 3, n \rrbracket, 
\forall j \in \llbracket 1, i-2 \rrbracket, M_{i,j}=0. \]
If $M \in M_n (k)$ and $H \in M_n (k)$ is an Hessenberg 
matrix such that there exists $P \in GL_n(k)$
with $H=PMP^{-1},$ we say that H is an Hessenberg form
of $M.$
\end{deftn}

\begin{deftn}
In the previous definition of an Hessenberg matrix $M,$
 if we only have $M_{i,j}= O(\pi^{n_{i,j}}),$ then we say
 that $M$ is an approximate Hessenberg matrix.
 This generalize directly to approximate Hessenberg form. 
\end{deftn}

It is not hard to prove that every matrix over a field admits
an Hessenberg form.
We prove here that over $K,$ if a matrix is 
known at finite (jagged) precision,
we can compute an approximate Hessenberg form of it.
Moreover, we can provide an exact change of basis matrix.
It relies on the following algorithm.


\noindent\hrulefill

\noindent {\bf Algorithm 1:} {\tt Approximate Hessenberg form computation}

\noindent{\bf Input:} a matrix $M$ in $M_n(K).$

\smallskip

\noindent 0.\ $P:=I_n.$ \: $H:=M.$


\noindent 1.\ {\bf for} $j=1,\dots,n-1$ {\bf do} 

\noindent 2.\  \:  {\bf swap} the row $j+1$ with a row $i_{min}$ ($i_{min} \geq 2$) s.t. $val(H_{i_{min},j})$ is minimal. 

\noindent 3.\  \:  {\bf for} $i=j+2,\dots,n$ {\bf do} 

\noindent 4. \ \: \:  \textbf{Eliminate} the significant digits of $H_{i,j}$ by pivoting with row $j+1$ 
using a matrix $T.$

\noindent 5. \ \: \:  $H:=H \times T^{-1}.$ \: $P:=T \times P.$

\noindent 6. \textbf{Return} $H,P.$

\vspace{-1ex}\noindent\hrulefill

\medskip



\begin{prop} 
Algorithm 1 computes $H$ and $P$ realizing an approximate Hessenberg form of $M.$
$P$ is exact over finite extensions of $\mathbb{Q}_p$ and $k((X))$, and the computation is in $O(n^3)$ operations in $K$ at precision the maximum precision of a coefficent in $M.$
\end{prop}
\begin{proof}
Let us assume that $K$ is a finite extensions of $\mathbb{Q}_p$ or $k((X)).$
Inside the nested \textbf{for} loop, if we want to eliminate $p^{u_y} \varepsilon_y+O(p^{n_y})$ with pivot $p^{u_x} \varepsilon_x+O(p^{n_x}),$
with the $\varepsilon$'s being units,
the corresponding coefficient of the corresponding shear matrix is the lift(in $\mathbb{Z}, $  $\mathbb{F}_q[X],$ $\mathbb{Q}[X]$ or adequate extension) of $p^{u_y-u_x} \varepsilon_y \varepsilon_x^{-1} \mod \pi^{\min (n_x,n_y)}.$
Exactness follows directly. Over other fields, we can not lift, but the computations are still valid.
The rest is clear.
\end{proof}

\begin{rem} \label{rem:char_pol_from_hessenberg}
From an Hessenberg form of $M,$ it is well known
that one can compute the characteristic polynomial of 
$M$ in $O(n^3)$ operations in $K.$ See \cite{Cohen:2013} page
55 and 56.
However, this computation involves some divisions, and its
behaviour regarding to precision is not easy to
quantify.
\end{rem}


\subsection{Computation of the inverse}

In this Subsection, we prove that to compute the inverse of
a matrix over a CDVF $K$, SNF is precision-wise optimal in the flat-precision case.
We first recall what is the differential of the computation of the inverse of a matrix.

\begin{lem}
Let $u \: : \: GL_n (K) \rightarrow GL_n(K),$ $M \mapsto M^{-1}.$
Then for $M \in GL_n (K),$ $du(M) \cdot dM=M^{-1} dM M^{-1}.$
It is always surjective.
\end{lem}

We then have the following result about the loss in precision when computing the inverse.

\begin{prop}
Let $cond(M)$ be the valuation of the last invariant factor of $M.$
If $dM$ is a flat precision of $O(\pi^N)$ on $M$ then $M^{-1}$
can be computed at precision $O(\pi^{N-2cond(M)})$ by a \textbf{SNF} computation
and this lower-bound is optimal
at least when $N$ is large.
\end{prop}
\begin{proof}
The smallest valuation of a coefficient of $M^{-1}$ is $-cond(M).$
It is $-2cond(M)$ for $M^{-2}$ and it is then clear that $-2cond(M)$
can be obtained as the valuation of a coefficient of $du(M) \cdot dM$
and the smallest that can be achieved this way for $dM$ in a precision lattice
of flat precision. Hence the optimality of the bound given, at least when 
$N$ is large, thanks to Lemma 3.14 of \cite{caruso-roe-vaccon:14a}

Now, the computation of an SNF form over a CDVF was described in \cite{Vaccon-these}.
From $M$ known at flat precision $O(\pi^N),$ we can obtain an exact $\Delta$ and $P,Q$ 
known at precision at least $O(\pi^{N-cond(M)})$ with coefficients in $O_K$
and determinant in $O_K^*$ realizing an SNF form of $M.$
There is no loss in precision when computing $P^{-1}$ and $Q^{-1}.$
Now, computing $\Delta^{-1}$ exactly, its smallest coefficient being  of valuation $-cond(M),$
we obtain $M^{-1}=Q^{-1} \Delta^{-1} P^{-1}$ known at precision at least $O(\pi^{N-2cond(M)}),$
which concludes the proof.
\end{proof}



\subsection{The comatrix of $X{-}H$}

In this subsection, we want to compute $\com (X Id -H)$
by using the SNF computation of the previous subsection.
As it is, in a way, optimal for the computation of the
inverse in a CDVF, we hope to get reasonnable behaviour
for the computation of $\com (X-M)$ even though the coefficients
are not, \textit{a priori}, set to live in a CDVF.
We show that we can reduce this task to the computation
of an SNF over a CDVF, with no division in $K.$

Firstly, we need a lemma to relate comatrices of
similar matrices:

\begin{lem} \label{lem:comatrix_of_similar}
If $M_1,M_2 \in M_n(K)$ and $P \in GL_n (K)$ are such that
$M_1=PM_2P^{-1},$ then:
\[ \com (X-M_1)=P \com (X-M_2) P^{-1}. \] 
\end{lem}

The second ingredient we need is reciprocal polynomials.
We extend its definition to matrices of polynomials.
\begin{deftn}
Let $d \in \mathbb{N}$ and $P \in K[X]$ of degree at most $d.$ 
We define the reciprocal polynomial of order $f$ of $P$ as $P^{rec,d}=X^d P \left( 1/X \right).$
Let $A \in M_n(K[X])$ a matrix of polynomials of degree at most $d.$
We denote by $A^{rec,d}$ the matrix
whose coefficients are the reciprocal polynomials of order $d$ 
of the coefficients of $A.$ 
\end{deftn}
We then have the following result :
\begin{lem}
Let $M \in M_n(K).$ Then:
\begin{eqnarray*}
com(1-XM)^{rec,n-1}=&com(X-M), \\
(\chi_M \times 1)^{rec,n}=&(1-XM) com(1-XM).\\
\end{eqnarray*}
\end{lem}
\begin{proof}
It all comes down to the following result:
let $A \in M_d(K[X])$ a matrix of polynomials of degree at most $1,$
then $det (A^{rec,1})=det(A)^{rec,d}.$
Indeed, one can use multilinearity of the determinant on $X^d det(A(1/X))$
to prove this result.
It directly implies the second part of the lemma.
As for the first part, since coefficients of the comatrix we are interested in
are determinants of a matrix of polynomials of degree at most 1, the result
is also clear.
\end{proof}

This lemma allows us to compute $com(1-XM)$ instead of $com(X-M).$
This has a remarkable advantage: the pivots during the computation of
the SNF of $com(1-XM)$ are units of $O_K[[X]],$ and are known
in advance to be on the diagonal. This leads to a very smooth
precision and complexity behaviour when the entry matrix lives in 
$M_n(O_K).$ 

\noindent\hrulefill

\noindent {\bf Algorithm 2:} {\tt Approximate $\com (X -H)$ }

\noindent{\bf Input:} an approximate Hessenberg matrix $H$ in $M_n(O_K).$

\smallskip

\noindent 0.\ $U:=1-XH.$ $U_0:=1-XH.$

\noindent 1.\ \textbf{Track} the following operations to get $P^{-1}$ and $Q^{-1},$
such that $U=PU_0Q.$


\noindent 2.\ {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 3.\  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,j},$ for $j\geq i+1$ 
using the invertible pivot
$U_{i,i}=1+XL_{i,i} \mod X^{n+1}$ (with $L_{i,i} \in \Zp [X]$). 

\noindent 4.\    {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 5. \  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,i+1},$
using the invertible pivot $U_{i,i}.$

\noindent 6. \ $\psi:=\prod_i U_{i,i}.$

\noindent 7. \ Rescale to get $U = Id \mod X^{n+1}.$

\noindent 8. \ $V:=\psi \times P^{-1} \times Q^{-1}   \mod X^{n+1}.$

\noindent 9. \ \textbf{Return} $V^{rec,n-1}, \psi^{rec,n}.$

\vspace{-1ex}\noindent\hrulefill

\medskip

\begin{theo}
Let $H \in M_n(O_K)$ be an approximate Hessenberg matrix.
Then, using Algorithm 2, one can compute $\com (X -H)$ in 
$\softO (n^3)$ operations in $O_K$ at the precision given by $H.$
\end{theo}
\begin{proof}
Firstly, the operations of the lines 2 and 3 are in $\softO (n^3)$ operations
in $O_K$ at the precision given by $H.$
Indeed, since $H$ is an approximate matrix, when we use $U_{i,i}$ as pivot:
on its columns, only $U_{i+1,i}$ can be a non zero-approximate coefficient.
As a consequence, when performing this column-pivoting, only two rows ($i$ and
$i+1$) lead to operations in $O_K [[K]]$ other than checking precision.
Hence, line 3 costs $\softO (n^2)$ for the computation of $U.$
Following line 1, the computation of $Q^{-1}$ is done by operations on rows, starting from the identity matrix.
Corresponding coefficient for the corresponding shear matrix
 is the opposite of the one defined by pivoting
and the indices for the receiving and giving rows are swapped compared to the operations on columns.
Thus, we can show that it requires no additional operation in $O_K[[X]].$ $Q^{-1}$ is just filled as
an upper-triangular matrix.
This proves that the resulting cost for lines 2 and 3 is indeed in $\softO (n^3)$ operations.

As for line 4 and 5, there are only $n-1$ eliminations, resulting again in a $\softO (n^3)$ cost
for the computation of $P^{-1}$ and $U.$

Line 6 is in $\softO (n^2)$ and 7 in $\softO (n^3).$

Thanks to the fact that the $P$ only corresponds to the product of $n-1$ 
shear matrices, the
product on line 8 is in $\softO (n^3).$
We emphasize that no division has been done throughout the algorithm.
Line 9 is costless, and the result is then proved.
\end{proof}


\subsection{The comatrix of $X{-}M$}



\noindent\hrulefill

\noindent {\bf Algorithm 3:} {\tt Approximate $\com (X{-}M)$ }

\noindent{\bf Input:} an approx. $M \in M_n(O_K),$ with $discr(\chi_M) \neq 0.$ 

\smallskip

\noindent 0.\ Find $P \in GL_n(O_K)$ and $H \in M_n(O_K),$ approximate Hessenberg,
such that $M=PHP^{-1},$ using Algorithm 1. 

\noindent 1.\ Compute $A=\com (X-H)$ using Algorithm 2.


\noindent 2.\ Do $row(A,1) \leftarrow row(A,1)+\sum_{i=2}^n \mu_i row(A,i),$ for
random $\mu_i \in O_K,$ by doing $T \times A$ for some $T \in GL_n(O_K).$
Compute $B:=TAT^{-1}.$

\noindent 3.\ Similarily compute $C:=S^{-1}BS$ for $S \in GL_n(O_K)$ corresponding to
sending a random linear combination of the columns of index $j \in \llbracket 2,n \rrbracket$
to the first column of $B.$ 

\noindent 4.\  \textbf{If} $gcd(C_{1,1}, \chi(M)) \neq 1,$ \textbf{then} go to 2.

\noindent 5. \ Compute $U,V \in O_K[X]^n$ such that $C=U {}^t V \mod \chi (M),$
by inverting $C_{1,1} \mod \chi(M).$

\noindent 6. \ Return $com(X-M):=(PT^{-1}S U \times V S^{-1} T P^{-1}) \mod \chi (M).$

\vspace{-1ex}\noindent\hrulefill

\medskip


\begin{theo}
Algorithm 3 compute 
$\com (X-M) \mod (\chi(M))$ in average complexity $\softO (n^3)$
operations in $O_K$ at initial precision.
\end{theo}
\begin{proof}
As we have already seen, completing Step 1 and 2 is in $\softO (n^3).$
Multiplying by $T$ or $S$ or their inverse corresponds
to $n$ operations on rows or columns over a matrix with coefficients
in $O_K[X]$ of degree at most $n.$
Thus, it is in $\softO (n^3).$
Step 5 and 6 are also in $\softO (n^3).$
All that is to prove is that the set of $P$ and $S$ to avoid
is of dimension at most $n-1.$
As in the proof of \ref{prop:factor_com}, we can work modulo $X-\lambda$
for $\lambda$ a root of $\chi (M)$ (in an algebraic closure)
and then apply Chinese Remainder Theorem.
The goal of the Step $2$ is to ensure the first row of $B$ contains an
invertible modulo $\chi (M).$
Since $A(\lambda)$ is of rank one, the $\mu_i$'s have to avoid an
affine hyperplane so that $row(B,1) \mod (X-\lambda)$ is a non-zero vector.
Hence for $row(B,1) \mod \chi (M)$ to contain an invertible coefficient,
a finite union of affine hyperplane is to avoid.
Similarily, the goal of Step 3 is to put an invertible coefficient (modulo
$\chi(M)$) on $C_{1,1},$ and again, only a finite union of affine
hyperplane is to avoid.
Hence, the set that the $\mu_i$'s have to avoid is a finite union
of hyperplane, and hence, is of dimension at most $n-1.$
Thus, almost any $\mu_i$ lead to a matrix $C$ passing the test
on Step 4, which concludes the proof.
\end{proof}

\section{Differential\\via Frobenius form}
\label{sec:diffFrob}

The algorithm designed in the previous section computes the differential 
$d \chi_M$ of $\chi$ at a given matrix $M \in M_n(K)$ for a cost of 
$O(n^3)$ operations in $K$. This seems to be optimal given that the 
(naive) size of the $d \chi_M$ is $n^3$: it is a matrix of size $n
\times n^2$. It turns out however that improvements are still possible!
Indeed, thanks to Proposition~\ref{prop:shortcom}, the matrix of 
$d \chi_M$ admits a compact form which can be encoded using only $O(n^2)$ 
coefficients. The aim of this short section is to design a fast 
algorithm (with complexity $\softO(n^\omega)$) for computing this short 
form. The price to pay is that divisions in $K$ appear, which can be an 
issue regarding to precision in particular cases.

From now on, we fix a matrix $M \in M_n(K)$ for which $d \chi_M$ is 
surjective. Let $(\alpha, P, Q, \chi_M)$ be the quadruaple encoding
the short form of $d \chi_M$; we recall that they are related by the
relations:
\begin{align*}
d \chi_M(dM) & =\tr(\com(X{-}M) \cdot dM) \\
\com(X{-}M) & = \alpha \cdot P V^t \cdot V Q^t \mod \chi_M.
\end{align*}
The matrix $P$ can be computed as follows. Pick $c \in K^n$. Define 
$c_i = M^i c$ for all $i \geq 1$. The $c_i$'s can be computed in 
$\softO(n^\omega)$ operations in $K$ using \cite{}. Let $P_\inv$ be the 
$n \times n$ matrix whose rows are the $c_i$'s for $1 \leq i \leq n$. 
Remark that $P_\inv$ is invertible if and only if $(c_0, c_1, \ldots, 
c_{n-1})$ is a basis of $K^n$ if and only if $c$ is a cyclic vector. 
Moreover after base change to the basis $(c_0, \ldots, c_{n-1})$, the matrix 
$M$ takes the shape \eqref{eq:companion}. In other words, if $P_\inv$
is invertible, then $P = P_\inv^{-1}$ is a solution of $M = P N P^{-1}$.
Observe moreover that the condition ``$P_\inv$ is invertible'' is open
for the Zariski topology. It then happens with high probability as soon
as it is not empty, that is as soon as $M$ admits a cyclic vector, which
holds by assumption.

The characteristic polynomial $\chi_M$ can be recovered thanks to the
relation $(a_0, a_1, \ldots, a_{n-1}) = -c_n \cdot P$.

Now, instead of computing directly $Q$, we first compute a matrix $R$ 
with the property that $N^t = R N R^{-1}$. For that, we apply the same
strategy as above except that we start with the vector $e = (1, 0, 
\ldots, 0)$ (and not with a random vector). A simple computation shows
that, for $1 \leq i \leq n{-}1$, the vector $N^i e$ has the shape:
$$N^i e = (0, \ldots, 0, -a_0, \star, \ldots, \star)$$
with $n{-}i$ starting zeros. Therefore the $N^i e$'s form a basis of
$K^n$, \emph{i.e.} $e$ is always a cyclic vector of $N$. Once $R$ has
been computed, we recover $Q$ using the relation $Q = P_\inv^t R$.

It remains to compute the scaling factor $\alpha$. For this, we write
the relation:
\begin{equation}
\label{eq:shortcomN}
\com(X{-}N) = \alpha \cdot V^t \cdot V R^t \mod \chi_M
\end{equation}
which comes from Eq.~\eqref{eq:shortcom} after multiplication on the 
left by $P^{-1}$ and multiplication on the right by $P$. We observe
moreover that the first row of $R$ is $(1, 0, \ldots, 0)$. Evaluating
the top left entry of Eq.~\eqref{eq:shortcomN}, we end up with the 
relation:
$$\alpha = a_1 + a_2 X + \cdots + a_{n-1} X^{n-2} + X^{n-1}.$$
No further computation are then needed to derive the value of $\alpha$.

\section{Optimal jagged precision}
\label{sec:optjagged}

\todo{Write a small introduction}

\subsection{On the characteristic polynomial}

For $0 \leq k < n$, let $\pi_k : K[X] \to K$ be the mapping taking a 
polynomial to its coefficients in $X^k$. By applying 
\cite{caruso-roe-vaccon:14a}*{Lem. 3.4} to the composite $\pi_k 
\circ \chi_M$, one can figure out the optimal precision on the
$k$-th coefficient of the characteristic polynomial of $M$ (at
least if $M$ is given at enough precision).

Let us consider more precisely the case where $M$ is given at 
jagged precision: the $(i,j)$ entry of $M$ is given at precision 
$O(\pi^{N_{i,j}})$ for some integers $N_{i,j}$. 
Lemma 3.4 of \cite{caruso-roe-vaccon:14a} then shows that
the optimal precision on the $k$-th coefficient of $\chi_M$ is 
$O(\pi^{N'_k})$ where $N'_k$ is given by the formula:
\begin{equation}
\label{eq:optjagged}
N'_k = \min_{1 \leq i, j\leq n} N_{j,i} + \val(\pi_k(C_{i,j})),
\end{equation}
where $C_{i,j}$ is the $(i,j)$ entry of the comatrix $\com(X{-}M)$.

\begin{prop} \label{prop:optimal_jagged}
If $M \in M_n(O_K)$ is given at (high enough) jagged precision, 
then we can compute the optimal jagged precision on $\chi_M$ in 
$\softO (n^3)$ operations in $K$.
\end{prop}

\begin{proof}
We have seen in \S \ref{sec:diffHess} and \S \ref{sec:diffFrob}
that the computation of the matrix $C = \com(X{-}M)$ can be carried out 
within $\softO(n^3)$ operations in $K$ (either with the Hessenberg 
method or the Frobenius method). We conclude by applying 
Eq.~\eqref{eq:optjagged} which requires no further operation in $K$
(but $n^3$ evaluations of valuations and $n^3$ manipulations of 
integers).
\end{proof}

\begin{rem}
If $M \in M_n(O_K)$ is given at (high enough) \emph{flat} precision, 
then we can avoid the final base change step in the Hessenberg method.
Indeed, observe that, thanks to Lemma \ref{lem:comatrix_of_similar}, 
we can write:
$$\tr(\com (X{-}M) \cdot dM)=\tr(\com (X{-}H)\cdot P^{-1} dM P)$$
where $P$ lies in $\GL_n(\OK)$. Moreover, the latter condition implies
that $P^{-1} dM P$ runs over $M_n(\OK)$ when $P$ runs over $M_n(\OK)$.
As a consequence, the integer $N'_k$ giving the optimal precision on the 
$k$-th coefficient of $M$ is also equal to
$N + \min_{1 \leq i, j\leq n} \val(\pi_k(C^H_{i,j}))$
where $C^H_{i,j}$ is the $(i,j)$ entry of $\com(X{-}H)$,
where $H$ is the Hessenberg form of $M$.
\end{rem}

As a consequence of the previous discussion, once the optimal jagged 
precision is known, it is possible to lift arbitrarily the precision on 
$M$ and then to perform a computation of the characteristic polynomial 
at the optimal precision. This requires $\softO(n^3)$ operations in $K$ 
but, for several instances, may require to increase a lot the precision.

\medskip

\noindent
{\bf Numerical experiments.}
We have made numerical experiments in \textsc{SageMath}~\cite{sage}
in order to compare the optimal precision obtained with the methods
explained above with the actual precision obtained by the software.
For doing so, we picked a sample of $1000$ random matrices $M$ in 
$M_9(\Q_2)$ where all the entries are given at the same \emph{relative
precision}.
We recall that, in \textsc{SageMath}, random elements $x \in \Q_p$ are 
generated as follows. We fix an integer $N$ --- the so-called relative
precision --- and generate elements of $\Q_p$ of the shape
$$x = p^v \cdot \big(a + O\big(p^{N+v_p(a)}\big)\big)$$
where $v$ is a random integer generated according to the distribution:
$$\mathbb P [v = 0] = \frac 1 5 \quad ; \quad
\mathbb P [v = n] = \frac 2 {5\cdot |n| \cdot (|n|+1)} \text{ for }
|n| \geq 1$$
and $a$ is an integer in the range $[0, p^N)$, selected uniformly at random.

Once this sample has been generated, we computed, for each $k \in \{0, 
1, \ldots, 8\}$, the three following quantities:

\vspace{-2mm}

\begin{enumerate}[$\bullet$]
\renewcommand{\itemsep}{0pt}
\item the optimal precision on the $k$-th coefficient of the 
characteristic polynomial of $M$ given by Eq.~\eqref{eq:optjagged}
\item in the capped relative model\footnote{Each 
coefficient carries its own precision which is updated after each 
elementary arithmetical operation.},
the precision gotten on the $k$-th coefficient of the 
characteristic polynomial of $M$ computed \emph{via} the call:

\hfill$M\texttt{.charpoly(algorithm="df")}$\hfill\null

\item in the model of floating-point arithmetic (see 
\cite{caruso-coursepadic}*{\S 2.3}), the number of correct digits of the 
$k$-th coefficient of the characteristic polynomial of $M$.
\end{enumerate}

\begin{rem}
The keyword \texttt{algorithm="df"} forces \texttt{SageMath} to use the 
division free algorithm of \cite{}. It is likely that, proceeding so, we 
limit the loss of precision.
\end{rem}

\noindent
The table of Figure~\ref{fig:exp} summarizes the results obtained. 
%
\begin{figure}
\hfill
{\renewcommand*{\arraystretch}{1.3}
\begin{tabular}{|c|r|r|r|}
\cline{2-4} 
\multicolumn{1}{l|}{} 
  & \multicolumn{3}{c|}{Average loss of accuracy} \\
\cline{2-4}
\multicolumn{1}{l|}{\null\hspace{6mm}\null} 
  & \makebox[1.5cm]{\hfill Optimal\hfill\null}
  & \makebox[1.5cm]{\hfill CR\hfill\null}
  & \makebox[1.5cm]{\hfill FP\hfill\null} \\
\hline
\rule{0pt}{2.7ex}%
$X^0$ & $3.17$ & $196\phantom{.00}$ & $189\phantom{.00}$ \vspace{-1.5ex} \\
{\scriptsize (det.)} 
& {\scriptsize dev: $1.76$} & {\scriptsize dev: $240$} & {\scriptsize dev: $226$} \\
\rule{0pt}{2.7ex}%
$X^1$ & $2.98$ & $161\phantom{.00}$ & $156\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.69$} & {\scriptsize dev: $204$} & {\scriptsize dev: $195$} \\
\rule{0pt}{2.7ex}%
$X^2$ & $2.75$ & $129\phantom{.00}$ & $126\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.57$} & {\scriptsize dev: $164$} & {\scriptsize dev: $164$} \\
\rule{0pt}{2.7ex}%
$X^3$ & $2.74$ & $108\phantom{.00}$ & $105\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.73$} & {\scriptsize dev: $144$} & {\scriptsize dev: $143$} \\
\rule{0pt}{2.7ex}%
$X^4$ & $2.57$ &  $63.2\phantom{0}$ &  $60.6\phantom{0}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.70$} & {\scriptsize dev: $85.9$} & {\scriptsize dev: $85.8$} \\
\rule{0pt}{2.7ex}%
$X^5$ & $2.29$ &  $51.6\phantom{0}$ &  $49.7\phantom{0}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.66$} & {\scriptsize dev: $75.3$} & {\scriptsize dev: $74.9$} \\
\rule{0pt}{2.7ex}%
$X^6$ & $2.07$ &   $9.04$           &   $8.59$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.70$} & {\scriptsize dev: $26.9$} & {\scriptsize dev: $26.4$} \\
\rule{0pt}{2.7ex}%
$X^7$ & $1.64$ &   $5.70$           &   $5.38$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.65$} & {\scriptsize dev: $15.3$} & {\scriptsize dev: $14.7$} \\
\rule{0pt}{2.7ex}%
$X^8$ & $0.99$ &   $0.99$           &   $0.99$ \vspace{-1.5ex} \\
{\scriptsize (trace)} 
& {\scriptsize dev: $1.37$} & {\scriptsize dev: $1.37$} & {\scriptsize dev: $1.37$} \\
\hline
\end{tabular}}
\hfill\null

\medskip

\hfill
{\footnotesize Results for a sample of $1000$ instances}
\hfill\null

\caption{Average loss of accuracy on the coefficients
of the characteristic polynomial of a random $9 \times 9$
matrix over $\Q_2$}
\label{fig:exp}

\end{figure}
%
It should be read as follows. First, the acronyms CR and FP 
refers to ``capped relative'' and ``floating-point'' respectively.
The numbers displayed in the table are the average loss of
\emph{relative} precision. 
More precisely, if $N$ is the relative precision at
which the entries of the input random matrix $M$ have been generated
and $v$ is the valuation of the $k$-th coefficient of $\chi_M$, then:

\vspace{-2mm}

\begin{enumerate}[$\bullet$]
\renewcommand{\itemsep}{0pt}
\item the column ``Optimal'' is the average of the quantities 
$(N'_k{-}v) - N$ (where $N'_k$ is defined by Eq.~\eqref{eq:optjagged}): 
$N'_k{-}v$ is the optimal \emph{relative} precision, so that the
difference $(N'_k{-}v) - N$ is the loss of relative precision;
\item the column ``CR'' is the average of the quatities 
$(\text{CR}_k{-}v) - N$ where $\text{CR}_k$ is the computed (absolute) 
precision on the $k$-th coefficient of $\chi_M$;
\item the column ``FP'' is the average of the quatities 
$(\text{FP}_k{-}v) - N$ where $\text{FP}_k$ is the first position of
an incorrect digit on the $k$-th coefficient of $\chi_M$.
\end{enumerate}

We observe that the loss of relative accuracy stays under control in the 
``Optimal'' column whereas it has a very erratic behavior --- very large 
values and very large deviation as well --- in the two other columns. 
These experiments thus demonstrate the utility of the methods developed
in this paper.

\subsection{On eigenvalues}
\label{ssec:eigenvalues}

Let $M \in M_n(K)$ and $\lambda \in K$ be a \emph{simple}
\footnote{the corresponding generalized eigenspace has dimension $1$} eigenvalue 
of $M$. We are interesting in quantifying the optimal precision on 
$\lambda$ when $M$ is given with some uncertainty.

To do so, we fix an approximation $M_\app \in M_n(K)$ of $M$ and 
suppose that the uncertainty of $M$ is ``jagged'' in the sense that
each entry of $M$ is given at some precision $O(\pi^{N_{i,j}})$.
Let $\lambda_\app$ be the relevant eigenvalue of $M_\app$. We remark 
that it is possible to follow the eigenvalue $\lambda_\app$ on a small 
neighborhood $\calU$ of $M$. More precisely, there exists a unique continuous 
function $f : \calU \to K$ such that:
\begin{enumerate}[$\bullet$]
\renewcommand{\itemsep}{0pt}
\item $f(M_\app) = \lambda_\app$, and
\item $f(M')$ is an eigenvalue of $M'$ for all $M' \in \calU$.
\end{enumerate}

\begin{lem}
The function $f$ is strictly differentiable on a neighborhood of 
$M_\app$.
The differential of $f$ at $M$ is the linear mapping:
$$dM \mapsto d \lambda = - \frac{\tr(\com(\lambda-M) \cdot dM)}
{\chi'_M(\lambda)}$$
where $\chi'_M$ is the usual derivative of $\chi_M$.
\end{lem}

\begin{proof}
The first assertion follows from the implicit function Theorem.
Differentiating the relation $\chi_M(\lambda) = 0$, we get
$\chi'_M(\lambda) \cdot d \lambda + \tr(\com(X-M) \cdot dM)(\lambda) = 0$,
from which the Lemma follows.
\end{proof}

\noindent
Lemma 3.4 of \cite{caruso-roe-vaccon:14a} now implies that, if the
$N_{i,j}$'s are large enough and sufficiently well balanced, the optimal
precision on the eigenvalue $\lambda$ is $O(p^{N'})$ with:
$$N' = \min_{1 \leq i, j\leq n} \big(N_{j,i} + \val(C_{i,j}(\lambda)) - 
\val(\chi'_M(\lambda))\big)$$
where $C_{i,j}$ denotes as above the $(i,j)$ entry of $\com(X{-}M)$.
Writing
$\com(X{-}M) = \alpha \cdot P V^t \cdot V Q^t \text{ mod } \chi_M$
as in Proposition~\ref{prop:shortcom}, we find:
\begin{align*}
N' & = \val(\alpha(\lambda)) - \val(\chi'_M(\lambda)) \\
& \hspace{2mm} + \min_{1 \leq i, j\leq n} \big(N_{j,i} + 
\val(P_i V(\lambda)^t) + \val(V(\lambda) Q_j^t)\big)
\end{align*}
where $P_i$ denotes the $i$-th row of $P$ and, similarly, $Q_j$
denotes the $j$-th row of $Q$. Note moreover that $V(\lambda)$ is
the row vector $(1, \lambda, \ldots, \lambda^{n-1})$.
By the discussion of \S \ref{sec:diffFrob}, the exact value of $N'$ can be 
determined for a cost of $\softO(n^\omega)$ operations in $K$ and
$O(n^2)$ operations on integers. 

When $M$ is given at flat precision, \emph{i.e.} the $N_{i,j}$'s are all 
equal to some $N$, the formula for $N'$ may be rewritten:
\begin{align*}
N' & = N + \val(\alpha(\lambda)) - \val(\chi'_M(\lambda)) \\
& \hspace{2mm} + \min_{1 \leq i \leq n} \val(P_i V(\lambda)^t)
+ \min_{1 \leq j \leq n} \val(V(\lambda) Q_j^t)\big)
\end{align*}
and can therefore now be evaluated for a cost of $\softO(n^\omega)$
operations in $K$ and only $O(n)$ operations on integers.

\medskip

To conclude with, let us discuss briefly the situation where we want
to figure out the optimal jagged precision on a tuple $(\lambda_1,
\ldots, \lambda_s)$ of simple eigenvalues. Applying what we have done
previously, we find that the optimal precision on $\lambda_k$ is 
\begin{align*}
N'_k & = \val(\alpha(\lambda)) - \val(\chi'_M(\lambda)) \\
& \hspace{2mm} + \min_{1 \leq i, j\leq n} \big(N_{j,i} + 
\val(P_i V(\lambda)^t) + \val(V(\lambda) Q_j^t)\big).
\end{align*}

\begin{prop}
\label{prop:opteigenvalues}
The $N'_k$'s can be all computed in $\softO(n^\omega)$ operations
in $K$ and $O(n^2 s)$ operations on integers.

If the $N_{i,j}$'s are all equal, the above complexity can be 
lowered to $\softO(n^\omega)$ operations in $K$ and $O(n s)$ operations 
on integers.
\end{prop}

\begin{proof}
The $\alpha(\lambda_k)$'s and the $\chi'_M(\lambda_k)$'s can be computed 
for a cost of $\softO(ns)$ operations in $K$ using fast multipoint 
evaluation methods (see 10.7 of \cite{gathen-gerhard:13a}).
On the other hand, we observe that $P_i V(\lambda_k)^t$ is nothing but
the $(i,k)$ entry of the matrix:
$$P \cdot \left( \begin{matrix}
\lambda_1 & \cdots & \lambda_s \\
\lambda_1^2 & \cdots & \lambda_s^2 \\
\vdots & & \vdots \\
\lambda_1^{n-1} & \cdots & \lambda_s^{n-1}
\end{matrix} \right).$$
The latter product can be computed in $\softO(n^\omega)$ operations in 
$K$\footnote{It turns out that $\softO(n^2)$ is also possible because
the right factor is a structured matrix (a truncated Vandermonde):
computing the above product reduces to evaluate a polynomial at the
points $\lambda_1, \ldots, \lambda_s$.}. Therefore all the $P_i 
V(\lambda_k)^t$'s (for $i$ and $k$ varying) 
can be determined with the same complexity. 
Similarly all the $V(\lambda) Q_j^t$ are computed for the same cost.
The first assertion of Proposition~\ref{prop:opteigenvalues} follows.
The second assertion is now proved similarly to the case of a unique
eigenvalue.
\end{proof}


\bibliographystyle{plain}
\bibliography{charpoly}

\end{document}
