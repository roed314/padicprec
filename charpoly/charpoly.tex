\documentclass[sigconf]{acmart}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\hyphenation{regarding}

\usepackage{amsmath,amssymb}
%\usepackage{amsthmnoproof}
\usepackage{amsthm}
\usepackage{mathrsfs}
%\let\bibsection\relax
%\usepackage{amsrefs}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
%\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
%\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
%\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\adj}{Adj}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\rec}{rec}
\DeclareMathOperator{\cond}{cond}
\DeclareMathOperator{\disc}{Disc}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\Fq}{\mathbb{F}_q}
\newcommand{\R}{\mathbb R}
\newcommand{\OK}{\mathcal{O}_K}

\newcommand{\famN}{\mathcal{N}}

\newcommand{\llb}{[\mkern-2.5mu[}
\newcommand{\rrb}{]\mkern-2.5mu]}
\newcommand{\llp}{(\mkern-2.5mu(}
\newcommand{\rrp}{)\mkern-2.5mu)}

\newcommand{\calU}{\mathcal{U}}

\newcommand{\softO}{O\tilde{~}}

\newcommand{\inv}{\text{\rm inv}}
\newcommand{\app}{\text{\rm app}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\newcommand{\done}[1]{\textcolor{blue}{#1}}
\newcommand{\tdo}[1]{\textcolor{red}{#1}}
\definecolor{answer}{rgb}{0,0.5,0.2}
\newcommand{\xavier}[1]{\textcolor{answer}{{\bf Xavier:} #1}}
\newcommand{\tristan}[1]{\textcolor{answer}{{\bf Tristan:} #1}}
\newcommand{\david}[1]{\textcolor{answer}{{\bf David:} #1}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\clubpenalty=10000
\widowpenalty = 10000

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\begin{document}

\title{Characteristic polynomials of p-adic matrices}

\author{Xavier Caruso}
\affiliation{Universit\'e Rennes 1}
\email{xavier.caruso@normalesup.org}
\author{David Roe}
\affiliation{University of Pittsburg}
\email{roed@pitt.edu}
\author{Tristan Vaccon}
\affiliation{Universit\'e de Limoges}
\email{tristan.vaccon@unilim.fr}

\copyrightyear{2017}
\acmYear{2017}
\setcopyright{acmlicensed}
\acmConference{ISSAC '17}{July 25-28, 2017}{Kaiserslautern, Germany}
\acmPrice{15.00}
\acmDOI{http://dx.doi.org/10.1145/3087604.3087618}
\acmISBN{978-1-4503-5064-8/17/07}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010148.10010149.10010150</concept_id>
<concept_desc>Computing methodologies~Algebraic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Algebraic algorithms}

\keywords{Algorithms, $p$-adic precision, characteristic polynomial,
eigenvalue}

\begin{abstract}
We analyze the precision of the characteristic polynomial $\chi_M$ of 
an $n \times n$ $p$-adic matrix $M$ using differential precision methods 
developed previously.  When $M$ is an integral matrix whose entries are 
all given at the same precision $O(p^N)$, we give a criterion (checkable 
within $\softO(n^\omega)$ operations in $\Fp$) for the existence of a
coefficient of $\chi_M$ with more accuracy than $O(p^N)$.
In general, we provide two algorithms for determining the optimal 
precision of the coefficients of $\chi_M$ and of $M$'s eigenvalues.
We provide evidence showing that classical algorithms do not reach
this optimal precision in general.
\end{abstract}

\maketitle

\section{Introduction}

The characteristic polynomial is a fundamental invariant of a matrix: its roots
give the eigenvalues, and the trace and determinant can be extracted from its coefficients.
In fact, the best known division-free algorithm for computing determinants
over arbitrary rings \cite{kaltofen-villard:04a} does so using the characteristic polynomial.
Over $p$-adic fields, computing the characteristic polynomial is a key ingredient in
algorithms for counting points of varieties over finite fields (see
\cite{kedlaya:01a, harvey:07a, harvey:14a}).

When computing with $p$-adic matrices, entries may only be approximated
at some finite precision $O(p^N)$.  As a
consequence, in designing algorithms for such matrices one must analyze
not only the running time of the algorithm but also the accuracy of the result.

Let $M \in M_n(\Qp)$, with every entry given at precision $O(p^N)$.
The simplest approach for computing the characteristic polynomial of $M$
is to compute $\det(X{-}M)$ either using recursive row expansion or various division free
algorithms \cite{seifullin:02a,kaltofen:92a}.  There are two issues
with these methods.  First, they are slower than alternatives that allow division,
requiring $O((n+1)!)$, $\softO(n^5)$ and $\softO(n^{3 + \omega/2})$ operations in $K$.  Second,
while the lack of division implies that the result is accurate modulo $p^N$ as
long as $M \in M_n(\Zp)$, they still do not yield the optimal precision.

A faster approach over a field is to compute the Frobenius normal form of $M$,
which is achievable in running time $\softO(n^\omega)$ \cite{keller-gehrig:85a,pernet-storjohann:07a}.
However, the use of division frequently leads to catastrophic losses of precision.
In many examples, no precision remains at the end of the calculation.

Instead, we separate the computation of the precision of $\chi_M$ from the computation
of an approximation to $\chi_M$.  Given some precision on $M$, we use the theory developed in \cite{caruso-roe-vaccon:14a}
to find the best possible precision for $\chi_M$.  The analysis of this precision is the subject
of much of this paper.  With this precision known, the actual calculation of $\chi_M$
may proceed by lifting $M$ to a temporarily higher precision and then using a sufficiently
stable algorithm (see Remark \ref{rem:lift_for_optimal}).

\medskip

\noindent
{\bf Previous contributions.}
Since the description of Kedlaya's algorithm in
\cite{kedlaya:01a}, the computation of characteristic polynomials
over $p$-adic numbers
has become a crucial ingredient in many counting-points algorithms.
For example, \cite{harvey:07a, harvey:14a} use $p$-adic cohomology
and the characteristic polynomial of Frobenius to compute zeta functions
of hyperelliptic curves.

In most of these papers, the precision analysis usually
dwells on how to obtain the matrices (\textit{e.g.} of action of Frobenius)
that are involved in the point-counting schemes.
However, the computation of their characteristic polynomials 
attracts less attention: some
refer to fast algorithms (using division), while others
apply division-free algorithms.

In \cite{caruso-roe-vaccon:15a}, the authors have begun the application
of the theory of differential precision of \cite{caruso-roe-vaccon:14a}
to the stable computation of characteristic polynomials.
They describe the optimal precision
for the characteristic polynomial, but do not give practical algorithms
to compute this optimal precision.

\medskip

\noindent
{\bf The contribution of this paper.}
This paper provides a theoretical and concrete study of the propagation 
of the precision during the computation of the characteristic polynomial 
of matrix defined over the $p$-adics. Thanks to the application of the 
general framework of differential precision developed in 
\cite{caruso-roe-vaccon:14a,caruso-roe-vaccon:15a}, we know that the 
optimal precision of the characteristic polynomial $\chi_M$ of a matrix 
$M \in M_n(\mathbb{Q}_p)$ is controled by the adjugate $\adj(X{-}M).$
In this article, we provide:
\begin{enumerate}
\renewcommand{\itemsep}{0pt}
\item Proposition \ref{prop:shortcom}: a short representation of
the adjugate matrix $\adj(X{-}M)$ whose size is $O(n^2)$ elements of the base
field (instead of $O(n^3)$ for a dense representation);
\item two algorithms to compute $\adj(X{-}M)$: the first one is 
division-free and performs $\softO(n^3)$ operations in $\Zp$ while 
the second one may perform divisions and has complexity $\softO(n^\omega)$;
\end{enumerate}
and deduce from this the following applications to the question
we are interested in:
\begin{enumerate}
\renewcommand{\itemsep}{0pt}
\setcounter{enumi}{2}
\item Corollary \ref{cor:prec_gain}: a simple criterion to decide 
whether $\chi_M$ is defined at precision higher than the precision of 
$M$ (when $M$ lies in $M_n(\mathbb{Z}_p)$ and all its entries are given 
at the same precision);
\item two algorithms to compute the optimal precision on each 
coefficient of the characteristic polynomial of a matrix $M \in 
M_n(\Qp)$ (whose entries may be given at different precision): the first 
one is division-free and has complexity $\softO(n^3)$ (operations in
$\Zp$) while the second 
one may perform divisions but has complexity $\softO(n^\omega)$;
\item Proposition \ref{prop:opteigenvalues}: a $\softO(n^\omega)$
algorithm to compute the optimal precision on each eigenvalue of $M$.
\end{enumerate}
Moreover, we provide evidence showing that classical algorithms for 
computing the characteristic polynomials do not reach the optimal 
precision in many contexts.
Note that we focus on computing the precision of $\chi_M$: we do not give a stable
algorithm for computing the characteristic polynomial itself.

\medskip

\noindent
{\bf Organization of the article.}
In Section \ref{sec:theo_study}, we review the differential
theory of precision developed in \cite{caruso-roe-vaccon:14a}
and apply it to the specific case of the characteristic polynomial.
We also describe when the characteristic polynomial will have a higher
precision than the input matrix and give a compact description
of $\adj(X-M)$, the main ingredient in the differential.

In Section \ref{sec:diffHess}, we develop $\softO(n^3)$ algorithms to approximate
the Hessenberg form of $M$, and through it to find $\adj(X-M)$ and thus
find the precision of the characteristic polynomial of $M$.  In Section \ref{sec:diffFrob},
we give a $\softO(n^\omega)$ algorithm to compute the compact description of $\adj(X-M)$.

Finally, we propose in Section \ref{sec:optjagged}
algorithms to compute the optimal coefficient-wise precision
for the characteristic polynomial.  We also give the results
of some experiments demonstrating that these methods can lead
to dramatic gains in precision over standard interval arithmetic.
We close by describing the precision associated to
eigenvalues of a matrix.

\medskip

\noindent
{\bf Notations.} 
Throughout the paper, $K$ will refer to a complete,
discrete valuation field, $\val : K \twoheadrightarrow \Z \cup \{+\infty\}$ to its valuation,
$\OK$ its ring of integers and $\pi$ a uniformizer. 
We will write that $f(n)=\softO (g(n))$ if there exists some
$k \in \mathbb{N}$ such that $f(n)=O(g(n) \log (n)^k).$
We will write $M$ for an $n \times n$ matrix over $K$,
and $\chi$ the characteristic polynomial map, $\chi_M \in K[X]$ for
the characteristic polynomial of $M$ and $d\chi_M$ for the differential
of $\chi$ at $M$, as a linear map from $M_n(K)$ to the space
of polynomials of degree less than $n$.
We fix an $\omega \in \R$ such that the multiplication
of two matrices over a ring is in $O(n^\omega)$ operations in the
ring. Currently, the smallest known $\omega$ is less than
$2.3728639 $ thanks to \cite{legall:14a}. 
We will denote by $I_n$ the identity matrix of rank $n$ in $M_n(K).$
When there is no ambiguity, we will drop this $I_n$
for scalar matrices, \textit{e.g.} for $\lambda \in K$
and $M \in M_n (K),$ $\lambda-M$ denotes $\lambda I_n - M.$
We write $\sigma_1(M), \dots, \sigma_n(M)$ for the
elementary divisors of $M$, sorted in increasing order of valuation.
Finally, given a polynomial $\chi(X) = a_0 + a_1 X + \cdots + a_{n-1} X^{n-1} + X^n$,
the \emph{companion matrix} $\mathscr{C}$ associated to $\chi$ is
\begin{equation}
\label{eq:companion}
\mathscr{C} = \left( \begin{matrix}
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & 1 & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & 0 & 1 \\
-a_0 & -a_1 & \cdots & \cdots & -a_{n-1}
\end{matrix} \right).
\end{equation}

\section{Theoretical study}
\label{sec:theo_study}

\subsection{The theory of p-adic precision} \label{sec:padicprec}

We recall some of the definitions and results of \cite{caruso-roe-vaccon:14a}
as a foundation for our discussion of the precision for the characteristic polynomial
of a matrix.  We will be concerned with two spaces in what follows:
the space $M_n(K)$ of $n \times n$ matrices with entries in $K$ and the space
$K_n[X]$ of monic degree $n$ polynomials over $K$.  We also define $K_{<n}[X]$ (resp. 
${\OK}_{<n}[X]$) as the space of polynomials over $K$ (resp. over $\OK$) 
of degree strictly less than $n$.

One cannot represent
matrices $M \in M_n(K)$ exactly, but instead must specify an approximation
together with a measure of the approximation's accuracy; there are several ways to do so.
When using \emph{jagged} precision, one gives a precision on each entry,
representing the $(i,j)$ entry as $a_{i,j} + O(\pi^{N_{i,j}})$. The integer $N_{i,j}$ 
is called the \emph{absolute precision} and may depend on $(i,j)$.
When the $N_{i,j}$ are all equal, we say that the precision datum is 
\emph{flat}.

We may also model precision more flexibly by using lattices.  We 
recall that a \emph{lattice} $H$ in $M_n(K)$ is the $\OK$-span of a vector
space basis, or equivalently the image of $M_n(\OK)$ under a 
$K$-linear transformation of $M_n(K)$ \cite[\S 2.2]{caruso-roe-vaccon:14a}.
For $M \in M_n(K)$, cosets of the form $M + H$ are special
neighborhoods of $M$; we will use them for modeling precision on $M$. 
Given a family of integers $\famN = (N_{i,j})_{i,j}$ the lattice
\[
H_{\famN} = \big\{ \, M = (a_{i,j})_{i,j} \in M_n(\Qp) \,\, | \,\, \forall i,\!j\!:
\pi^{N_{i,j}} \text{ divides } a_{i,j} \, \big\}
\]
corresponds to the jagged precision introduced above.
The benefit of working with lattices is that they behave well under 
differentiable maps \cite[Lemma 3.4]{caruso-roe-vaccon:14a}, but they
require more space and time to compute. 

Jagged, flat and lattice precision is defined similarly for monic polynomials, 
taking care that no precision is specified for the leading 
coefficient which must be exactly~$1$. The underlying 
vector space is now $K_{<n}[X]$ and we define a lattice in it as the
image of ${\OK}_{<n}[X]$ under some linear mapping.

Let $\chi : M_n(K) \to K_n[X]$ be the characteristic polynomial map
and $d \chi_M$ for its differential at $M$.
Our analysis of the precision behavior of $\chi$ rests upon
the computation of its derivative $d\chi$, using \cite[Lemma 3.4]{caruso-roe-vaccon:14a}
Let $\adj(M)$ denote the adjugate\footnote{In \cite{caruso-roe-vaccon:14a,caruso-roe-vaccon:15a} the adjugate was referred to as the \emph{comatrix}} of $M$ (when $M \in \GL_n(K)$,
we have $\adj(M) {=} \det(M) M^{-1}$).  Recall from
\cite[Appendix B]{caruso-roe-vaccon:14a} and \cite[\S 3.3]{caruso-roe-vaccon:15a}
that $d\chi_M$ is given by
\begin{equation} \label{eq:dchi}
\begin{array}{rcl}
d\chi_M: \quad M_n(K) & \to & K_{<n}[X] \smallskip \\
 dM & \mapsto & \tr(\adj(X{-}M) \cdot dM).
\end{array}
\end{equation}

When $d\chi_M$ is surjective and $H$ is sufficiently ``small'' and ``regular,'' we may give
the precision of $\chi_M$ precisely as
\begin{equation} \label{eq:mainlem}
\chi(M + H) = \chi(M) + d\chi_M(H).
\end{equation}
The equality in \eqref{eq:mainlem} justifies our use of the phrase \emph{optimal} precision
to denote the lattice precision $d\chi_M(H)$. To make the conditions on $H$ under
which \eqref{eq:mainlem} holds more precise, let
$\sigma_1(M), \dots, \sigma_n(M)$ denote the elementary divisors of $M$.

\begin{prop} \label{prop:mainlem}
Let $M \in M_n(K)$ and assume that $d \chi_M$ is surjective. Let:
\[
\nu = \max\left(\sum_{i=1}^{n-1} \val(\sigma_i(M)), 0\right).
\]
Then, for all integers $a, b \geq 0$ with $a + \nu < 2b$,
any lattice $H$ such that $M_n(\pi^b \OK) \subset H \subset
M_n(\pi^a \OK)$
satisfies Eq.~\eqref{eq:mainlem}
\end{prop}

\begin{proof}
Recall \cite[Definition 3.3]{caruso-roe-vaccon:15a} that the \emph{precision polygon}
of $M$ is the lower convex hull of the Newton polygons of the entries of $\adj(X-M)$.
By \cite[Proposition 3.4]{caruso-roe-vaccon:15a}, the endpoints of the precision polygon
occur at height $0$ and $\sum_{i=1}^{n-1} \val(\sigma_i(M))$.  By convexity,
${\OK}_{<n}[X] \subset d\chi_M(M_n(\pi^\nu \OK))$.

Since the coefficients of $\chi_M$ are given by polynomials in the entries of $M$
with integral coefficients, \cite[Proposition 2.2]{caruso-roe-vaccon:15a} implies
the conclusion.
\end{proof}

For further discussion of using lattices to measure precision,
details on the computation of $d\chi_M$ in \eqref{eq:dchi}, and 
examples of additional precision in $\chi_M$,
see \cite[\S\S 2.4.3, 3.2.2]{caruso:17a}.

\subsection{Several facts about the differential of \texorpdfstring{$\chi$}{x}}

We may rephrase the condition that $d\chi_M$ is surjective using more familiar invariants attached to $M$.

\begin{prop}
\label{prop:surjectivity}
For $M \in M_n(K)$, the following conditions are equivalent:
\begin{enumerate}[(i)]
\renewcommand{\itemsep}{0pt}
\item \label{pt:surj} the differential $d\chi_M$ is surjective
\item \label{pt:cyc} the matrix $M$ has a cyclic vector (\emph{i.e.} $M$ is similar
to a companion matrix)
\item \label{pt:1d} the eigenspaces of $M$ over the algebraic
closure $\bar{K}$ of $K$ all have dimension $1$
\item \label{pt:charmin} the characteristic polynomial of $M$ is equal to the minimal polynomial of $M$.
\end{enumerate}
\end{prop}

\begin{proof}
The equivalence of \eqref{pt:cyc}, \eqref{pt:1d}, and \eqref{pt:charmin} is standard; see \emph{e.g.}
\cite[\S 7.1]{hoffman-kunze:LinearAlgebra}.  We now show
\eqref{pt:cyc} $\Rightarrow$ \eqref{pt:surj} and \eqref{pt:surj} $\Rightarrow$ \eqref{pt:1d}

For any $P \in \GL_n(K)$, the image of $d\chi$ at $M$ will be the same
as the image of $d\chi$ at $PMP^{-1}$, so we may assume that
$M$ is a companion matrix.  For a companion matrix, the last column of
$\adj(X{-}M)$ consists of $1, X, X^2, \dots, X^{n-1}$ so $d\chi_M$ is surjective.

Now suppose that $M$ has an eigenvalue $\lambda$ over $\bar{K}$ occurring in multiple Jordan blocks.
After conjugating into Jordan normal form over $\bar{K}$, the entries of $\adj(X{-}M)$
will also be block diagonal, and divisible within each block by the product of $(X-\mu)^{d_\mu}$,
where $\mu, d_\mu$ ranges over the eigenvalues and dimensions of the other Jordan blocks.
Since $\lambda$ occurs in two Jordan blocks,
$X - \lambda$ will divide every entry of $\adj(X{-}M)$ and $d\chi_M$ will not be surjective.
\end{proof}

We also have an integral analogue of Proposition \ref{prop:surjectivity}.

\begin{prop}
\label{prop:intsurj}
For $M \in M_n(\OK)$, the following conditions are equivalent:
\begin{enumerate}[(i)]
\renewcommand{\itemsep}{0pt}
\item \label{pt:surjp} the image of $M_n(\OK)$ under $d\chi_M$ is 
${\OK}_{<n}[X]$.
\item \label{pt:cycp} the reduction of $M$ modulo $\pi$ has a cyclic vector.
\end{enumerate}
\end{prop}
\begin{proof}
The condition \eqref{pt:surjp} is equivalent to the surjectivity of $d\chi_M$ modulo $\pi$.  The equivalence with
\eqref{pt:cycp} is as in Proposition \ref{prop:surjectivity}, but over the residue field of $K$.
\end{proof}

The relationship between precision and the images of lattices under $d\chi_M$ allows us to
apply Proposition \ref{prop:intsurj} to determine when the precision of the characteristic polynomial
is the minimum possible.

\begin{cor} \label{cor:prec_gain}
Suppose that $M \in \GL_n(\OK)$ is known with flat precision $O(\pi^N)$.
Then the characteristic polynomial of $M$ has precision lattice strictly contained in $O(\pi^N)$
if and only if the reduction of $M$ modulo $\pi$ does not have a cyclic vector.
\end{cor}

Note that this criterion is checkable using $\softO(n^\omega)$ operations in the residue field \cite{storjohann:01a}.

\medskip

\noindent
{\bf Stability under multiplication by $X$.}
By definition, the codomain of $d \chi_M$ is $K_{< n}[X]$. 
However, when $M$ is given, $K_{< n}[X]$ is canonically isomorphic
to $K[X]/\chi_M(X)$ as a $K$-vector space. For our purpose, it will 
often be convenient to view $d \chi_M$ as a $K$-linear mapping
$M_n(K) \to K[X]/\chi_M(X)$.

\begin{prop}
Let $R$ be the subring of $K[X]$ consisting of polynomials $f$ for
which $f(M) \in M_n(\OK)$, and $\Lambda = d \chi_M \big(M_n(\OK)\big)$
as a submodule of $K[X]/\chi_M(X)$.  Then $\Lambda$ is stable under 
multiplication by $R$.
\end{prop}
\begin{proof}
Let $A = \adj(X{-}M)$ and $f \in R$.  By \eqref{eq:dchi}, $\Lambda$ is given by
the $\OK$-span of the entries of $A$.   Using the fact that the product of a matrix
with its adjugate is the determinant, $(X{-}M) \cdot A = \chi_M$ and thus
$f(X) \cdot A \equiv f(M) \cdot A \pmod{\chi_M(X)}$.  The span of the entries
of the left hand side is precisely $f(X) \cdot \Lambda$, while the span of the entries
of the right hand side is contained within $\Lambda$ since $f(M) \in M_n(\OK)$.
\end{proof}

\begin{cor}
If $M \in M_n(\OK)$, then $d \chi_M \big(M_n(\OK)\big)$ 
is stable under multiplication by $X$, so is a module over $\OK[X]$.
\end{cor}

\medskip

\noindent
{\bf Compact form of $d \chi_M$.}
Let $\mathscr{C}$ be the companion matrix associated to $\chi_M$ as in \eqref{eq:companion}.
Assume one of the equivalent conditions of Proposition~\ref{prop:surjectivity}, giving a matrix 
$P \in \GL_n(K)$ such that $M = P \mathscr{C} P^{-1}$. Then Proposition~\ref{prop:surjectivity}
also holds for $M^T$, yielding an
invertible matrix $Q \in \GL_n(K)$ with $M^T = Q \mathscr{C} Q^{-1}$.

\begin{prop}
\label{prop:shortcom}
We keep the previous notations and assumptions.
Let $Y$ be the row vector $(1, X, \ldots, X^{n-1})$. Then
\begin{equation}
\label{eq:shortcom}
\adj(X{-}M) = \alpha \cdot P Y^T \cdot Y Q^T
\mod \chi_M
\end{equation}
for some $\alpha \in K[X]$.
\end{prop}

\begin{proof}
Write $A = \adj(X{-}M)$. From $(X{-}M) \cdot A \equiv 0 
\pmod{\chi_M}$, we deduce $(X{-}\mathscr{C}) \cdot P^{-1} A \equiv 0 \pmod{\chi_M}$. 
Therefore each column of $P^{-1} A$ lies in the right kernel of $X{-}\mathscr{C}$
modulo $\chi_M$. On the other hand, a direct computation shows that
every column vector $W$ lying in the right kernel of $X{-}\mathscr{C}$ modulo 
$\chi_M$ can be written as $W = w \cdot Y^T$ for some $w \in 
K[X]/\chi_M$. We deduce that $A \equiv P \cdot Y^T B \pmod{\chi_M}$
for some row vector $B$.
Applying the same reasoning with $M^T$, we find that $B$ can be
written $B = \alpha Y Q^T$ for some $\alpha \in K[X]/\chi_M$ and
we are done.
\end{proof}

Proposition~\ref{prop:shortcom} shows that $\adj(X{-}M)$ can be encoded 
by the datum of the quadruple $(\alpha, P, Q, \chi_M)$ whose total size 
stays within $O(n^2)$: the polynomials $\alpha$ and $\chi_M$ are 
determined by $2n$ coefficients while we need $2n^2$ entries to 
write down the matrices $P$ and $Q$. 
We shall see moreover in Section \ref{sec:diffFrob} that interesting
information can be read off of this short form $(\alpha, P, Q, 
\chi_M)$.

\begin{rem}
With the previous notation, if $U \in GL_n(K),$
the quadruple for 
$UMU^{-1}$ is
$(\alpha, UP, (U^T)^{-1}Q, \chi_M),$
which can be computed in $O(n^\omega)$ operations in $K.$
This is faster than computing $U \adj(X-M) U^{-1}$ using the dense
representation, which requires $\softO(n^{\omega+1})$ operations in $K.$
\end{rem}

\section{Hessenberg form}
\label{sec:diffHess}

In this section, we combine the computation of a Hessenberg form of a 
matrix and the computation of the inverse through the Smith normal form 
(SNF) over a complete discrete valuation field (CDVF) to compute 
$\adj(X{-}M)$ and $d \chi$. If $M \in M_n(\OK)$, then only division 
by invertible elements of $\OK$ will occur.

\begin{rem}
In what follows, we will count operations in $K$ (regardless to 
precision) for expressing our complexities. This choice makes sense 
because we will not usually need much precision on the entries of 
$\adj(X{-}M)$ (roughly only their valuations matter).
\end{rem}

\subsection{Hessenberg form}

We begin with the computation of
an approximate Hessenberg form.

\begin{deftn}
A \emph{Hessenberg matrix} is a matrix $M \in M_n(K)$ with
\[
M_{i,j}=0 \mbox{ for $j \le i-2$.}
\]
Given integers $n_{i,j}$, an \emph{approximate Hessenberg matrix}
is a matrix $M \in M_n(K)$ with
\[
M_{i,j} = O(\pi^{n_{i,j}}) \mbox{ for $j \le i-2$.}
\]
If $M \in M_n (K)$ and $H \in M_n (K)$ is an (approximate) Hessenberg matrix
similar to $M$, we say that H is an \emph{(approximate) Hessenberg form} of $M.$
\end{deftn}

It is not hard to prove that every matrix over a field admits
a Hessenberg form.
We prove here that we may compute an approximate Hessenberg form 
for any matrix known at finite jagged precision over $K$.
Moreover, we provide an exact change of basis matrix.

\noindent\hrulefill

\noindent {\bf Algorithm 1:} {\tt Approximate Hessenberg form computation}

\noindent{\bf Input:} a matrix $M$ in $M_n(K).$

\smallskip

\noindent 0.\ $P:=I_n.$ \: $H:=M.$


\noindent 1.\ {\bf for} $j=1,\dots,n-1$ {\bf do} 

\noindent 2.\  \:  {\bf swap} the row $j+1$ with a row $i_{min}$ ($i_{min} \geq 2$) s.t. $\val(H_{i_{min},j})$ is minimal. 

\noindent 3.\  \:  {\bf for} $i=j+2,\dots,n$ {\bf do} 

\noindent 4. \ \: \:  \textbf{Eliminate} the significant digits of $H_{i,j}$ by pivoting with row $j+1$ 
using a matrix $T.$

\noindent 5. \ \: \:  $H:=H \times T^{-1}.$ \: $P:=T \times P.$

\noindent 6. \textbf{Return} $H,P.$

\vspace{-1ex}\noindent\hrulefill

\medskip

Note that we use the notation of matrix multiplication in line 5, but actual updating of
$H$ and $P$ uses row and column operations (individually using $O(n)$ operations in $K$).

\begin{prop} 
Algorithm 1 computes $H$ and $P$ realizing an approximate Hessenberg form of $M$.
$P$ is exact  
and the computation is in $O(n^3)$ operations in $K$ at precision the maximum precision of a coefficient in $M.$
\end{prop}
\begin{proof}
Inside the nested \textbf{for} loop, to eliminate $\pi^{u_y} \varepsilon_y+O(\pi^{n_y})$ with pivot $\pi^{u_x} \varepsilon_x+O(\pi^{n_x}),$
the corresponding coefficient of the corresponding shear matrix is 
any exact element of $K$ congruent to 
$\pi^{u_y-u_x} \varepsilon_y \varepsilon_x^{-1} \mod \pi^{u_y-u_x\min (n_x-u_x,n_y-u_y)}$.
Exactness follows directly. 
The rest is clear.
\end{proof}

\begin{rem} \label{rem:char_pol_from_hessenberg}
From a Hessenberg form of $M,$ it is well known
that one can compute the characteristic polynomial of 
$M$ in $O(n^3)$ operations in $K$ \cite[pp. 55--56]{Cohen:2013}
However, this computation involves division, and its
precision behavior is not easy to quantify.
\end{rem}


\subsection{Computation of the inverse}

In this section, we prove that to compute the inverse of
a matrix over $K$, the Smith normal form is precision-wise optimal in the flat-precision case.
We first recall the differential of matrix inversion.

\begin{lem}
Let $u \: : \: GL_n (K) \rightarrow GL_n(K),$ $M \mapsto M^{-1}.$
Then for $M \in GL_n (K),$ $du_M(dM)=M^{-1} dM M^{-1}.$
It is always surjective.
\end{lem}

We then have the following result about the loss in precision when computing the inverse.

\begin{prop}
Let $\cond(M) = \val(\sigma_n(M))$.
If $dM$ is a flat precision of $O(\pi^N)$ on $M$ then $M^{-1}$
can be computed at precision $O(\pi^{N-2\cond(M)})$ by a \textbf{SNF} computation
and this lower-bound is optimal,
at least when $N$ is large.
\end{prop}

\begin{proof}
The smallest valuation of an entry of $M^{-1}$ is $-\cond(M)$.
Consequently, $N - 2\cond(M)$
can be obtained as the valuation of an entry of $du_M(dM)$,
and it is the smallest that can be achieved this way for $dM$ in a flat precision lattice.
Hence the optimality of the bound given for large 
$N$ \cite[Lemma 3.4]{caruso-roe-vaccon:14a}.

Now, the computation of the Smith normal form was described in \cite{Vaccon-these}.
From $M$ known at flat precision $O(\pi^N),$ we can obtain an exact $\Delta$, and $P$ and $Q$ 
known at precision at least $O(\pi^{N-\cond(M)})$, with coefficients in $\OK$
and determinant in $\OK^\times$ realizing an Smith normal form of $M.$
There is no loss in precision when computing $P^{-1}$ and $Q^{-1}.$
Since the smallest valuation occurring in $\Delta^{-1}$ is $-\cond(M),$
we see that $M^{-1}=Q^{-1} \Delta^{-1} P^{-1}$ is known at precision at least $O(\pi^{N-2\cond(M)}),$
which concludes the proof.
\end{proof}

\subsection{The adjugate of \texorpdfstring{$X{-}H$}{X-H}}

In this section, we compute $\adj(X-H)$ for a Hessenberg matrix $H$
using the Smith normal form computation of the previous section.
The entries of $\adj(X-H)$ lie in $K[X]$, which is not a complete
discrete valuation field,
so we may not directly apply the methods of the previous section.
However, we may relate $\adj(X-H)$ to $\adj(1-XH)$, whose
entries lie in $K\llp X\rrp$.  In this way, we compute $\adj(X-H)$
using an SNF method, with no division in $K$.

We begin by relating adjugates of
similar matrices:

\begin{lem} \label{lem:adjugate_of_similar}
If $M_1,M_2 \in M_n(K)$ and $P \in GL_n (K)$ are such that
$M_1=PM_2P^{-1},$ then:
\[ \adj (X-M_1)=P \adj (X-M_2) P^{-1}. \] 
\end{lem}

The second ingredient we need is reciprocal polynomials.
We extend its definition to matrices of polynomials.
\begin{deftn}
Let $d \in \mathbb{N}$ and $f \in K[X]$ of degree at most $d.$ 
We define the reciprocal polynomial of order $d$ of $f$ as $f^{\rec,d}=X^d f \left( 1/X \right).$
Let $F \in M_n(K[X])$ a matrix of polynomials of degree at most $d.$
We denote by $F^{\rec,d}$ the matrix with $(F^{\rec,d})_{i,j} = (F_{i,j})^{\rec,d}$.
\end{deftn}
We then have the following result :
\begin{lem}
Let $M \in M_n(K).$ Then:
\begin{align*}
\adj(1-XM)^{\rec,n-1}&=\adj(X-M), \\
(\chi_M I_n)^{\rec,n}&=(1-XM) \adj(1-XM).\\
\end{align*}
\end{lem}
\begin{proof}
For any matrix $F \in M_d(K[X])$ of polynomials of degree at most $1$,
we have $\det (F^{\rec,1})=\det(F)^{\rec,d}.$
This result directly implies the second part of the lemma; the first part follows
from the fact that the entries of $\adj(X-M)$ and of $\adj(1-XM)$
are determinants of size $n-1$.
\end{proof}

This lemma allows us to compute $\adj(1-XM)$ instead of $\adj(X-M).$
This has a remarkable advantage: the pivots during the computation of
the SNF of $\adj(1-XM)$ are units in $K\llb X\rrb$ (in $\OK\llb X\rrb$ if $M \in M_n(\OK)$) and are known
in advance to be on the diagonal. This leads to a very smooth
precision and complexity behavior when $M \in M_n(\OK).$ 

\noindent\hrulefill

\noindent {\bf Algorithm 2:} {\tt Approximate $\adj (X -H)$ }

\noindent{\bf Input:} an approximate Hessenberg matrix $H$ in $M_n(\OK).$

\smallskip

\noindent 0.\ $U:=1-XH.$ $U_0:=1-XH.$

\noindent 1.\ While updating $U$, \textbf{track} $P$ and $Q$ so that $U_0=PUQ$ is always satisfied.

\noindent 2.\ {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 3.\  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,j},$ for $j\geq i+1$ 
using the invertible pivot
$U_{i,i}=1+XL_{i,i} \mod X^{n+1}$ (with $L_{i,i} \in \OK[X]$). 

\noindent 4.\    {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 5. \  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i+1,i},$
using the invertible pivot $U_{i,i}.$

\noindent 6. \ $\psi:=\prod_i U_{i,i}.$

\noindent 7. \ Rescale to get $U = I_n \mod X^{n+1}.$

\noindent 8. \ $Y:=\psi \times P \times Q   \mod X^{n+1}.$ \footnote{The product $P \times Q$
should be implemented by sequential row operations corresponding to the eliminations in Step 5
in order to avoid a product of two matrices in $M_n(\OK[X])$.}

\noindent 9. \ \textbf{Return} $Y^{\rec,n-1}, \psi^{\rec,n}.$

\vspace{-1ex}\noindent\hrulefill

\medskip

\begin{theo}
Let $H \in M_n(\OK)$ be an approximate Hessenberg matrix with
flat precision $O(\pi^N).$
Then, using Algorithm 2, one can compute $\adj (X{-}H)$
and $\chi_H$ in 
$\softO (n^3)$ operations in $\OK$ at precision $O(\pi^N).$
\end{theo}
\begin{proof}
First, the operations of the lines 2 and 3 use $\softO (n^3)$ operations
in $\OK$ at precision $O(\pi^N).$
Indeed, since $H$ is an approximate Heisenberg matrix, when we use $U_{i,i}$ as pivot
the only other nonzero coefficient in its column is $U_{i+1,i}$.
As a consequence, when performing this column-pivoting, only two rows ($i$ and
$i+1$) lead to operations in $\OK \llb X \rrb$ other than checking precision.
Hence, line 3 costs $\softO (n^2)$ for the computation of $U.$
Following line 1, the computation of $Q$ is done by operations on rows, starting from the identity matrix.
The order in which the entries of $U$ are cleared implies that $Q$ is just filled in as an upper triangular matrix:
no additional operations in $\OK\llb X\rrb$ are required. Thus the total cost
for lines 2 and 3 is indeed $\softO (n^3)$ operations.

For lines 4 and 5, there are only $n{-}1$ eliminations, resulting in a $\softO (n^2)$ cost
for the computation of $U.$ Rather than actually constructing $P$, we just track the eliminations
performed in order to do the corresponding row operations on $Q$, since we only need the product $P \times Q$.

Line 6 is in $\softO (n^2)$ and line 7 in $\softO (n^3).$

Thanks to the fact that the $P$ only corresponds to the product of $n{-}1$ 
shear matrices, the
product on line 8 is in $\softO (n^3).$
We emphasize that no division has been done.
Line 9 is costless, and the result is proven.
\end{proof}
\begin{rem}
If $M \in M_n(K)$ does not have coefficients in $\OK,$
we may apply Algorithms 1 and 2 to $p^v M \in M_n(\OK)$
in $\softO (n^3)$ operations in $\OK$, and then divide
the coefficient of $X^k$ in the resulting polynomial by $p^{kv}$.
\end{rem}

We will see in Section \ref{sec:optjagged} that for an
entry matrix with coefficients known at flat precision,
Algorithms 1 and 2 are enough to
know the optimal jagged precision on $\chi_M.$

\subsection{The adjugate of \texorpdfstring{$X{-}M$}{X-M}}

In this section, we combine Proposition \ref{prop:shortcom}
with Algorithm 2 to compute the adjugate of $X{-}M$
when $\chi_M$ is squarefree.   Note that, under
the assumption that $d\chi_M$ is surjective,
$\chi_M$ is squarefree if and only if $M$ is diagonalizable.  The result is
the following $\softO(n^3)$ algorithm, where the only
divisions are for gcd and modular inverse computations.

\noindent\hrulefill

\noindent {\bf Algorithm 3:} {\tt Approximate $\adj (X{-}M)$ }

\noindent{\bf Input:} an approx. $M \in M_n(\OK),$ with $\disc(\chi_M) \neq 0.$ 

\smallskip

\noindent 0.\ Find $P \in GL_n(\OK)$ and $H \in M_n(\OK),$ approximate Hessenberg,
such that $M=PHP^{-1},$ using Algorithm 1. 

\noindent 1.\ Compute $A=\adj (X-H)$ and $\chi_M = \chi_H$ using Algorithm 2.


\noindent 2.\ Do $\row(A,1) \leftarrow \row(A,1)+\sum_{i=2}^n \mu_i \row(A,i),$ for
random $\mu_i \in \OK,$ by doing $T \times A$ for some $T \in GL_n(\OK).$
Compute $B:=TAT^{-1}.$

\noindent 3.\ Similarily compute $C:=S^{-1}BS$ for $S \in GL_n(\OK)$ corresponding to
adding a random linear combination of the columns of index $j \ge 2$
to the first column of $B.$ 

\noindent 4.\  \textbf{If} $\gcd(C_{1,1}, \chi_M) \neq 1,$ \textbf{then} go to 2.

\noindent 5. Let $F$ be the inverse of $C_{1,1} \mod \chi_M$.

\noindent 6. Let $U := \col(C,1)$ and $Y := F \cdot \row(C,1) \mod \chi_M$.

\noindent 7. Return $\adj(X-M):=(PT^{-1}S U \times Y S^{-1} T P^{-1}) \mod \chi_M.$

\vspace{-1ex}\noindent\hrulefill

\medskip


\begin{theo}
\label{thm:alg3}
For $M \in M_n(\OK)$ such that $\disc( \chi_M) \neq 0,$
Algorithm 3 computes
$\adj (X-M) \pmod{\chi_M}$ in average complexity $\softO (n^3)$
operations in $K$.
The only divisions occur in taking gcds and inverses modulo $\chi_M$.
\end{theo}
\begin{proof}
As we have already seen, completing Steps 0 and 1 is in $\softO (n^3).$
Multiplying by $T$ or $S$ or their inverse corresponds
to $n$ operations on rows or columns over a matrix with coefficients
in $\OK[X]$ of degree at most $n$.
Thus, it is in $\softO(n^3).$
Step 5 is in $\softO(n)$, Step 6 in $\softO(n^2)$ and Step 7 in $\softO(n^3)$.
We need only prove that the set of $P$ and $S$ to avoid
is of dimension at most $n-1$.

The idea is to work modulo $X-\lambda$
for $\lambda$ a root of $\chi_M$ (in an algebraic closure)
and then apply Chinese Remainder Theorem.
The goal of the Step $2$ is to ensure the first row of $B$ contains an
invertible entry modulo $\chi_M$.
Since $A(\lambda)$ is of rank one, the $\mu_i$'s have to avoid an
affine hyperplane so that $\row(B,1) \mod (X-\lambda)$ is a non-zero vector.
Hence we need only avoid a finite union of affine hyperplanes
in order for $\row(B,1) \mod \chi (M)$ to contain an invertible coefficient.

Similarly, the goal of Step 3 is make $C_{1,1}$ invertible modulo
$\chi_M$.  Again, only a finite union of affine
hyperplane need be avoided.
Thus, almost any choice of $\mu_i$ leads to a matrix $C$ passing
Step 4, concluding the proof.
\end{proof}


\begin{rem}
As in the previous section, it is possible
to scale $M \in M_n(K)$ so as to
get coefficients in $\OK$ and apply the previous algorithm.
\end{rem}
\begin{rem}
We refer to \cite{caruso:15a} for the handling
of the precision of gcd and modular inverse computations.
In this article, ways to tame the loss of precision
coming from divisions are explored, following
the methods of \cite{caruso-roe-vaccon:14a}.
\end{rem}


\section{Frobenius form}
\label{sec:diffFrob}

The algorithm designed in the previous section computes the differential 
$d \chi_M$ of $\chi$ at a given matrix $M \in M_n(K)$ for a cost of 
$\softO (n^3)$ operations in $K$. This seems to be optimal given that 
the (naive) size of the $d \chi_M$ is $n^3$: it is a matrix of size $n 
\times n^2$. It turns out however that improvements are still possible! 
Indeed, thanks to Proposition~\ref{prop:shortcom}, the matrix of $d 
\chi_M$ admits a compact form which can be encoded using only $O(n^2)$ 
coefficients. The aim of this short section is to design a fast 
algorithm (with complexity $\softO(n^\omega)$ operations in $K$) for 
computing this short form. The price to pay is that divisions in $K$ 
appear, which can be an issue regarding to precision in particular 
cases. In this section, we only estimate the number of operations in $K$ 
and not their behavior on precision.

We now fix a matrix $M \in M_n(K)$ for which $d \chi_M$ is 
surjective. Let $(\alpha, P, Q, \chi_M)$ be the quadruple encoding
the short form of $d \chi_M$; we recall that they are related by the
relations:
\begin{align*}
d \chi_M(dM) & =\tr(\adj(X{-}M) \cdot dM) \\
\adj(X{-}M) & = \alpha \cdot P Y^T \cdot Y Q^T \mod \chi_M.
\end{align*}
We may approximate $\chi_M$ in $\softO(n^\omega)$ operations in $K$
by approximating the Frobenius normal form of $M$ \cite{storjohann:01a}.

The matrix $P$ can be computed as follows. Pick $c \in K^n$. Define 
$c_i = M^i c$ for all $i \geq 1$. The $c_i$'s can be computed in 
$\softO(n^\omega)$ operations in $K,$ \textit{e.g.}
using the first algorithm
of \cite{keller-gehrig:85a}. Let $P_\inv$ be the 
$n \times n$ matrix whose rows are the $c_i$'s for $1 \leq i \leq n$. 
Remark that $P_\inv$ is invertible if and only if $(c_0, c_1, \ldots, 
c_{n-1})$ is a basis of $K^n$ if and only if $c$ is a cyclic vector. 
Moreover after base change to the basis $(c_0, \ldots, c_{n-1})$, the matrix 
$M$ takes the shape \eqref{eq:companion}. In other words, if $P_\inv$
is invertible, then $P = P_\inv^{-1}$ is a solution of $M = P \mathscr{C} P^{-1}$,
where $\mathscr{C}$ is the companion matrix similar to $M$.
Moreover, observe that the condition ``$P_\inv$ is invertible'' is open
for the Zariski topology. It then happens with high probability as soon
as it is not empty, that is as soon as $M$ admits a cyclic vector, which
holds by assumption.


The characteristic polynomial $\chi_M$ can be recovered thanks to the
relation $a_0c_0 + a_1c_1 + \dots + a_{n-1}c_{n-1} = -c_{n-1} \cdot P$.

Now, instead of directly computing $Q$, we first compute a matrix $R$ 
with the property that $\mathscr{C}^T = R \mathscr{C} R^{-1}$. To do so,
we apply the same strategy as above except that we start with the vector
$e = (1, 0, \ldots, 0)$ (and not with a random vector). A simple computation shows
that, for $1 \leq i \leq n{-}1$, the vector $\mathscr{C}^i e$ has the shape:
$$\mathscr{C}^i e = (0, \ldots, 0, -a_0, \star, \ldots, \star)$$
with $n{-}i$ starting zeros. Therefore the $\mathscr{C}^i e$'s form a basis of
$K^n$, \emph{i.e.} $e$ is always a cyclic vector of $\mathscr{C}$.
We may then let $R$ have columns $e, \mathscr{C}e, \dots, \mathscr{C}^{n-1}e$
and recover $Q$ using the relation $Q = P_\inv^T R$.

It remains to compute the scaling factor $\alpha$. We write
\begin{equation}
\label{eq:shortcomN}
\adj(X{-}\mathscr{C}) = \alpha \cdot Y^T \cdot Y R^T \mod \chi_M
\end{equation}
by multiplying Eq.~\eqref{eq:shortcom} on the 
left by $P^{-1}$ and on the right by $P$. We observe
moreover that the first row of $R$ is $(1, 0, \ldots, 0)$. Evaluating
the top left entry of Eq.~\eqref{eq:shortcomN}, we end up with the 
relation:
$$\alpha = a_1 + a_2 X + \cdots + a_{n-1} X^{n-2} + X^{n-1}.$$
No further computation are then needed to derive the value of $\alpha$.
We summarize this section with the following theorem:

\begin{theo}
\label{thm:compute_shortcom}
Given $M \in M_n(K)$ with $d \chi_M$ surjective,
one can compute $(\alpha, P, Q, \chi_M) \in K[X]$ with
$\softO(n^\omega)$ operations in $K$ such that
$\adj(X{-}M) = \alpha \cdot P Y^T \cdot Y Q^T \mod \chi_M$.
\end{theo}

\section{Optimal jagged precision}
\label{sec:optjagged}

In the previous Sections, \ref{sec:diffHess} and \ref{sec:diffFrob},
we have proposed algorithms to obtain the adjugate of 
$X{-}M.$ Our motivation for these computations is to then
be able to understand what is the optimal precision on $\chi_M.$
In this section, we provide some answers to this question,
along with numerical evidence.
We also show that it is then possible to derive 
optimal precision of eigenvalues of $M.$ 

\subsection{On the characteristic polynomial}

For $0 \leq k < n$, let $\beta_k : K[X] \to K$ be the mapping taking a 
polynomial to its coefficients in $X^k$. By applying 
\cite[Lemma 3.4]{caruso-roe-vaccon:14a} to the composite $\beta_k 
\circ \chi_M$, one can figure out the optimal precision on the
$k$-th coefficient of the characteristic polynomial of $M$ (at
least if $M$ is given at enough precision).

Let us consider more precisely the case where $M$ is given at 
jagged precision: the $(i,j)$ entry of $M$ is given at precision 
$O(\pi^{N_{i,j}})$ for some integers $N_{i,j}$. 
Lemma 3.4 of \cite{caruso-roe-vaccon:14a} then shows that
the optimal precision on the $k$-th coefficient of $\chi_M$ is 
$O(\pi^{N'_k})$ where $N'_k$ is given by the formula:
\begin{equation}
\label{eq:optjagged}
N'_k = \min_{1 \leq i, j\leq n} N_{j,i} + \val(\beta_k(A_{i,j})),
\end{equation}
where $A_{i,j}$ is the $(i,j)$ entry of the adjugate $\adj(X{-}M)$.

\begin{prop} \label{prop:optimal_jagged}
If $M \in M_n(\OK)$ is given at (high enough) jagged precision, 
then we can compute the optimal jagged precision on $\chi_M$ in 
$\softO (n^3)$ operations in $K$.
\end{prop}

\begin{proof}
We have seen in \S \ref{sec:diffHess} and \S \ref{sec:diffFrob}
that the computation of the matrix $A = \adj(X{-}M)$ can be carried out 
within $\softO(n^3)$ operations in $K$ (either with the Hessenberg 
method or the Frobenius method). We conclude by applying 
Eq.~\eqref{eq:optjagged} which requires no further operation in $K$
(but $n^3$ evaluations of valuations and $n^3$ manipulations of 
integers).
\end{proof}

\begin{rem}
If $M \in M_n(\OK)$ is given at (high enough) \emph{flat} precision, 
then we can avoid the final base change step in the Hessenberg 
and thus any division by a non-invertible.
Indeed, observe that, thanks to Lemma \ref{lem:adjugate_of_similar}, 
we can write:
$$\tr(\adj (X{-}M) \cdot dM)=\tr(\adj (X{-}H)\cdot P^{-1} dM P)$$
where $P$ lies in $\GL_n(\OK)$. Moreover, the latter condition implies
that $P^{-1} dM P$ runs over $M_n(\OK)$ when $P$ runs over $M_n(\OK)$.
As a consequence, the integer $N'_k$ giving the optimal precision on the 
$k$-th coefficient of $M$ is also equal to
$N + \min_{1 \leq i, j\leq n} \val(\beta_k(A^H_{i,j}))$
where $A^H_{i,j}$ is the $(i,j)$ entry of $\adj(X{-}H)$,
where $H$ is the Hessenberg form of $M$.
\end{rem}

\begin{rem} \label{rem:lift_for_optimal}
As a consequence of the previous discussion, once the optimal jagged 
precision is known, it is possible to lift the entries of $M$ to a sufficiently
large precision, rescale them to have entries in $O_K$
and then use Algorithm 2 to
compute the characteristic polynomial.
The output might then need to be rescaled and
 truncated to the optimal precision. 
This requires $\softO(n^3)$ operations in $O_K$ 
and may require a large intermediate precision.
Better methods for approximating $\chi_M$ with
lower intermediate precision would be a valuable
future contribution.
\end{rem}

\medskip

\noindent
{\bf Numerical experiments.}
We have made numerical experiments in \textsc{SageMath}~\cite{sage}
in order to compare the optimal precision obtained with the methods
explained above with the actual precision obtained by the software.
For doing so, we picked a sample of $1000$ matrices $M$ in 
$M_9(\Q_2)$ where all the entries are chosen randomly as follows.
We fix an integer $N$, the relative precision, and generate elements of $\Q_p$ of the shape
\[
x = p^v \cdot \big(a + O\big(p^{N+v_p(a)}\big)\big)
\]
where $v$ is an integer generated according to the distribution:
\[
\mathbb P [v = 0] = \frac 1 5 \quad ; \quad
\mathbb P [v = n] = \frac 2 {5\cdot |n| \cdot (|n|+1)} \text{ for }
|n| \geq 1
\]
and $a$ is an integer in the range $[0, p^N)$, selected uniformly at random.
This distribution is chosen merely because it is the default in \textsc{SageMath}.

Once this sample has been generated, we computed, for each $k \in \{0, 
1, \ldots, 8\}$, the three following quantities:

%\vspace{-2mm}

\begin{enumerate}[$\bullet$]
\renewcommand{\itemsep}{0pt}
\item the optimal precision on the $k$-th coefficient of the 
characteristic polynomial of $M$ given by Eq.~\eqref{eq:optjagged}.
(Note that the method for computing $\adj(X{-}M)$ is irrelevant
since we only record the precision loss, not the runtime.)
\item in the capped relative mode (each 
coefficient carries its own precision which is updated after each 
elementary arithmetic operation),
the precision on the $k$-th coefficient of the 
characteristic polynomial of $M$ is computed \emph{via} the call:

\smallskip

\hfill$M\texttt{.charpoly(algorithm="df")}.$\hfill\null

\smallskip

\noindent 
Note that the precision behavior described persists when using algorithms
other than the division-free method.

\item in the model of floating-point arithmetic (see 
\cite[\S 2.3]{caruso:17a}), the number of correct digits of the 
$k$-th coefficient of the characteristic polynomial of $M$.
\end{enumerate}

\noindent
The table of Figure~\ref{fig:exp} summarizes the results obtained. 
\begin{figure}
\hfill
{\renewcommand*{\arraystretch}{1.3}
\begin{tabular}{|c|r|r|r|}
\cline{2-4} 
\multicolumn{1}{l|}{} 
  & \multicolumn{3}{c|}{Average loss of accuracy} \\
\cline{2-4}
\multicolumn{1}{l|}{\null\hspace{6mm}\null} 
  & \makebox[1.5cm]{\hfill Optimal\hfill\null}
  & \makebox[1.5cm]{\hfill CR\hfill\null}
  & \makebox[1.5cm]{\hfill FP\hfill\null} \\
\hline
\rule{0pt}{2.7ex}%
$X^0$ & $3.17$ & $196\phantom{.00}$ & $189\phantom{.00}$ \vspace{-1.5ex} \\
{\scriptsize (det.)} 
& {\scriptsize dev: $1.76$} & {\scriptsize dev: $240$} & {\scriptsize dev: $226$} \\
\rule{0pt}{2.7ex}%
$X^1$ & $2.98$ & $161\phantom{.00}$ & $156\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.69$} & {\scriptsize dev: $204$} & {\scriptsize dev: $195$} \\
\rule{0pt}{2.7ex}%
$X^2$ & $2.75$ & $129\phantom{.00}$ & $126\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.57$} & {\scriptsize dev: $164$} & {\scriptsize dev: $164$} \\
\rule{0pt}{2.7ex}%
$X^3$ & $2.74$ & $108\phantom{.00}$ & $105\phantom{.00}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.73$} & {\scriptsize dev: $144$} & {\scriptsize dev: $143$} \\
\rule{0pt}{2.7ex}%
$X^4$ & $2.57$ &  $63.2\phantom{0}$ &  $60.6\phantom{0}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.70$} & {\scriptsize dev: $85.9$} & {\scriptsize dev: $85.8$} \\
\rule{0pt}{2.7ex}%
$X^5$ & $2.29$ &  $51.6\phantom{0}$ &  $49.7\phantom{0}$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.66$} & {\scriptsize dev: $75.3$} & {\scriptsize dev: $74.9$} \\
\rule{0pt}{2.7ex}%
$X^6$ & $2.07$ &   $9.04$           &   $8.59$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.70$} & {\scriptsize dev: $26.9$} & {\scriptsize dev: $26.4$} \\
\rule{0pt}{2.7ex}%
$X^7$ & $1.64$ &   $5.70$           &   $5.38$ \vspace{-1.5ex} \\
& {\scriptsize dev: $1.65$} & {\scriptsize dev: $15.3$} & {\scriptsize dev: $14.7$} \\
\rule{0pt}{2.7ex}%
$X^8$ & $0.99$ &   $0.99$           &   $0.99$ \vspace{-1.5ex} \\
{\scriptsize (trace)} 
& {\scriptsize dev: $1.37$} & {\scriptsize dev: $1.37$} & {\scriptsize dev: $1.37$} \\
\hline
\end{tabular}}
\hfill\null

\medskip

\hfill
{\footnotesize Results for a sample of $1000$ instances}
\hfill\null

\caption{Average precision loss on 
the characteristic polynomial of a random $9 \times 9$
matrix over $\Q_2$}
\label{fig:exp}

\end{figure}
%
It should be read as follows. The acronyms CR and FP 
refer to ``capped relative'' and ``floating-point'' respectively.
The numbers displayed are the average loss of
\emph{relative} precision. 
More precisely, if $N$ is the relative precision of
the entries of $M$ and $v$ is the valuation of
the $k$-th coefficient of $\chi_M$, then:

%\vspace{-2mm}

\begin{enumerate}[$\bullet$]
\renewcommand{\itemsep}{0pt}
\item the column ``Optimal'' is the average of the quantities 
$(N'_k{-}v) - N$ (where $N'_k$ is defined by Eq.~\eqref{eq:optjagged}): 
$N'_k{-}v$ is the optimal \emph{relative} precision, so that the
difference $(N'_k{-}v) - N$ is the loss of relative precision;
\item the column ``CR'' is the average of the quatities 
$(\text{CR}_k{-}v) - N$ where $\text{CR}_k$ is the computed (absolute) 
precision on the $k$-th coefficient of $\chi_M$;
\item the column ``FP'' is the average of the quatities 
$(\text{FP}_k{-}v) - N$ where $\text{FP}_k$ is the first position of
an incorrect digit on the $k$-th coefficient of $\chi_M$.
\end{enumerate}

We observe that the loss of relative accuracy stays under control in the 
``Optimal'' column whereas it has very erratic behavior in the two other columns,
demonstrating the utility of the methods developed in this paper.

\subsection{On eigenvalues}
\label{ssec:eigenvalues}

Let $M \in M_n(K)$ and $\lambda \in K$ be a \emph{simple}
\footnote{the corresponding generalized eigenspace has dimension $1$} eigenvalue 
of $M$. We are interesting in quantifying the optimal precision on 
$\lambda$ when $M$ is given with some uncertainty.

To do so, we fix an approximation $M_\app \in M_n(K)$ of $M$ and 
suppose that the uncertainty of $M$ is jagged:
each entry of $M$ is given at some precision $O(\pi^{N_{i,j}})$.
Let $\lambda_\app$ be the relevant eigenvalue of $M_\app$. We remark 
that it is possible to follow the eigenvalue $\lambda_\app$ on a small 
neighborhood $\calU$ of $M$. More precisely, there exists a unique continuous 
function $f : \calU \to K$ such that
$f(M_\app) = \lambda_\app$, and
$f(M')$ is an eigenvalue of $M'$ for all $M' \in \calU$.

\begin{lem}
The function $f$ is strictly differentiable on a neighborhood of 
$M_\app$, with differential at $M$ the map
\[
dM \mapsto d \lambda = - \frac{\tr(\adj(\lambda-M) \cdot dM)}
{\chi'_M(\lambda)},
\]
where $\chi'_M$ is the usual derivative of $\chi_M$.
\end{lem}

\begin{proof}
The first assertion follows from the implicit function theorem,
the second from differentiating $\chi_M(\lambda) = 0$, which gives
$\chi'_M(\lambda) \cdot d \lambda + \tr(\adj(X-M) \cdot dM)(\lambda) = 0$.
\end{proof}

\noindent
Lemma 3.4 of \cite{caruso-roe-vaccon:14a} now implies that, if the
$N_{i,j}$'s are large enough and sufficiently well balanced, the optimal
precision on the eigenvalue $\lambda$ is $O(\pi^{N'})$ with:
$$N' = \min_{1 \leq i, j\leq n} \big(N_{j,i} + \val(A_{i,j}(\lambda)) - 
\val(\chi'_M(\lambda))\big)$$
where $A_{i,j}$ denotes as above the $(i,j)$ entry of $\adj(X{-}M)$.
Writing
$\adj(X{-}M) = \alpha \cdot P Y^T \cdot Y Q^T \text{ mod } \chi_M$
as in Proposition~\ref{prop:shortcom}, we find:
\begin{align}
N' & = \val(\alpha(\lambda)) - \val(\chi'_M(\lambda)) \nonumber \\
& \hspace{2mm} + \min_{1 \leq i, j\leq n} \big(N_{j,i} + 
\val(P_i Y(\lambda)^T) + \val(Y(\lambda) Q_j^T)\big) \label{eq:Nprime}
\end{align}
where $P_i$ denotes the $i$-th row of $P$ and, similarly, $Q_j$
denotes the $j$-th row of $Q$. Note moreover that $Y(\lambda)$ is
the row vector $(1, \lambda, \ldots, \lambda^{n-1})$.
By the discussion of \S \ref{sec:diffFrob}, the exact value of $N'$ can be 
determined for a cost of $\softO(n^\omega)$ operations in $K$ and
$O(n^2)$ operations on integers. 

When $M$ is given at flat precision, \emph{i.e.} the $N_{i,j}$'s are all 
equal to some $N$, the formula for $N'$ may be rewritten:
\begin{align}
N' & = N + \val(\alpha(\lambda)) - \val(\chi'_M(\lambda)) \nonumber \\
& \hspace{2mm} + \min_{1 \leq i \leq n} \val(P_i Y(\lambda)^T)
+ \min_{1 \leq j \leq n} \val(Y(\lambda) Q_j^T)\big) \label{eq:Nprimeflat}
\end{align}
and can therefore now be evaluated for a cost of $\softO(n^\omega)$
operations in $K$ and only $O(n)$ operations with integers.

\medskip

To conclude, let us briefly discuss the situation where we want
to figure out the optimal jagged precision on a tuple $(\lambda_1,
\ldots, \lambda_s)$ of simple eigenvalues. Applying \eqref{eq:Nprime},
we find that the optimal precision on $\lambda_k$ is 
\begin{align*}
N'_k & = \val(\alpha(\lambda_k)) - \val(\chi'_M(\lambda_k)) \\
& \hspace{2mm} + \min_{1 \leq i, j\leq n} \big(N_{j,i} + 
\val(P_i Y(\lambda_k)^T) + \val(Y(\lambda_k) Q_j^T)\big).
\end{align*}

\begin{prop}
\label{prop:opteigenvalues}
The $N'_k$'s can be all computed in $\softO(n^\omega)$ operations
in $K$ and $O(n^2 s)$ operations with integers.

If the $N_{i,j}$'s are all equal, the above complexity can be 
lowered to $\softO(n^\omega)$ operations in $K$ and $O(n s)$ operations 
with integers.
\end{prop}

\begin{proof}
The $\alpha(\lambda_k)$'s and the $\chi'_M(\lambda_k)$'s can be computed 
for a cost of $\softO(ns)$ operations in $K$ using fast multipoint 
evaluation methods (see \S 10.7 of \cite{gathen-gerhard:13a}).
On the other hand, we observe that $P_i Y(\lambda_k)^T$ is
the $(i,k)$ entry of the matrix:
$$P \cdot \left( \begin{matrix}
\lambda_1 & \cdots & \lambda_s \\
\lambda_1^2 & \cdots & \lambda_s^2 \\
\vdots & & \vdots \\
\lambda_1^{n-1} & \cdots & \lambda_s^{n-1}
\end{matrix} \right).$$
The latter product can be computed in $\softO(n^2)$ operations in 
$K$. Indeed,
the right factor is a truncated Vandermonde matrix, so that
computing the above product reduces to evaluating a polynomial at the
points $\lambda_1, \ldots, \lambda_s$. Therefore all the $P_i 
Y(\lambda_k)^T$'s (for $i$ and $k$ varying) 
can be determined with the same complexity. 
Similarly all the $Y(\lambda) Q_j^T$ are computed for the same cost.
The first assertion of Proposition~\ref{prop:opteigenvalues} follows.
The second assertion is now proved similarly to the case of a unique
eigenvalue.
\end{proof}

\bibliographystyle{plain}
\bibliography{charpoly}

\end{document}

\begin{comment}
\begin{prop} \label{prop:factor_com}
Let $M \in M_n (\OK)$ be such that $discr(\chi (M)) \neq 0.$
We assume that $A=\adj (X-M)$ is such that $\gcd(A_{1,1}, \chi (M))=1$
(in $K[X]$).
Then for $U=col(A,1)$ and $\Lambda=A_{1,1}^{-1} row(A,1) \mod \chi (M),$
both in $\OK[X]^n,$ we have: 
\[ \adj (X-M)= U \cdot ^T \mod \chi (M).\]
\todo{I think that the correct condition is $d \chi_M$ surjective (which is a bit weaker).}
\end{prop}


\begin{proof}
We first prove that for all $i,j \in \llbracket 1, n \rrbracket,$ we have
\begin{equation}A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod \chi (M). \label{eqn:2-minors-com} \end{equation}
Let $\lambda$ be a root of $\chi (M)$ in an algebraic closure of $K.$
Then $A(\lambda)$ is of rank one.
Indeed, $A(\lambda) \times (\lambda-M) =0.$
Since $\chi (M)$ is square-free, then $\lambda-M$ is of rank $n-1$
and then $A(\lambda)$ is of rank at most one.
Nevertheless, we can remark that
 $\gcd(A_{1,1}, \chi (M))=1,$ $A_{1,1} \neq 0 \mod (X-\lambda)$
and thus, $A(\lambda)$ is of rank exactly one.
As a consequence, all $2 \times 2$-minors of $A(\lambda)$ are zero.
It means that \[A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod (X-\lambda). \]
By the Chinese Remainder Theorem, our first result is then proved.

Now, let us take $U=col(A,1)$ the first column of $A$ and
$V=A_{1,1}^{-1} row(A,1) \mod \chi (M)$ (as $A_{1,1}$ is
invertible $\mod \chi (M)).$
Then thanks to \eqref{eqn:2-minors-com}, $U$ and $V$ satisfy the desired equality. 
\end{proof}

\begin{rem}
If $M \in M_n (\OK)$ is such that for some $U,V \in O_{K}[X]^n$
we have $\adj (X-M)= U \cdot V^T \mod \chi (M)$ and if 
$P\in GL_n (\OK)$ then
\[\adj (X-PMP^{-1})=(PU) \cdot ( V^T P^{-1}) \mod \chi_M.\]
In other words, factorizability of $\adj(X-M)$ is stable
by change of basis.
\end{rem}
\end{comment}
