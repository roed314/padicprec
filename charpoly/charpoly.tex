\documentclass{sig-alternate-05-2015}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\hyphenation{regarding}

\usepackage{amsmath,amssymb}
\usepackage{amsthmnoproof}
\usepackage{amsrefs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\com}{Com}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lcm}{lcm}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\newcommand{\OK}{\mathcal{O}_K}

\newcommand{\softO}{O\tilde{~}}

\newcommand{\inv}{\text{\rm inv}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\CopyrightYear{2016}
\setcopyright{acmcopyright}
\conferenceinfo{ISSAC '16,}{July 19-22, 2016, Waterloo, ON, Canada}
\isbn{978-1-4503-4380-0/16/07}\acmPrice{\$15.00}
\doi{http://dx.doi.org/10.1145/2930889.2930898}

\permission{Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only.}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\title{Characteristic polynomials of p-adic matrices}

\numberofauthors{3}
\author{
\alignauthor Xavier Caruso\\
  \affaddr{Universit\'e Rennes 1}\\
  \affaddr{\textsf{xavier.caruso@normalesup.org}}
\alignauthor David Roe\\
  \affaddr{Pittsburg University}\\
  \affaddr{\textsf{roed@pitt.edu}}
\alignauthor Tristan Vaccon\\
  \affaddr{Universit\'e de Limoges}\\
  \affaddr{\textsf{tristan.vaccon@unilim.fr}}
}

\maketitle

\begin{abstract}
We analyze the precision of the characteristic polynomial $\chi(A)$ of
an $n \times n$ $p$-adic matrix $A$ using differential precision methods
developed previously.  When $A$ is integral with precision $O(p^k)$,
we give a criterion (checkable in time $\softO(n^\omega)$) for
$\chi(A)$ to have precision exactly $O(p^k)$.  We also give a $\softO(n^3)$
algorithm for determining the optimal precision when the criterion is not
satisfied, and give examples when the precision is larger than $O(p^k)$.
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010148.10010149.10010150</concept_id>
<concept_desc>Computing methodologies~Algebraic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\vspace{-1mm}
\ccsdesc[500]{Computing methodologies~Algebraic algorithms}
\printccsdesc

\vspace{-1.5mm}
\keywords{Algorithms, $p$-adic precision, Newton polygon, factorization}

%\vspace{1mm}
% \noindent
% {\bf Categories and Subject Descripto\RS:} \\
%\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
%  Manipulation -- \emph{Algebraic Algorithms}
%
% \vspace{1mm}
% \noindent
% {\bf General Terms:} Algorithms, Theory
%
% \vspace{1mm}
% \noindent
% {\bf Keywords:} $p$-adic precision, linear algebra, ultrametric analysis
%\medskip

\section{Introduction}

\todo{Example of $M$ and $M + 1$}

\subsection{Notation} Throughout the paper, $K$ will refer to a complete,
discrete valuation field, $\val : K \twoheadrightarrow \Z \cup \{+\infty\}$ to its valuation,
$\OK$ its ring of integers and $\pi$ a uniformizer.

\section{Theoretical study}

\subsection{The theory of p-adic precision}

We recall some of the definitions and results of \cite{caruso-roe-vaccon:14a}
as a foundation for our discussion of the precision for the characteristic polynomial
of a matrix.  We will be concerned with two $K$-manifolds in what follows:
the space $M_n(K)$ of $n \times n$ matrices with entries in $K$ and the space
$K_n[X]$ of monic degree $n$ polynomials over $K$.  Given a matrix $M \in M_n(K)$,
the most general kind of precision structure we may attach to $M$ is a
\emph{lattice} $H$ in the tangent space at $M$.  However, representing an
arbitrary lattice requires $n^2$ basis vectors, each with $n^2$ entries.  We therefore
frequently work with certain classes of lattices, either \emph{jagged} lattices
where we specify a precision for each matrix entry or \emph{flat} lattices where
every entry is known to a fixed precision $O(p^m)$.  Similarly, precision for
monic polynomials can be specified by giving a lattice in the tangent space
at $f(X) \in K_n[X]$, or restricted to jagged or flat precision in the interest
of simplicity.

Let $\chi : M_n(K) \to K_n[X]$ be the characteristic polynomial map.
Our analysis of the precision behavior of $\chi$ rests upon
the computation of its derivative $d\chi$, using \cite{caruso-roe-vaccon:14a}*{Lem. 3.4}.
For a matrix $M \in M_n(K)$ we identify the tangent space $V$
at $M$ with $M_n(K)$ itself, and the tangent space $W$ at $\chi(M)$ with
the space $K_{<n}[X]$ of polynomials of degree less than $n$.
Let $\com(M)$ denote the comatrix of $M$ (when $M \in \GL_n(K)$,
we have $\com(M) = \det(M) M^{-1}$).  Recall
\citelist{\cite{caruso-roe-vaccon:14a}*{Appendix B} \cite{caruso-roe-vaccon:15a}*{\S 3.3}}
that $d\chi(M)$ is given by
\begin{equation} \label{eq:dchi}
d\chi(M): dM \mapsto \tr(\com(X-M) \cdot dM).
\end{equation}
Write $B^-_V(r)$ (resp. $B_V(r)$) for the open (resp. closed) ball of radius $r$ in $V$.

\begin{prop} \label{prop:mainlem}
Suppose that $M \in M_n(K)$, all the Jordan blocks of $M$ have distinct generalized eigenvalues, and
let $C = \max(\lvert \det(M)^{-1} \rvert, 1)$.  \todo{What if $\det(M) = 0$?}. Then, for all $\rho \in (0, 1]$ and all
$r \in (0, C \cdot \rho^{-1})$, any lattice $H$ such that $B_V^-(\rho r) \subset H \subset B_V(r)$
satisfies:
\begin{equation}
\chi(M + H) = \chi(M) + d\chi(H).
\end{equation}
\end{prop}

\begin{proof}
We first show that the distinct generalized eigenvalues condition
guarantees that $d\chi(M): V \to W$ is surjective.  For any $A \in \GL_n(K)$,
the image of $d\chi(M)$ will be the same as the image of $d\chi(AMA^{-1})$ ,
so we may assume that $M$ is in Jordan form.  Letting $dM$ range over
matrices with a single $1$ entry and all other entries $0$, it suffices to show
that the entries of $\com(X-M)$ span $W$.  Suppose that the blocks of $M$
have dimensions $d_1, \dots, d_m$ with corresponding generalized eigenvalues
$\lambda_1, \dots, \lambda_m$.  The only nonzero entries of $\com(X-M)$
are those on or below the diagonal within the Jordan blocks, and within the $i$th
block, each entry along the $j$th subdiagonal is
$\pm(X-\lambda_i)^{d_i - j - 1} \prod_{k \ne i} (X-\lambda_k)^{d_k}$.
As long as the $\lambda_i$ are distinct, these form a basis for $W$.

Recall \cite{caruso-roe-vaccon:15a}*{Def. 3.3} that the \emph{precision polygon}
of $M$ is the lower convex hull of the Newton polygons of the entries of $\com(X-M)$.
Since the precision polygon lies below the Newton polygon \cite{caruso-roe-vaccon:15a}*{Prop. 3.4},
and since the smallest entry of the Newton polygon occurs either with the leading
coefficient or the constant term, $B_W(1) \subset d\chi(M)(B_V(C))$.

Since the coefficients of $\chi(M)$ are given by polynomials in the entries of $M$
with integral coefficients, \cite{caruso-roe-vaccon:15a}*{Prop. 2.2} implies
the conclusion.
\end{proof}

\begin{prop}
\label{prop:surjectivity}
For $M \in M_n(K)$, the following conditions are equivalent:
\begin{enumerate}[(i)]
\renewcommand{\itemsep}{0pt}
\item the differential $d\chi(M)$ is surjective
\item the matrix $M$ has a cyclic vector (\emph{i.e.} $M$ is similar
to a companion matrix)
\item the eigenspaces of $M$ associated to eigenvalues in an algebraic
closure of $\bar K$ have all dimension $1$
%\item the Jordan blocks of $M$ all have distinct generalized eigenvalues
\end{enumerate}
\end{prop}

By definition, the codomain of $d \chi_M$ is $K_{< n}[X]$. 
However, when $M$ is given, $K_{< n}[X]$ is canonically isomorphic
to $K[X]/\chi_M(X)$ as a $K$-vector space. For our purpose, it will 
often be convenient to view $d \chi_M$ as an $K$-linear mapping
$M_n(K) \to K[X]/\chi_M(X)$.

\begin{prop}
Let $A$ be the subring of $K[X]$ consisting of polynomials $P$ for
which $P(M) \in M_n(\OK)$. Then $d \chi_M \big(M_n(\OK)\big)$ 
(considered as a submodule of $K[X]/\chi_M(X)$) is stable by 
multiplication by the elements of $A$.
\end{prop}
\begin{proof}
\todo{The image is generated by the entries of $C = \com(X-M)$
and we have $P(M) \cdot C \equiv  P(X) \cdot C \pmod{\chi_M(X)}$ since the entries of $P(X) \cdot C$
are linear combinations with integer coefficients of the entries of $C$.  $C*(X-M) = \chi(M)$ (since matrix * comatrix is determinant).}
\end{proof}

\begin{cor}
If the coefficients of $M$ lie in $\OK$, then $d \chi_M \big(M_n(\OK)\big)$ 
is stable by multiplication by $X$ and hence is a module over $\OK[X]$.
\end{cor}

\begin{prop}
Let $M \in M_n(K)$, $r>0$ and $H \subset K[X] / \chi(X)$ be the image
of $B_V(r)$ under $d\chi(M)$.  Then $X \cdot H = H$.
\end{prop}
\begin{proof}
\todo{Finish proof}
\end{proof}


We can also give a criterion on $M$ that allows an easy description of the image of flat
precision lattices under $d\chi(M)$.
\begin{prop}
Suppose that $M \in M_n(\OK)$, and that the reduction $\bar{M}$ of $M$ modulo $\pi$ has
a cyclic vector.  Then for any $r > 0$, we have $d\chi(M)(B_V(r)) = B_W(r)$.
\end{prop}
\begin{proof}
First note that it suffices to prove the claim for $r=1$ since $d\chi(M)$ is linear.
Next, it suffices to check that $d\chi(M)$ is surjective modulo $\pi$.
Surjectivity of differential
can be seen: write matrix in Jordan normal form.  Comatrix(X - M) surjective iff when you write it
in Frobenius normal form it has only one block.
\end{proof}

\todo{Image modulo $p$ and elementary divisors. 
Consequences: (1)~computation in $\softO(n^\omega)$ and (2)~gain criterium.}  

\begin{comment}
\begin{prop} \label{prop:factor_com}
Let $M \in M_n (O_K)$ be such that $discr(\chi (M)) \neq 0.$
We assume that $A=\com (X-M)$ is such that $gcd(A_{1,1}, \chi (M))=1$
(in $K[X]$).
Then for $U=col(A,1)$ and $V=A_{1,1}^{-1} row(A,1) \mod \chi (M),$
both in $O_K[X]^n,$ we have: 
\[ \com (X-M)= U \cdot {}^t V \mod \chi (M).\]
\todo{I think that the correct condition is $d \chi_M$ surjective (which
is a bit weaker).}
\end{prop}


\begin{proof}
We first prove that for all $i,j \in \llbracket 1, n \rrbracket,$ we have
\begin{equation}A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod \chi (M). \label{eqn:2-minors-com} \end{equation}
Let $\lambda$ be a root of $\chi (M)$ in an algebraic closure of $K.$
Then $A(\lambda)$ is of rank one.
Indeed, $A(\lambda) \times (\lambda-M) =0.$
Since $\chi (M)$ is square-free, then $\lambda-M$ is of rank $n-1$
and then $A(\lambda)$ is of rank at most one.
Nevertheless, we can remark that
 $gcd(A_{1,1}, \chi (M))=1,$ $A_{1,1} \neq 0 \mod (X-\lambda)$
and thus, $A(\lambda)$ is of rank exactly one.
As a consequence, all $2 \times 2$-minors of $A(\lambda)$ are zero.
It means that \[A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod (X-\lambda). \]
By the Chinese Remainder Theorem, our first result is then proved.

Now, let us take $U=col(A,1)$ the first column of $A$ and
$V=A_{1,1}^{-1} row(A,1) \mod \chi (M)$ (as $A_{1,1}$ is
invertible $\mod \chi (M)).$
Then thanks to \eqref{eqn:2-minors-com}, $U$ and $V$ satisfy the desired equality. 
\end{proof}

\begin{rem}
If $M \in M_n (O_K)$ is such that for some $U,V \in O_{K}[X]^n$
we have $\com (X-M)= U \cdot {}^t V \mod \chi (M)$ and if 
$P\in GL_n (O_K)$ then
\[\com (X-PMP^{-1})=(PU) \cdot ( {}^tV P^{-1}) \mod \chi(M).\]
In other words, factorizability of $\com(X-M)$ is stable
by change of basis.
\end{rem}
\end{comment}

\subsection{Compact form of $d \chi_M$}

Let $C$ be the companion matrix associated to $\chi_M$:
\begin{equation}
\label{eq:companion}
N = \left( \begin{matrix}
0 & 1 & 0 & \cdots & 0 \\
\vdots & \ddots & 1 & \ddots & \vdots \\
\vdots & & \ddots & \ddots & 0 \\
0 & \cdots & \cdots & 0 & 1 \\
-a_0 & -a_1 & \cdots & \cdots & -a_{n-1}
\end{matrix} \right)
\end{equation}
with $\chi_M = a_0 + a_1 X + \cdots + a_{n-1} X^{n-1} + X^n$.
By Proposition~\ref{prop:surjectivity}, there exists a matrix 
$P \in \GL_n(K)$ such that $M = P N P^{-1}$. Applying the same
result to the transpose of $M$, we find that there exists another
invertible matrix $Q \in \GL_n(K)$ such that $M^t = Q N Q^{-1}$.

\begin{prop}
\label{prop:shortcom}
We keep the previous notations and assumptions.
Let $V$ be the row vector $(1, X, \ldots, X^{n-1})$. Then
\begin{equation}
\label{eq:shortcom}
\com(X{-}M) = \alpha \cdot P V^t \cdot V Q^t
\mod \chi_M
\end{equation}
for some $\alpha \in K[X]$.
\end{prop}

\begin{proof}
Write $C = \com(X{-}M)$. From $(X{-}M) \cdot C \equiv 0 
\pmod{\chi_M}$, we deduce $(X{-}N) \cdot P^{-1} C \equiv 0 \pmod{\chi_M}$. 
Therefore each column of $P^{-1} C$ lies in the right kernel of $X{-}N$
modulo $\chi_M$. On the other hand, a direct computation shows that
every column vector $W$ lying in the right kernel of $X{-}N$ modulo 
$\chi_M$ can be written as $W = w \cdot V^t$ for some $w \in 
K[X]/\chi_M$. We deduce that $C \equiv P \cdot V^t B \pmod{\chi_M}$
for some row vector $B$.
Applying the same reasoning with $M^t$, we find that $B$ can be
written $B = \alpha V Q^t$ for some $\alpha \in K[X]/\chi_M$ and
we are done.
\end{proof}

Proposition~\ref{prop:shortcom} shows that $\com(X{-}M)$ can be encoded 
by the datum of the quadruple $(\alpha, P, Q, \chi_M)$ whose total size 
stays within $O(n^2)$ : the polynomials $\alpha$ and $\chi_M$ are 
determined by $2n$ coefficients while we need $2n^2$ coefficients to 
write down the matrices $P$ and $Q$. 
We shall see moreover in \S \ref{sec:eigenvalues} that interesting
informations can be read off on this short form $(\alpha, P, Q, 
\chi_M)$.

\section{Differential\\via Hessenberg form}
%\section{Computation of the characteristic polynomial and its derivative}

In this section, we combine the computation of an Hessenberg form
of a matrix and the computation of the inverse through the SNF
over a complete discrete valuation field
to compute $\com (X-M)$ and $d \chi.$
%The computation of an Hessenberg form provides an $O(n^3)$
%algorithm to compute the characteristic polynomial.

\subsection{Hessenberg form}

\todo{Describe algorithm (with matrices and endormophisms?). 
Remark about precision.}

\begin{deftn}
$M \in M_n (k)$ is said to be an Hessenberg matrix if
\[\forall i \in \llbracket 3, n \rrbracket, 
\forall j \in \llbracket 1, i-2 \rrbracket, M_{i,j}=0. \]
If $M \in M_n (k)$ and $H \in M_n (k)$ is an Hessenberg 
matrix such that there exists $P \in GL_n(k)$
with $H=PMP^{-1},$ we say that H is an Hessenberg form
of $M.$
\end{deftn}

\begin{deftn}
In the previous definition of an Hessenberg matrix $M,$
 if we only have $M_{i,j}= O(\pi^{n_{i,j}}),$ then we say
 that $M$ is an approximate Hessenberg matrix.
 This generalize directly to approximate Hessenberg form. 
\end{deftn}

It is not hard to prove that every matrix over a field admits
an Hessenberg form.
We prove here that over $K,$ if a matrix is 
known at finite (jagged) precision,
we can compute an approximate Hessenberg form of it.
Moreover, we can provide an exact change of basis matrix.
It relies on the following algorithm.


\noindent\hrulefill

\noindent {\bf Algorithm 1:} {\tt Approximate Hessenberg form computation}

\noindent{\bf Input:} a matrix $M$ in $M_n(K).$

\smallskip

\noindent 0.\ $P:=I_n.$ \: $H:=M.$


\noindent 1.\ {\bf for} $j=1,\dots,n-1$ {\bf do} 

\noindent 2.\  \:  {\bf swap} the row $j+1$ with a row $i_{min}$ ($i_{min} \geq 2$) s.t. $val(H_{i_{min},j})$ is minimal. 

\noindent 3.\  \:  {\bf for} $i=j+2,\dots,n$ {\bf do} 

\noindent 4. \ \: \:  \textbf{Eliminate} the significant digits of $H_{i,j}$ by pivoting with row $j+1$ 
using a matrix $T.$

\noindent 5. \ \: \:  $H:=H \times T^{-1}.$ \: $P:=T \times P.$

\noindent 6. \textbf{Return} $H,P.$

\vspace{-1ex}\noindent\hrulefill

\medskip



\begin{prop} 
Algorithm 1 computes $H$ and $P$ realizing an approximate Hessenberg form of $M.$
$P$ is exact over finite extensions of $\mathbb{Q}_p$ and $k((X))$, and the computation is in $O(n^3)$ operations in $K$ at precision the maximum precision of a coefficent in $M.$
\end{prop}
\begin{proof}
Let us assume that $K$ is a finite extensions of $\mathbb{Q}_p$ or $k((X)).$
Inside the nested \textbf{for} loop, if we want to eliminate $p^{u_y} \varepsilon_y+O(p^{n_y})$ with pivot $p^{u_x} \varepsilon_x+O(p^{n_x}),$
with the $\varepsilon$'s being units,
the corresponding coefficient of the corresponding shear matrix is the lift (in $\mathbb{Z}, $  $k[X]$ or adequate extension) of $p^{u_y-u_x} \varepsilon_y \varepsilon_x^{-1} \mod \pi^{\min (n_x,n_y)}.$
Exactness follows directly. Over other fields, we can not lift, but the computations are still valid.
The rest is clear.
\end{proof}

\begin{rem} \label{rem:char_pol_from_hessenberg}
From an Hessenberg form of $M,$ it is well known
that one can compute the characteristic polynomial of 
$M$ in $O(n^3)$ operations in $K.$ See \cite{Cohen:2013} page
55 and 56.
However, this computation involves some divisions, and its
behaviour regarding to precision is not easy to
quantify.
\end{rem}


\subsection{Computation of the inverse}

In this Subsection, we prove that to compute the inverse of
a matrix over a CDVF $K$, SNF is precision-wise optimal in the flat-precision case.
We first recall what is the differential of the computation of the inverse of a matrix.

\begin{lem}
Let $u \: : \: GL_n (K) \rightarrow GL_n(K),$ $M \mapsto M^{-1}.$
Then for $M \in GL_n (K),$ $du(M) \cdot dM=M^{-1} dM M^{-1}.$
It is always surjective.
\end{lem}

We then have the following result about the loss in precision when computing the inverse.

\begin{prop}
Let $cond(M)$ be the valuation of the last invariant factor of $M.$
If $dM$ is a flat precision of $O(\pi^N)$ on $M$ then $M^{-1}$
can be computed at precision $O(\pi^{N-2cond(M)})$ by a \textbf{SNF} computation
and this lower-bound is optimal
at least when $N$ is large.
\end{prop}
\begin{proof}
The smallest valuation of a coefficient of $M^{-1}$ is $-cond(M).$
It is $-2cond(M)$ for $M^{-2}$ and it is then clear that $-2cond(M)$
can be obtained as the valuation of a coefficient of $du(M) \cdot dM$
and the smallest that can be achieved this way for $dM$ in a precision lattice
of flat precision. Hence the optimality of the bound given, at least when 
$N$ is large, thanks to Lemma 3.14 of \cite{caruso-roe-vaccon:14a}

Now, the computation of an SNF form over a CDVF was described in \cite{Vaccon-these}.
From $M$ known at flat precision $O(\pi^N),$ we can obtain an exact $\Delta$ and $P,Q$ 
known at precision at least $O(\pi^{N-cond(M)})$ with coefficients in $O_K$
and determinant in $O_K^*$ realizing an SNF form of $M.$
There is no loss in precision when computing $P^{-1}$ and $Q^{-1}.$
Now, computing $\Delta^{-1}$ exactly, its smallest coefficient being  of valuation $-cond(M),$
we obtain $M^{-1}=Q^{-1} \Delta^{-1} P^{-1}$ known at precision at least $O(\pi^{N-2cond(M)}),$
which concludes the proof.
\end{proof}



\subsection{$\com (X -H)$ and flat precision}

In this subsection, we want to compute $\com (X Id -H)$
by using the SNF computation of the previous subsection.
As it is, in a way, optimal for the computation of the
inverse in a CDVF, we hope to get reasonnable behaviour
for the computation of $\com (X-M)$ even though the coefficients
are not, \textit{a priori}, set to live in a CDVF.
We show that we can reduce this task to the computation
of an SNF over a CDVF, with no division in $K.$

Firstly, we need a lemma to relate comatrices of
similar matrices:

\begin{lem} \label{lem:comatrix_of_similar}
If $M_1,M_2 \in M_n(K)$ and $P \in GL_n (K)$ are such that
$M_1=PM_2P^{-1},$ then:
\[ \com (X-M_1)=P \com (X-M_2) P^{-1}. \] 
\end{lem}

The second ingredient we need is the reciprocal polynomial.
We extend its definition to matrices of polynomials.
\begin{deftn}
Let $P \in K[X]$ be of degree $d \in \mathbb{N}_{>0}.$ 
We define its reciprocal polynomial as $P^{rec}=X^d P \left( 1/X \right).$
Let $A \in M_n(K[X]).$ We denote by $A^{rec}$ the matrix
whose coefficients are the reciprocal polynomials of the
coefficients of $A.$ 
\end{deftn}
We then have the following result :
\begin{lem}
Let $M \in M_n(K).$ Then:
\begin{eqnarray*}
com(1-XM)^{rec}=&com(X-M), \\
(\chi_M \times 1)^{rec}=&(1-XM) com(1-XM).\\
\end{eqnarray*}
\end{lem}

This lemma allows us to compute $com(1-XM)$ instead of $com(X-M).$
This has a remarkable advantage: the pivots during the computation of
the SNF of $com(1-XM)$ are units of $O_K[[X]],$ and are known
in advance to be on the diagonal. This leads to a very smooth
precision and complexity behaviour. 

\noindent\hrulefill

\noindent {\bf Algorithm 2:} {\tt Approximate $\com (X -H)$ }

\noindent{\bf Input:} an approximate Hessenberg matrix $H$ in $M_n(O_K).$

\smallskip

\noindent 0.\ $U:=1-XH.$ $U_0:=1-XH.$

\noindent 1.\ \textbf{Track} the following operations to get $P^{-1}$ and $Q^{-1},$
such that $U=PU_0Q.$


\noindent 2.\ {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 3.\  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,j},$ for $j\geq i+1$ 
using the invertible pivot
$U_{i,i}=1+XL_{i,i} \mod X^{n+1}$ (with $L_{i,i} \in \Zp [X]$). 

\noindent 4.\    {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 5. \  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,i+1},$
using the invertible pivot $U_{i,i}.$

\noindent 6. \ $\psi:=\prod_i U_{i,i}.$

\noindent 7. \ Rescale to get $U = Id \mod X^{n+1}.$

\noindent 8. \ $V:=\psi \times P^{-1} \times Q^{-1}   \mod X^{n+1}.$

\noindent 9. \ \textbf{Return} $V^{rec}, \psi^{rec}.$

\vspace{-1ex}\noindent\hrulefill

\medskip

\begin{theo}
Let $H \in M_n(O_K)$ be an approximate Hessenberg matrix.
Then, using Algorithm 2, one can compute $\com (X -H)$ in 
$\softO (n^3)$ operations in $O_K$ at the precision given by $H.$
\end{theo}
\begin{proof}
Firstly, the operations of the lines 2 and 3 are in $\softO (n^3)$ operations
in $O_K$ at the precision given by $H.$
Indeed, since $H$ is an approximate matrix, when we use $U_{i,i}$ as pivot:
on its columns, only $U_{i+1,i}$ can be a non zero-approximate coefficient.
As a consequence, when performing this column-pivoting, only two rows ($i$ and
$i+1$) lead to operations in $O_K [[K]]$ other than checking precision.
Hence, line 3 costs $\softO (n^2)$ for the computation of $U.$
Following line 1, the computation of $Q^{-1}$ is done by operations on rows, starting from the identity matrix.
Corresponding coefficient for the corresponding shear matrix
 is the opposite of the one defined by pivoting
and the indices for the receiving and giving rows are swapped compared to the operations on columns.
Thus, we can show that it requires no additional operation in $O_K[[X]].$ $Q^{-1}$ is just filled as
an upper-triangular matrix.
This proves that the resulting cost for lines 2 and 3 is indeed in $\softO (n^3)$ operations.

As for line 4 and 5, there are only $n-1$ eliminations, resulting again in a $\softO (n^3)$ cost
for the computation of $P^{-1}$ and $U.$

Line 6 is in $\softO (n^2)$ and 7 in $\softO (n^3).$

Thanks to the fact that the $P$ only corresponds to the product of $n-1$ 
shear matrices, the
product on line 8 is in $\softO (n^3).$
We emphasize that no division has been done throughout the algorithm.
Line 9 is costless, and the result is then proved.
\end{proof}



\begin{prop} \label{prop:optimal_flat}
If $M \in M_n(O_K)$ is known at flat precision $dM$ (high enough), then 
we can compute the optimal jagged precision on $\chi (M)$
in $\softO (n^3)$ operations in $O_K$ at the initial flat precision.
\end{prop}
\begin{proof}
Computing $M=PHP^{-1},$ an approximate Hessenberg form, can be done in
$O(n^3)$ with no division.
Computing $\com (X-H)$ can be done in $\softO (n^3)$ with no division.
Then, by Lemma \ref{lem:comatrix_of_similar}, $tr(\com (X-M) \cdot dM)=tr(P\com (X-H)P^{-1} \cdot dM).$
thus, \[tr(\com (X-M) \cdot dM)=tr(\com (X-H) P^{-1}  dM P). \]
$dM$ being flat, so is $P^{-1}  dM P$ with same precision, 
and the jagged precision on $\chi (M)$ can then directly be read on $\com (X-H).$
\end{proof}

\subsection{Computation and optimal jagged precision}

When this time $dM$ is jagged, the trick of the proof of Proposition
\ref{prop:optimal_flat} can not work anymore.
Indeed, the computation of $P^{-1}  dM P$ and then of $\com (X-H) P^{-1}  dM P$
are not easy, as $P^{-1}  dM P$ is not jagged anymore.
Fortunately, Proposition~\ref{prop:factom_com} will be enough to 
conclude that optimal precision can still be read in $\softO (n^3)$ 
operations in $O_K.$


%Thanks to the previous results, we can now compute 
%$\com (X-M) \mod (\chi(M))$ in average complexity $\softO (n^3),$
%using the following algorithm.

\noindent\hrulefill

\noindent {\bf Algorithm 3:} {\tt Approximate $\com (X -M)$ }

\noindent{\bf Input:} an approx. $M \in M_n(O_K),$ with $discr(\chi_M) \neq 0.$ 

\smallskip

\noindent 0.\ Find $P \in GL_n(O_K)$ and $H \in M_n(O_K),$ approximate Hessenberg,
such that $M=PHP^{-1},$ using Algorithm 1. 

\noindent 1.\ Compute $A=\com (X-H)$ using Algorithm 2.


\noindent 2.\ Do $row(A,1) \leftarrow row(A,1)+\sum_{i=2}^n \mu_i row(A,i),$ for
random $\mu_i \in O_K,$ by doing $T \times A$ for some $T \in GL_n(O_K).$
Compute $B:=TAT^{-1}.$

\noindent 3.\ Similarily compute $C:=S^{-1}BS$ for $S \in GL_n(O_K)$ corresponding to
sending a random linear combination of the columns of index $j \in \llbracket 2,n \rrbracket$
to the first column of $B.$ 

\noindent 4.\  \textbf{If} $gcd(C_{1,1}, \chi(M)) \neq 1,$ \textbf{then} go to 2.

\noindent 5. \ Compute $U,V \in O_K[X]^n$ such that $C=U {}^t V \mod \chi (M),$
using Proposition \ref{prop:factor_com}.

\noindent 6. \ Return $com(X-M):=(PT^{-1}S U \times V S^{-1} T P^{-1}) \mod \chi (M).$

\vspace{-1ex}\noindent\hrulefill

\medskip


\begin{theo}
Algorithm 3 compute 
$\com (X-M) \mod (\chi(M))$ in average complexity $\softO (n^3)$
operations in $O_K$ at initial precision.
\end{theo}
\begin{proof}
As we have already seen, completing Step 1 and 2 is in $\softO (n^3).$
Multiplying by $T$ or $S$ or their inverse corresponds
to $n$ operations on rows or columns over a matrix with coefficients
in $O_K[X]$ of degree at most $n.$
Thus, it is in $\softO (n^3).$
Step 5 and 6 are also in $\softO (n^3).$
All that is to prove is that the set of $P$ and $S$ to avoid
is of dimension at most $n-1.$
As in the proof of \ref{prop:factor_com}, we can work modulo $X-\lambda$
for $\lambda$ a root of $\chi (M)$ (in an algebraic closure)
and then apply Chinese Remainder Theorem.
The goal of the Step $2$ is to ensure the first row of $B$ contains an
invertible modulo $\chi (M).$
Since $A(\lambda)$ is of rank one, the $\mu_i$'s have to avoid an
affine hyperplane so that $row(B,1) \mod (X-\lambda)$ is a non-zero vector.
Hence for $row(B,1) \mod \chi (M)$ to contain an invertible coefficient,
a finite union of affine hyperplane is to avoid.
Similarily, the goal of Step 3 is to put an invertible coefficient (modulo
$\chi(M)$) on $C_{1,1},$ and again, only a finite union of affine
hyperplane is to avoid.
Hence, the set that the $\mu_i$'s have to avoid is a finite union
of hyperplane, and hence, is of dimension at most $n-1.$
Thus, almost any $\mu_i$ lead to a matrix $C$ passing the test
on Step 4, which concludes the proof.
\end{proof}

\begin{rem}
If $dM$ is jagged, we can read the optimal
jagged precision from $\com (X-M).$
Indeed, we can write $\com (X-M)=\sum_{i=1}^{n-1} A_i X^i$
for some matrices $A_i \in M_n(O_K).$
Then the best jagged precision on the coefficient of 
$X^i$ can be read from $tr (A_i dM).$
What then is to do is to find the smallest 
$val((A_i)_{j,k} dM_{k,j}).$
\end{rem}

\begin{rem}
As a consequence of the previous remark, once the optimal jagged
precision is known, it is possible to lift arbitrarily
the precision on $M$ and then to perform
a computation of the characteristic polynomial
from an Hessenberg form as in \ref{rem:char_pol_from_hessenberg},
in $O(n^3).$
By truncating afterward at the optimal jagged precision,
we can achieve by this way this optimal precision.
\end{rem}

\section{Differential\\via Frobenius form}

The algorithm designed in the previous section computes the differential 
$d \chi_M$ of $\chi$ at a given matrix $M \in M_n(K)$ for a cost of 
$O(n^3)$ operations in $K$. This seems to be optimal given that the 
(naive) size of the $d \chi_M$ is $n^3$: it is a matrix of size $n
\times n^2$. It turns out however that improvements are still possible!

Precisely, we will prove in this section that $d \chi_M$ has a very 
special form, allowing a compact encoding using only $O(n^2)$ coefficients. 
We shall moreover design fast algorithms (with complexity $\softO
(n^\omega)$) for computing it. The price to pay is that divisions in
$K$ appear, which can be an issue regarding to precision in particular
cases.

From now on, we fix a matrix $M \in M_n(K)$ for which $d \chi_M$ is 
surjective.

\subsection{Fast computation}

We now move to algorithms and show that the quadruple $(\alpha, P, Q, 
\chi_M)$ can be computed for a cost of $\softO(n^\omega)$ operations
in the ground field $K$ with high probability.

The matrix $P$ can be computed as follows. Pick $c \in K^n$. Define 
$c_i = M^i c$ for all $i \geq 1$. The $c_i$'s can be computed in 
$\softO(n^\omega)$ operations in $K$ using \cite{}. Let $P_\inv$ be the 
$n \times n$ matrix whose rows are the $c_i$'s for $1 \leq i \leq n$. 
Remark that $P_\inv$ is invertible if and only if $(c_0, c_1, \ldots, 
c_{n-1})$ is a basis of $K^n$ if and only if $c$ is a cyclic vector. 
Moreover after base change to the basis $(c_0, \ldots, c_{n-1})$, the matrix 
$M$ takes the shape \eqref{eq:companion}. In other words, if $P_\inv$
is invertible, then $P = P_\inv^{-1}$ is a solution of $M = P N P^{-1}$.
Observe moreover that the condition ``$P_\inv$ is invertible'' is open
for the Zariski topology. It then happens with high probability as soon
as it is not empty, that is as soon as $M$ admits a cyclic vector, which
holds by assumption.

The characteristic polynomial $\chi_M$ can be recovered thanks to the
relation $(a_0, a_1, \ldots, a_{n-1}) = -c_n \cdot P$.

Now, instead of computing directly $Q$, we first compute a matrix $R$ 
with the property that $N^t = R N R^{-1}$. For that, we apply the same
strategy as above except that we start with the vector $e = (1, 0, 
\ldots, 0)$ (and not with a random vector). A simple computation shows
that, for $1 \leq i \leq n{-}1$, the vector $N^i e$ has the shape:
$$N^i e = (0, \ldots, 0, -a_0, \star, \ldots, \star)$$
with $n{-}i$ starting zeros. Therefore the $N^i e$'s form a basis of
$K^n$, \emph{i.e.} $e$ is always a cyclic vector of $N$. Once $R$ has
been compute, we recover $Q$ using the relation $Q = P_\inv^t R$.

It remains to compute the scaling factor $\alpha$. For this, we write
the relation:
\begin{equation}
\label{eq:shortcomN}
\com(X{-}N) = \alpha \cdot V^t \cdot V R^t \mod \chi_M
\end{equation}
which comes from Eq.~\eqref{eq:shortcom} after multiplication on the 
left by $P^{-1}$ and multiplication on the right by $P$. We observe
moreover that the first row of $R$ is $(1, 0, \ldots, 0)$. Evaluating
the top left entry of Eq.~\eqref{eq:shortcomN}, we end up with the 
relation:
$$\alpha = a_1 + a_2 X + \cdots + a_{n-1} X^{n-2} + X^{n-1}.$$
No further computation are then needed to derive the value of $\alpha$.



\subsection{Optimal jagged precision}

\section{Eigenvalues}
\label{sec:eigenvalues}

\subsection{Optimal precision on a simple eigenvalue}

\todo{Optimal precision on $P(\lambda_i)$ and on $\lambda_i$}

\subsection{Legendre precision}

\todo{Describe Legendre precision and show that it is optimal for
a diagonalizable matrix over $\Zp$.}

%\subsection{Algorithm}

\bibliographystyle{plain}
\bibliography{charpoly}

\end{document}
