\documentclass{sig-alternate-05-2015}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\hyphenation{regarding}

\usepackage{amsmath,amssymb}
\usepackage{amsthmnoproof}
\usepackage{amsrefs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\com}{Com}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lcm}{lcm}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\newcommand{\OK}{\mathcal{O}_K}

\newcommand{\softO}{O\tilde{~}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\CopyrightYear{2016}
\setcopyright{acmcopyright}
\conferenceinfo{ISSAC '16,}{July 19-22, 2016, Waterloo, ON, Canada}
\isbn{978-1-4503-4380-0/16/07}\acmPrice{\$15.00}
\doi{http://dx.doi.org/10.1145/2930889.2930898}

\permission{Publication rights licensed to ACM. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only.}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\title{Characteristic polynomial of p-adic matrices}

\numberofauthors{3}
\author{
\alignauthor Xavier Caruso\\
  \affaddr{Universit\'e Rennes 1}\\
  \affaddr{\textsf{xavier.caruso@normalesup.org}}
\alignauthor David Roe\\
  \affaddr{Pittsburg University}\\
  \affaddr{\textsf{roed@pitt.edu}}
\alignauthor Tristan Vaccon\\
  \affaddr{Universit\'e de Limoges}\\
  \affaddr{\textsf{tristan.vaccon@unilim.fr}}
}

\maketitle

\begin{abstract}
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010148.10010149.10010150</concept_id>
<concept_desc>Computing methodologies~Algebraic algorithms</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\vspace{-1mm}
\ccsdesc[500]{Computing methodologies~Algebraic algorithms}
\printccsdesc

\vspace{-1.5mm}
\keywords{Algorithms, $p$-adic precision, Newton polygon, factorization}

%\vspace{1mm}
% \noindent
% {\bf Categories and Subject Descripto\RS:} \\
%\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
%  Manipulation -- \emph{Algebraic Algorithms}
%
% \vspace{1mm}
% \noindent
% {\bf General Terms:} Algorithms, Theory
%
% \vspace{1mm}
% \noindent
% {\bf Keywords:} $p$-adic precision, linear algebra, ultrametric analysis
%\medskip

\section{Introduction}

\todo{Example of $M$ and $M + 1$}

\subsection{Notation} Throughout the paper, $K$ will refer to a complete,
discrete valuation field, $\val : K \twoheadrightarrow \Z \cup \{+\infty\}$ to its valuation,
$\OK$ its ring of integers and $\pi$ a uniformizer.

\section{Theoretical study}

\subsection{The theory of p-adic precision}

We recall some of the definitions and results of \cite{caruso-roe-vaccon:14a}
as a foundation for our discussion of the precision for the characteristic polynomial
of a matrix.  We will be concerned with two $K$-manifolds in what follows:
the space $M_n(K)$ of $n \times n$ matrices with entries in $K$ and the space
$K_n[X]$ of monic degree $n$ polynomials over $K$.  Given a matrix $M \in M_n(K)$,
the most general kind of precision structure we may attach to $M$ is a
\emph{lattice} $H$ in the tangent space at $M$.  However, representing an
arbitrary lattice requires $n^2$ basis vectors, each with $n^2$ entries.  We therefore
frequently work with certain classes of lattices, either \emph{jagged} lattices
where we specify a precision for each matrix entry or \emph{flat} lattices where
every entry is known to a fixed precision $O(p^m)$.  Similarly, precision for
monic polynomials can be specified by giving a lattice in the tangent space
at $f(X) \in K_n[X]$, or restricted to jagged or flat precision in the interest
of simplicity.

The main benefit of working with lattices is the following result
\citelist{\cite{caruso-roe-vaccon:14a}*{Lem. 3.4} \cite{caruso-roe-vaccon:15a}*{Prop. 2.2}}.


%\begin{prop}

%\end{prop}

\todo{Recall the Lemma (with integral version)}

\subsection{Computation of the differential}

\todo{Formula with comatrix.}

\subsection{Properties of the image of $d \chi$}

\todo{Surjectivity. Stability under multiplication by $X$ mod $\chi$.}

\todo{Image modulo $p$ and elementary divisors. 
Consequences: (1)~computation in $\softO(n^\omega)$ and (2)~gain criterium.}

\section{Computation of $\chi$ and $d \chi$}

In this section, we combine the computation of an Hessenberg form
of a matrix and the computation of the inverse through the SNF
over a complete discrete valuation field
to compute $\com (X-A)$ and $d \chi.$
%The computation of an Hessenberg form provides an $O(n^3)$
%algorithm to compute the characteristic polynomial.

\subsection{Hessenberg form}

\todo{Describe algorithm (with matrices and endormophisms?). 
Remark about precision.}

\begin{deftn}
$M \in M_n (k)$ is said to be an Hessenberg matrix if
\[\forall i \in \llbracket 3, n \rrbracket, 
\forall j \in \llbracket 1, i-2 \rrbracket, M_{i,j}=0. \]
If $M \in M_n (k)$ and $H \in M_n (k)$ is an Hessenberg 
matrix such that there exists $P \in GL_n(k)$
with $H=PMP^{-1},$ we say that H is an Hessenberg form
of $M.$
\end{deftn}

\begin{deftn}
In the previous definition of an Hessenberg matrix $M,$
 if we only have $M_{i,j}= O(\pi^{n_{i,j}}),$ then we say
 that $M$ is an approximate Hessenberg matrix.
 This generalize directly to approximate Hessenberg form. 
\end{deftn}

It is not hard to prove that every matrix over a field admits
an Hessenberg form.
We prove here that over $K,$ if a matrix is 
known at finite (jagged) precision,
we can compute an approximate Hessenberg form of it.
Moreover, we can provide an exact change of basis matrix.
It relies on the following algorithm.


\noindent\hrulefill

\noindent {\bf Algorithm 1:} {\tt Approximate Hessenberg form computation}

\noindent{\bf Input:} a matrix $M$ in $M_n(K).$

\smallskip

\noindent 0.\ $P:=I_n.$ \: $H:=M.$


\noindent 1.\ {\bf for} $j=1,\dots,n-1$ {\bf do} 

\noindent 2.\  \:  {\bf swap} the row $j+1$ with a row $i_{min}$ ($i_{min} \geq 2$) s.t. $val(H_{i_{min},j})$ is minimal. 

\noindent 3.\  \:  {\bf for} $i=j+2,\dots,n$ {\bf do} 

\noindent 4. \ \: \:  \textbf{Eliminate} the significant digits of $H_{i,j}$ by pivoting with row $j+1$ 
using a matrix $T.$

\noindent 5. \ \: \:  $H:=H \times T^{-1}.$ \: $P:=T \times P.$

\noindent 6. \textbf{Return} $H,P.$

\vspace{-1ex}\noindent\hrulefill

\medskip



\begin{prop} 
Algorithm 1 computes $H$ and $P$ realizing an approximate Hessenberg form of $M.$
$P$ is exact over finite extensions of $\mathbb{Q}_p$ and $k((X))$, and the computation is in $O(n^3)$ operations in $K$ at precision the maximum precision of a coeffient in $M.$
\end{prop}
\begin{proof}
Let us assume that $K$ is a finite extensions of $\mathbb{Q}_p$ or $k((X)).$
Inside the nested \textbf{for} loop, if we want to eliminate $p^{u_y} \varepsilon_y+O(p^{n_y})$ with pivot $p^{u_x} \varepsilon_x+O(p^{n_x}),$
with the $\varepsilon$'s being units,
the coefficient of the transvection matrix is the lift (in $\mathbb{Z}, $  $k[X]$ or adequate extension) of $p^{u_y-u_x} \varepsilon_y \varepsilon_x^{-1} \mod \pi^{\min (n_x,n_y)}.$
Exactness follows directly. Over other fields, we can not lift, but the computations are still valid.
The rest is clear.
\end{proof}

\begin{rem} \label{rem:char_pol_from_hessenberg}
From an Hessenberg form of $M,$ it is well known
that one can compute the characteristic polynomial of 
$M$ in $O(n^3)$ operations in $K.$ See \todo{cite Cohen} page
55 and 56.
However, this computation involves some divisions, and its
behaviour regarding to precision is not easy to
quantify.
\end{rem}


\subsection{Computation of the inverse}

In this Subsection, we prove that to compute the inverse of
a matrix over a CDVF $K$, SNF is precision-wise optimal in the flat-precision case.
We first recall what is the differential of the computation of the inverse of a matrix.

\begin{lem}
Let $u \: : \: GL_n (K) \rightarrow GL_n(K),$ $M \mapsto M^{-1}.$
Then for $M \in GL_n (K),$ $du(M) \cdot dM=M^{-1} dM M^{-1}.$
It is always surjective.
\end{lem}

We then have the following result about the loss in precision when computing the inverse.

\begin{prop}
Let $cond(M)$ be the valuation of the last invariant factor of $M.$
If $dM$ is a flat precision of $O(\pi^N)$ on $M$ then $M^{-1}$
can be computed at precision $O(\pi^{N-2cond(M)})$ by a \textbf{SNF} computation
and this lower-bound is optimal
at least when $N$ is large.
\end{prop}
\begin{proof}
The smallest valuation of a coefficient of $M^{-1}$ is $-cond(M).$
It is $-2cond(M)$ for $M^{-2}$ and it is then clear that $-2cond(M)$
can be obtained as the valuation of a coefficient of $du(M) \cdot dM$
and the smallest that can be achieved this way for $dM$ in a precision lattice
of flat precision. Hence the optimality of the bound given, at least when 
$N$ is large, thanks to \todo{Reference for precision lemma}.

Now, the computation of an SNF form over a CDVF was described in \todo{reference these Vaccon}.
From $M$ known at flat precision $O(\pi^N),$ we can obtain an exact $\Delta$ and $P,Q$ 
known at precision at least $O(\pi^{N-cond(M)})$ with coefficients in $O_K$
and determinant in $O_K^*$ realizing an SNF form of $M.$
There is no loss in precision when computing $P^{-1}$ and $Q^{-1}.$
Now, computing $\Delta^{-1}$ exactly, its smallest coefficient being  of valuation $-cond(M),$
we obtain $M^{-1}=Q^{-1} \Delta^{-1} P^{-1}$ known at precision at least $O(\pi^{N-2cond(M)}),$
which concludes the proof.
\end{proof}



\subsection{$\com (X -H)$ and flat precision}

In this subsection, we want to compute $\com (X Id -H)$
by using the SNF computation of the previous subsection.
As it is, in a way, optimal for the computation of the
inverse in a CDVF, we hope to get reasonnable behaviour
for the computation of $\com (X-M)$ even though the coefficients
are not, \textit{a priori}, set to live in a CDVF.
We show that we can reduce this task to the computation
of an SNF over a CDVF, with no division in $K.$

Firstly, we need a lemma to relate comatrices of
similar matrices:

\begin{lem} \label{lem:comatrix_of_similar}
If $M_1,M_2 \in M_n(K)$ and $P \in GL_n (K)$ are such that
$M_1=PM_2P^{-1},$ then:
\[ \com (X-M_1)=P \com (X-M_2) P^{-1}. \] 
\end{lem}

The second ingredient we need is the reciprocal polynomial.
We extend its definition to matrices of polynomials.
\begin{deftn}
Let $A \in M_n(K[X]).$ We denote by $A^{rec}$ the matrix
whose coefficients are the reciprocal polynomials of the
coefficients of $A.$ 
\end{deftn}
We then have the following result :
\begin{lem}
Let $M \in M_n(K).$ Then:
\begin{eqnarray*}
com(1-XM)^{rec}=&com(X-M), \\
(\chi_M \times 1)^{rec}=&(1-XM) com(1-XM).\\
\end{eqnarray*}
\end{lem}

This lemma allows us to compute $com(1-XM)$ instead of $com(X-M).$
This has a remarkable advantage: the pivots during the computation of
the SNF of $com(1-XM)$ are units of $O_K[[X]],$ and are known
in advance to be on the diagonal. This leads to a very smooth
precision and complexity behaviour. 

\noindent\hrulefill

\noindent {\bf Algorithm 2:} {\tt Approximate $\com (X -H)$ }

\noindent{\bf Input:} an approximate Hessenberg matrix $H$ in $M_n(O_K).$

\smallskip

\noindent 0.\ $U:=1-XH.$ $U_0:=1-XH.$

\noindent 1.\ \textbf{Track} the following operations to get $P^{-1}$ and $Q^{-1},$
such that $U=PU_0Q.$


\noindent 2.\ {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 3.\  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,j},$ for $j\geq i+1$ 
using the invertible pivot
$U_{i,i}=1+XL_{i,i} \mod X^{n+1}$ (with $L_{i,i} \in \Zp [X]$). 

\noindent 4.\    {\bf for} $i=1,\dots,n-1$ {\bf do} 

\noindent 5. \  \:  \textbf{Eliminate}, modulo $X^{n+1}$ the coefficients $U_{i,i+1},$
using the invertible pivot $U_{i,i}.$

\noindent 6. \ $\psi:=\prod_i U_{i,i}.$

\noindent 7. \ Rescale to get $U = Id \mod X^{n+1}.$

\noindent 8. \ $V:=\psi \times P^{-1} \times Q^{-1}   \mod X^{n+1}.$

\noindent 9. \ \textbf{Return} $V^{rec}, \psi^{rec}.$

\vspace{-1ex}\noindent\hrulefill

\medskip

\begin{theo}
Let $H \in M_n(O_K)$ be an approximate Hessenberg matrix.
Then, using Algorithm 2, one can compute $\com (X -H)$ in 
$\softO (n^3)$ operations in $O_K$ at the precision given by $H.$
\end{theo}
\begin{proof}
Firstly, the operations of the lines 2 and 3 are in $\softO (n^3)$ operations
in $O_K$ at the precision given by $H.$
Indeed, since $H$ is an approximate matrix, when we use $U_{i,i}$ as pivot:
on its columns, only $U_{i+1,i}$ can be a non zero-approximate coefficient.
As a consequence, when performing this column-pivoting, only two rows ($i$ and
$i+1$) lead to operations in $O_K [[K]]$ other than checking precision.
Hence, line 3 costs $\softO (n^2)$ for the computation of $U.$
Following line 1, the computation of $Q^{-1}$ is done by operations on rows, starting from the identity matrix.
Coefficient for the transvection is the opposite of the one defined by pivoting
and the indices for the receiving and giving rows are swapped compared to the operations on columns.
Thus, we can show that it requires no additional operation in $O_K[[X]].$ $Q^{-1}$ is just filled as
an upper-triangular matrix.
This proves that the resulting cost for lines 2 and 3 is indeed in $\softO (n^3)$ operations.

As for line 4 and 5, there are only $n-1$ eliminations, resulting again in a $\softO (n^3)$ cost
for the computation of $P^{-1}$ and $U.$

Line 6 is in $\softO (n^2)$ and 7 in $\softO (n^3).$

Thanks to the fact that the $P$ only corresponds to the product of $n-1$ transvections, the
product on line 8 is in $\softO (n^3).$
We emphasize that no division has been done throughout the algorithm.
Line 9 is costless, and the result is then proved.
\end{proof}



\begin{prop} \label{prop:optimal_flat}
If $M \in M_n(O_K)$ is known at flat precision $dM$ (high enough), then 
we can compute the optimal jagged precision on $\chi (M)$
in $\softO (n^3)$ operations in $O_K$ at the initial flat precision.
\end{prop}
\begin{proof}
Computing $M=PHP^{-1},$ an approximate Hessenberg form, can be done in
$O(n^3)$ with no division.
Computing $\com (X-H)$ can be done in $\softO (n^3)$ with no division.
Then, by Lemma \ref{lem:comatrix_of_similar}, $tr(\com (X-M) \cdot dM)=tr(P\com (X-H)P^{-1} \cdot dM).$
thus, \[tr(\com (X-M) \cdot dM)=tr(\com (X-H) P^{-1}  dM P). \]
$dM$ being flat, so is $P^{-1}  dM P$ with same precision, 
and the jagged precision on $\chi (M)$ can then directly be read on $\com (X-H).$
\end{proof}

%\subsection{$\com (X -M)$ and jagged precision}


\subsection{Factorization}

When this time $dM$ is jagged, the trick of the proof of Proposition
\ref{prop:optimal_flat} can not work anymore.
Indeed, the computation of $P^{-1}  dM P$ and then of $\com (X-H) P^{-1}  dM P$
are not easy, as $P^{-1}  dM P$ is not jagged anymore.
Fortunately, a factorization result of $\com (X-M)$ will be enough
to conclude that optimal precision can still be read in $\softO (n^3)$
operations in $O_K.$ 

\begin{prop} \label{prop:factor_com}
Let $M \in M_n (O_K)$ be such that $discr(\chi (M)) \neq 0.$
We assume that $A=\com (X-M)$ is such that $gcd(A_{1,1}, \chi (M))=1$
(in $K[X]$).
Then for $U=col(A,1)$ and $V=A_{1,1}^{-1} row(A,1) \mod \chi (M),$
both in $O_K[X]^n,$ we have: 
\[ \com (X-M)= U \cdot {}^t V \mod \chi (M).\]
\end{prop}
\begin{proof}
We first prove that for all $i,j \in \llbracket 1, n \rrbracket,$ we have
\begin{equation}A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod \chi (M). \label{eqn:2-minors-com} \end{equation}
Let $\lambda$ be a root of $\chi (M)$ in an algebraic closure of $K.$
Then $A(\lambda)$ is of rank one.
Indeed, $A(\lambda) \times (\lambda-M) =0.$
Since $\chi (M)$ is square-free, then $\lambda-M$ is of rank $n-1$
and then $A(\lambda)$ is of rank at most one.
Nevertheless, we can remark that
 $gcd(A_{1,1}, \chi (M))=1,$ $A_{1,1} \neq 0 \mod (X-\lambda)$
and thus, $A(\lambda)$ is of rank exactly one.
As a consequence, all $2 \times 2$-minors of $A(\lambda)$ are zero.
It means that \[A_{1,1} A_{i,j}=A_{1,i}A_{1,j} \mod (X-\lambda). \]
By the Chinese Remainder Theorem, our first result is then proved.

Now, let us take $U=col(A,1)$ the first column of $A$ and
$V=A_{1,1}^{-1} row(A,1) \mod \chi (M)$ (as $A_{1,1}$ is
invertible $\mod \chi (M)).$
Then thanks to \eqref{eqn:2-minors-com}, $U$ and $V$ satisfy the desired equality. 
\end{proof}

\begin{rem}
If $M \in M_n (O_K)$ is such that for some $U,V \in O_{K}[X]^n$
we have $\com (X-M)= U \cdot {}^t V \mod \chi (M)$ and if 
$P\in GL_n (O_K)$ then
\[\com (X-PMP^{-1})=(PU) \cdot ( {}^tV P^{-1}) \mod \chi(M).\]
In other words, factorizability of $\com(X-M)$ is stable
by change of basis.
\end{rem}







\subsection{Computation and optimal jagged precision}

Thanks to the previous results, we can now compute 
$\com (X-M) \mod (\chi(M))$ in average complexity $\softO (n^3),$
using the following algorithm.

\noindent\hrulefill

\noindent {\bf Algorithm 3:} {\tt Approximate $\com (X -M)$ }

\noindent{\bf Input:} an approx. $M \in M_n(O_K),$ with $discr(\chi_M) \neq 0.$ 

\smallskip

\noindent 0.\ Find $P \in GL_n(O_K)$ and $H \in M_n(O_K),$ approximate Hessenberg,
such that $M=PHP^{-1},$ using Algorithm 1. 

\noindent 1.\ Compute $A=\com (X-H)$ using Algorithm 2.


\noindent 2.\ Do $row(A,1) \leftarrow row(A,1)+\sum_{i=2}^n \mu_i row(A,i),$ for
random $\mu_i \in O_K,$ by doing $T \times A$ for some $T \in GL_n(O_K).$
Compute $B:=TAT^{-1}.$

\noindent 3.\ Similarily compute $C:=S^{-1}BS$ for $S \in GL_n(O_K)$ corresponding to
sending a random linear combination of the columns of index $j \in \llbracket 2,n \rrbracket$
to the first column of $B.$ 

\noindent 4.\  \textbf{If} $gcd(C_{1,1}, \chi(M)) \neq 1,$ \textbf{then} go to 2.

\noindent 5. \ Compute $U,V \in O_K[X]^n$ such that $C=U {}^t V \mod \chi (M),$
using Proposition \ref{prop:factor_com}.

\noindent 6. \ Return $com(X-M):=(PT^{-1}S U \times V S^{-1} T P^{-1}) \mod \chi (M).$

\vspace{-1ex}\noindent\hrulefill

\medskip


\begin{theo}
Algorithm 3 compute 
$\com (X-M) \mod (\chi(M))$ in average complexity $\softO (n^3)$
operations in $O_K$ at initial precision.
\end{theo}
\begin{proof}
As we have already seen, completing Step 1 and 2 is in $\softO (n^3).$
Multiplying by $T$ or $S$ or their inverse corresponds
to $n$ operations on rows or columns over a matrix with coefficients
in $O_K[X]$ of degree at most $n.$
Thus, it is in $\softO (n^3).$
Step 5 and 6 are also in $\softO (n^3).$
All that is to prove is that the set of $P$ and $S$ to avoid
is of dimension at most $n-1.$
As in the proof of \ref{prop:factor_com}, we can work modulo $X-\lambda$
for $\lambda$ a root of $\chi (M)$ (in an algebraic closure)
and then apply Chinese Remainder Theorem.
The goal of the Step $2$ is to ensure the first row of $B$ contains an
invertible modulo $\chi (M).$
Since $A(\lambda)$ is of rank one, the $\mu_i$'s have to avoid an
affine hyperplane so that $row(B,1) \mod (X-\lambda)$ is a non-zero vector.
Hence for $row(B,1) \mod \chi (M)$ to contain an invertible coefficient,
a finite union of affine hyperplane is to avoid.
Similarily, the goal of Step 3 is to put an invertible coefficient (modulo
$\chi(M)$) on $C_{1,1},$ and again, only a finite union of affine
hyperplane is to avoid.
Hence, the set that the $\mu_i$'s have to avoid is a finite union
of hyperplane, and hence, is of dimension at most $n-1.$
Thus, almost any $\mu_i$ lead to a matrix $C$ passing the test
on Step 4, which concludes the proof.
\end{proof}

\begin{rem}
If $dM$ is jagged, we can read the optimal
jagged precision from $\com (X-M).$
Indeed, we can write $\com (X-M)=\sum_{i=1}^{n-1} A_i X^i$
for some matrices $A_i \in M_n(O_K).$
Then the best jagged precision on the coefficient of 
$X^i$ can be read from $tr (A_i dM).$
What then is to do is to find the smallest 
$val((A_i)_{j,k} dM_{k,j}).$
\end{rem}

\begin{rem}
As a consequence of the previous remark, once the optimal jagged
precision is known, it is possible to lift arbitrarily
the precision on $M$ and then to perform
a computation of the characteristic polynomial
from an Hessenberg form as in \ref{rem:char_pol_from_hessenberg},
in $O(n^3).$
By truncating afterward at the optimal jagged precision,
we can achieve by this way this optimal precision.
\end{rem}

\section{The case of diagonalizable matrices}

\subsection{Legendre precision}

\todo{Describe Legendre precision and show that it is optimal for
a diagonalizable matrix over $\Zp$.}

\subsection{Algorithm}


\bibliographystyle{plain}
\bibliography{../roebib/Biblio}

\end{document}
