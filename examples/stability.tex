\documentclass{sig-alternate}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage{amsrefs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\com}{Com}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
%\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}
\newtheorem{rmk}[theo]{Remark}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\renewcommand{\O}{\mathcal O}
\newcommand{\OK}{\mathcal{O}_K}
\newcommand{\XX}{\mathbf X}
\newcommand{\trans}{{}^{\text t}}
\newcommand{\T}{\mathcal{T}}

\renewcommand{\prec}{\text{\rm prec}}

\newcommand{\id}{\textrm{id}}
\newcommand{\Epi}{\textrm{Epi}}
\renewcommand{\c}{\text{\rm c}}

\newcommand{\DI}{\text{\rm DI}}
\newcommand{\II}{\text{\rm II}}

\newcommand{\lb}{\ensuremath{\llbracket}}
\newcommand{\rb}{\ensuremath{\rrbracket}}
\newcommand{\lp}{(\!(}
\newcommand{\rp}{)\!)}
\newcommand{\col}{\: : \:}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\title{About p-adic stability}

\numberofauthors{3}
\author{
\alignauthor Xavier Caruso\\
  \affaddr{Universit\'e Rennes 1}\\
  \email{\normalsize \textsf{xavier.caruso@normalesup.org}}
\alignauthor Tristan Vaccon\\
  \affaddr{Universit\'e Rennes 1}\\
  \email{\normalsize \textsf{tristan.vaccon@univ-rennes1.fr}}
\alignauthor David Roe \\
  \affaddr{University of Calgary}\\
  \email{\normalsize \textsf{roed.math@gmail.com}}
}

\maketitle

\begin{abstract}
Based on the framework developed in \cite{caruso-roe-vaccon:14a}, we study the 
$p$-adic stability of many classical algorithms involving many usual
algebraic objects (matrices, univariate polynomials,
vector spaces). We observe in particular that generic algorithms ---
which are designed to work over an arbitrary field --- are generally
very unstable.
\end{abstract}

\vspace{1mm}
 \noindent
 {\bf Categories and Subject Descriptors:} \\
\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
  Manipulation -- \emph{Algebraic Algorithms}

 \vspace{1mm}
 \noindent
 {\bf General Terms:} Algorithms, Theory

 \vspace{1mm}
 \noindent
 {\bf Keywords:} $p$-adic precision
\medskip

\section{Introduction}

For about twenty years, the use of $p$-adic methods in symbolic 
computation was gaining more and more popularity: they were succesfully 
used for instance 
to compute compose products of polynomials 
\cite{boston-gonzalez-perdry-schost:05a}, 
to produce hyperelliptic curves of genus $2$ with complex multiplication 
\cite{gaudry-houtmann-weng-ritzenthaler-kohel:06a},
to compute isogenies between elliptic curves \cite{lercier-sirvent:08a} 
and, last but not least,
to count points on certain varieties (mainly curves) using $p$-adic 
cohomologies (\emph{cf} \cite{kedlaya:01a,lauder:04a} and many
followers).
However, a general framework allowing a precise study of $p$-adic 
precision --- which is a main issue we encounter when trying to work 
with $p$-adic numbers --- was designed only recently in 
\cite{caruso-roe-vaccon:14a}. 

The present paper, which may be considered as a continuation of 
\cite{caruso-roe-vaccon:14a}, aims at using the theory of \emph{loc. 
cit.} in order to analyse the $p$-adic stability of many standard 
algorithms for computing with basic algebraic structures: numbers, 
matrices, univariate polynomials and vector spaces. For each considered 
algorithm, we will follow the same protocol: 
first, we will compute the theoretical loss of precision of the 
underlying question using the framework of \cite{caruso-roe-vaccon:14a} 
and then compare it to those observed in available implementations in 
\textsc{pari} \cite{pari}, \textsc{magma} \cite{magma} and/or 
\textsc{sage} \cite{sage}.

The conclusion of our analysis is that generic algorithms which are 
designed to work over an arbitrary field are often quite unstable. Very 
roughly, denoting by $D$ the number of divisions performed by the 
algorithm under analysis and by $q$ the cardinality of the residue 
field, it appears that the numbers of lost digits is about $O(\frac D 
q)$ whereas the theoretical loss of precision is closer than $O(\log_q 
D)$ --- and sometimes even $O(1)$. We would like to underline in 
particular that the instability phenomema are much more apparent when 
the residue field is small. It is the reason why we will always take
$K = \Q_2$ is our examples.

\medskip

\noindent
{\bf Organization of the paper.}
The section \ref{sec:theory} introduces the theory of ultrametric 
precision developped in \cite{caruso-roe-vaccon:14a} and completes it on a technical 
point. The analysis of standard algorithms are treated in the next
sections: \S\S \ref{sec:numbers}--\ref{sec:vectorspaces} are 
successively devoted to the case of numbers, matrices, univariate 
polynomials, multivariate polynomials and finally vector spaces.

\medskip

\noindent
{\bf Notations.}
Throughout the paper, the letter $K$ will refer to a \emph{complete 
discrete valuation field of characteristic $0$}, that is either a finite 
extension of the field of $p$-adic numbers $\Qp$ or a field of Laurent 
series over a field of characteristic $0$. We denote by $\val : K \to \Z 
\cup \{+\infty\}$ of $K$ --- it is the $p$-adic valuation when $K = \Qp$ 
and the usual valuation of a Laurent series when $K = k((t))$ --- and by 
$\Vert \cdot \Vert$ the corresponding norm.

\section{The theory of p-adic precision}
\label{sec:theory}

\subsection{The main lemma and its consequences}

\todo{Recall results of \cite{caruso-roe-vaccon:14a}.}

{\color{Bittersweet}

We saw in the core of the article that the differential of an operation
encodes the intrinsic loss/gain of precision when performing this
operation. The aim of this appendix is to compute the differential of
many common operations on numbers, polynomials and matrices. Surprisingly,
we observe that all differentials we will consider are rather easy to
compute even if the underlying operation is quite involved (\emph{e.g.}
Gr\"obner basis).

In what follows, we use freely the ``method of physicists'' to compute
differentials: given a function $f$ differentiable at some point $x$, we
consider a small perturbation $dx$ of $x$ and write $f(x+dx) = y + dy$
by expanding LHS and neglecting terms of order $2$. The differential of
$f$ at $x$ is then the linear mapping $dx \mapsto dy$.

}


\subsection{A bound on a growing function}
\label{ssec:boundLambdaf}

In the next section, we will compute the derivative of many standard 
operations and remark that it often has a simple expression in term of 
the input and the output. In other terms, the function $f$ modeling
such an operation satisfies a differential equation of the form
$f' = g \circ (f, \id)$
where $g$ is a given --- and hopefully rather simple --- function. The 
aim of this subsection is to study this differential equation and to 
derive from it some bounds on the growing function $\Lambda(f)$.
We shall actually work with the slight more general differential 
equation:
\begin{equation}
\label{eq:diffequah}
f' = g \circ (f, h).
\end{equation}
Here $g$ and $h$ are known locally analytic functions and $f$ is the 
unknown, which is assumed to be locally analytic as well. The function
$f$ goes from $U$ to $V$, two open subsets in $K$-Banach spaces $E$ 
and $F$ respectively. The function $h$ goes from $U$ to $W$ where $W$
is again an open subset in a $K$-Banach space $G$. Consequently, $g$
goes from $V \times W$ to the vector space $\mathcal L(E,F)$ gathering
all continuous linear applications $E \to F$.
In what follows, we always assume that $V$ and $W$ contains the origin, 
$f(0) = 0$, $h(0) = 0$ and $g(0) \neq 0$. There assumptions are harmless 
for two reasons: first, we can always shift $f$ and $h$ (and $g$ 
accordingly) so that they both vanish at $0$ and second, in order to 
apply the theory of \cite{caruso-roe-vaccon:14a}, the derivative $f'(0)$ needs to be 
surjective and therefore \emph{a fortiori} nonzero.

We assume that we are given in addition two nondecreasing convex 
functions $\Lambda_g$ and $\Lambda_h$ such that $\Lambda(g) \leq 
\Lambda_g$ and $\Lambda(h) \leq \Lambda_h$. We assume in addition that 
there exists $\nu$ such that $\Lambda_g$ is constant on the interval 
$]{-}\infty, \nu]$\footnote{We note that this assumption is fullfiled if 
we take $\Lambda_g = \Lambda(g)$ because we have assumed that $g(0)$ 
does not vanish.}. We introduce the functions $\tau_\nu$ and $\Lambda_f$ 
defined by:
$$\begin{array}{rll}
&\tau_\nu(x) = x & \text{if } x \leq \nu \\
& \hphantom{\tau_\nu(x)} = {+} \infty & \text{otherwise} \medskip \\
\text{and} &
\multicolumn{2}{l}{\Lambda_f(x) = 
  \tau_\nu \circ (\id + \Lambda_g \circ \Lambda_h)(x + \alpha)}
\end{array}$$
where $\alpha$ is a real number satisfying $\Vert n! \Vert \geq 
e^{-\alpha n}$ for all $n$. A suitable value for $\alpha$ is 
$$\alpha = - \frac p {p-1} \cdot \log \Vert p \Vert > 0$$ 
where $p$ denotes the characteristic of the residue field and where, by 
convention, the above expression equals $0$ if $p = 0$ (we recall that 
we have assumed that $K$ has characteristic $0$).

\begin{prop}
\label{prop:boundLambdaf}\label{PROP:BOUNDLAMBDAF}
We have $\Lambda(f) \leq \Lambda_f$.
\end{prop}

\begin{proof}
See Appendix \ref{app:proof}.
\end{proof}

\begin{figure}
\null \hfill
\begin{tikzpicture}[scale=0.8]
\draw[thick,green!30,fill=green!20] 
   (-2,-1)--(-1,2)--(-1,3)--(3.5,3)
 --(2.5,2)--(-3.5,-4)--cycle;
\draw[->] (-5,0)--(3.5,0);
\draw[->] (0,-5)--(0,3);
\node[below, scale=0.75] at (3.5,0) { $x$ };
\node[right, scale=0.75] at (0,3) { $y$ };
\draw[black!80,dotted] (-1,2)--(0,2);
\node[right,scale=0.75] at (0,2) { $\nu$ };
\draw[black!80,dotted] (-2,0)--(-2,-1)--(0,-1);
\node[above,scale=0.75] at (-2.4,0) { $\Lambda_h^{-1}(\nu){-}\alpha$ };
\node[right,scale=0.75] at (0,-1) { $\Lambda_h^{-1}(\nu){+}\mu$ };
\draw[Blue,very thick] (-5,-4)--(-2,-1)--(-1,2);
\fill[Blue] (-1,2) circle (0.06);
\draw[Blue] (-1,2)--(-1,3);
\draw[Sepia, very thick] (-4.5,-5)--(3.5,3);
\draw[Blue, very thick,dashed] (-2,-1)--(-3.5,-4);
\node[below right,rotate=45,scale=0.9] at (-4.3,-4.8) 
  { $y = x + \log \Vert g(0) \Vert$ };
\node[above right,rotate=45,scale=0.9] at (-4.8,-3.8) 
  { $y = x + \alpha + \mu$ };
\end{tikzpicture}
\hfill \null

\caption{Admissible region for the graph of $\Lambda(f)$}
\label{fig:area}
\end{figure}

Figure \ref{fig:area} illustrates Proposition \ref{prop:boundLambdaf}. 
The blue plain line represents the graph of the function $\Lambda_f$. A 
quick computation shows that, on a neighborhood of ${-}\infty$, this 
function is given by $\Lambda_f(x) = x + \alpha + \mu$
where $\mu$ is the value that $\Lambda_g$ takes on the interval 
$]{-}\infty, \nu]$. Proposition \ref{prop:boundLambdaf} says that the
graph of $\Lambda(f)$ lies below the plain blue line. We remark
moreover that the Taylor expansion of $f(x)$ starts with the term
$g(0) x$. Hence, on a neighborhood on ${-}\infty$, we have 
$\Lambda(f)(x) = x + \log \Vert g(0) \Vert$. Using convexity, we 
get:
$$\Lambda(f)(x) \geq x + \log \Vert g(0) \Vert, 
  \quad \forall x \in \R.$$
In other words, the graph of $\Lambda(f)$ lies above the brown line.
Furthermore, we know that the slopes of $\Lambda(f)$ are all integral
because $f$ is locally analytic. Hence, $\Lambda(f)$ cannot lie above
the dashed blue line defined as the line of slope $2$ passing through
the first break point of the blue plain line --- which has coordinate 
$(y_0 - \alpha - \mu, y_0)$ with $y_0 = \min(\Lambda_h^{-1}(\nu) + \mu, 
\nu)$. As a conclusion, we have proved that the graph of $\Lambda(f)$ 
must coincide with the brown line until it meets the dashed blue line 
and then has to stay in the green area.

As a consequence of the above discussion, we derive the following 
proposition which can be directly combined with Proposition 3.12 of 
\cite{caruso-roe-vaccon:14a}.

\begin{prop}
\label{prop:boundLambdaf2}
Keeping the above notations, we have:
$$\Lambda(f)_{\leq 2} (x) \leq 2(x + \alpha + \mu) -
\min(\Lambda_h^{-1}(\nu) + \mu, \: \nu)$$
for all $x \leq \min(\Lambda_h^{-1}(\nu) - \alpha, \: \nu - \mu - \alpha)$.
\end{prop}

\begin{proof}
Just remark that $y = 2(x + \alpha + \mu) - y_0$ is the equation of 
the dashed blue line.
\end{proof}

\noindent
{\bf About the choice of the function $h$}.
At the beginning of \S \ref{ssec:boundLambdaf}, we have introduced a 
function $h$ in our differential equation. The aim of this was to be 
more flexible and possibly get this way better bounds on $\Lambda(f)$. 
In this paragraph, we discuss a recommended choice for it.

Actually, we do recommend taking $h$ as the identity function 
\emph{but} changing the norm on the codomain of $h$ at the same time.
More precisely, if $F$ --- which is the $K$-Banach in which $f$ takes
its values --- is equipped with the norm $\Vert \cdot \Vert_F$, we 
recommand endowing it with the second norm defined by
$\Vert x \Vert'_F = \lambda \cdot \Vert x \Vert_F$ ($x \in F)$
where $\lambda$ is a positive real number that we shall choose later 
and taking $h : (F, \Vert \cdot \Vert_F) \to (F, \Vert \cdot \Vert'_F)$ 
acting as the identity on the underlying vector spaces. Doing this, the 
function $\Lambda(h)$ maps $x$ to $x + \log \lambda$ ($x \in \R$) and we 
choose $\Lambda_h = \Lambda(h)$.

We believe that a good way to optimize $\lambda$ is to choose it so that 
the two quantities in the minima in Proposition \ref{prop:boundLambdaf2} 
agree. With the expression we got just above for $\Lambda_h$, our 
heuristic yields $\log \lambda = \mu$, \emph{i.e.} $\lambda = e^\mu$.
Making this choice, the norms of $f(x)$ and $h(x)$ are comparable --- 
at least, on the domain we are interested in --- and we do not loose so 
much when we write that the norm of the couple $(f(x), h(x))$ is the 
maximum of those of $f(x)$ and $h(x)$.

{\color{Bittersweet}

\section{Numbers}
\label{sec:numbers}

The most basic operations are, of course, sums, products and inverses of
elements of $K$. Their differential are well-known and quite easy to
compute: if $z = x + y$ (resp. $z = xy$, resp $z = \frac 1 x$), we have
$dz = dx + dy$ (resp. $dz = x \cdot dy + dx \cdot y$, resp $dz = -
\frac{dx}{x^2}$).

Slightly more interesting is the $n$-th power map $f$ from $K$ to
itself. Its differential $f'(x)$ is found by differentiating the
equation $y = x^n$, obtaining $dy = n x^{n-1} dx$.
Hence $f'(x)$ maps the ball $B_K(r)$ to $B_K(\lvert nx^{n-1} \rvert \cdot r)$. According to Lemma \ref{lem:main}, this
means that the behavior of the precision depends on the absolute value
of $n$. By the ultrametric inequality, we always have $\lvert n \rvert \leq 1$ but
may have $\lvert n \rvert = 0$ if the characteristic of $K$ divides $n$ or
$\lvert n \rvert = p^{-k}$ if $K$ is $p$-adic with $\val(n) = k$.  In the first case,
$f'(x)$ also vanishes and Lemma \ref{lem:main} does
not apply.  In the second, Lemma \ref{lem:main} reflects the well-known fact that
raising a $p$-adic number to the $p^k$-th power increases the precision
by $k$ extra digits.

}

\section{Matrices}
\label{sec:matrices}

\todo{Write a short introduction.}

The notation $M_{n,m}(K)$ refers to the $K$-vector space of matrices 
with $n$ rows and $m$ columns with coefficients in $K$.

\medskip

\noindent
{\bf Smith form.}
In what follows, we will often use the \emph{Smith decomposition} of a 
matrix $M \in M_{n,m}(K)$: we recall that it is a factorization of $M$ 
as a product $U_M \cdot \Delta_M \cdot V_M$ when $U_M$ and $V_M$ are 
unimodular (square) matrices of size $n$ and $m$ respectively, with 
coefficients in $\O_K$ and $\Delta_M$ satisfies the two following
conditions:

\noindent
(i) its $(i,j)$-th entry vanishes as soon as $i \neq j$

\noindent
(ii) its $(i,i)$-th entry divides its $(j,j)$-th entry when $i
\leq j$.

\noindent
Such a factorization is unique and can be easily computed by performing 
elementary operations on rows and columns.
Regarding to precision, one can prove that if the entries of $M$ are all 
known up to some $O(\pi^N)$, it is possible to compute a factorization 
$U_M \cdot \Delta_M \cdot V_M$ where $U_M$ and $V_M$ are \emph{exact}
matrices satisfying the above conditions and where $\Delta_M$ is known
at precision $O(\pi^N)$ and satisfies~(i) and~(ii) modulo $\pi^N$.


\subsection{Product of matrices}




{\color{Bittersweet}

\subsection{Determinant}

We first outline the standard computation of the differential of the function
$\det : M_n(K) \to K$. Suppose that $M \in \GL_n(K)$ and that $\com(M) = \det(M) M^{-1}$.
Then
\begin{align*}
\det(M + dM) &= \det(M) \cdot \det(I + M^{-1} \cdot dM)  \\
&= \det(M) \cdot \big(1 + \tr(M^{-1} \cdot dM)\big) \\
&= \det(M) + \tr(\com(M) \cdot dM).
\end{align*}
The differential of $\det$ at $M$ is then $dM \mapsto \tr(\com(M) \cdot dM)$. It turns
out that this formula is still valid when $M$ is not invertible.

\subsection{Characteristic polynomials.}

The same computation extends readily to characteristic polynomials,
since they are defined as determinants. More precisely, let us 
consider the function $\chi : M_n(K) \to K_n[X]$ taking a matrix 
$M$ to its monic characteristic polynomial $\det(X-M)$.
Then $\chi$ is differentiable at each point $M \in M_n(K)$ and its 
differential is given by $dM \mapsto \tr(\com(X{-}M) \cdot dM)$.

\subsection*{LU factorization.}

Define the LU factorization of a square matrix $M \in M_n(K)$ as a decomposition $M = LU$ 
where $L$ is lower triangular and unipotent and $U$ is upper triangular. 
Such a decomposition exists and is unique provided that no principal minor
of $M$ vanishes. We can then consider the mapping $M \mapsto (L,U)$ 
defined over the Zariski-open set of matrices satisfying the above 
condition. In order to differentiate it, we differentiate the relation 
$M = LU$ and rewrite the result as
$$L^{-1} dM \: U^{-1} = L^{-1} \cdot dL + dU \cdot U^{-1}.$$
We remark that in the right hand side of the above formula, the first
summand is lower triangular with zero diagonal whereas 
the second summand is upper triangular. Hence in order to compute $dL$
and $dU$, one can proceed as follows: 
\begin{enumerate}
\item compute the product $dX = L^{-1} dM \: U^{-1}$,
\item separate the lower and upper part of $dX$ obtaining $L^{-1} \cdot dL$ and $dU \cdot U^{-1}$
\item recover $dL$ and $dU$ by multiplying the above matrices by $L$ on the left and $U$ on the right respectively.
\end{enumerate}
The above discussion extends almost \emph{verbatim} to LUP 
factorization; the only difference is that LUP factorizations are not 
unique but they are on a small neighborhood of $M$ if we fix the matrix 
$P$.

\subsection*{QR factorization.}

A QR factorization of a square matrix $M \in M_n(K)$ will be
a decomposition $M = QR$ where $R$ is unipotent upper triangular and $Q$ is
orthogonal in the sense that $\trans Q \cdot Q$ is diagonal. As before, such 
a decomposition exists and is unique on a Zariski-open subset of $M_n(K)$. The
mapping $f : M \mapsto (Q,R)$ is then well defined on this subset. We 
would like to emphasize at this point that the orthogonality condition
defines a sub-manifold of $M_n(K)$ which is \emph{not} a
vector space: it is defined by equations of degree $2$. The codomain
of $f$ is then also a manifold; this example then fits into the setting
of Appendix \ref{sec:manifold} but not to those of Section \ref{sec:mainlemma}. 
We can differentiate $f$ by following the method used for LU 
factorization.  Differentiating the relation $M = QR$, we obtain
\begin{equation}
\label{eq:diffQR}
\trans Q \cdot dM \cdot R^{-1} = \trans Q \cdot dQ + \Delta \cdot dR 
\cdot R^{-1}
\end{equation}
where $\Delta = \trans Q \cdot Q$ is a diagonal matrix by definition.
Moreover by differentiating $\trans Q \cdot Q = \Delta$, we find that
$\trans Q \cdot dQ$ can be written as the sum of an antisymmetric 
matrix and a diagonal one. Since moreover $dR \cdot R^{-1}$ is upper
triangular with all diagonal entries equal to $0$, we see that 
\eqref{eq:diffQR} is enough to compute $dQ$ and $dR$ from $Q$, $R$ 
and $dM$.

\section{Univariate polynomials}
\label{sec:polynomials}

For any integer $d$, let us denote by $K_{< d}[X]$ the set of 
polynomials over $K$ of degree $< d$. It is a finite dimensional vector 
space of dimension $d$. The affine space $X^d + K_{< d}[X]$ is then 
the set of monic polynomials over $K$ of degree $d$. We denote it by 
$K_d[X]$.

\subsection*{Evaluation and interpolation.}

Beyond sums and products (which can be treated as before), two basic 
operations involving polynomials are evaluation and interpolation.
Evaluation models the function $(P,x) \mapsto P(x)$, where
$P$ is some polynomial and $x \in K$. Differentiating
it can be done by computing
\begin{equation}
\label{eq:diffeval}
(P + dP)(x + dx) = P(x + dx) + dP(x + dx) = P(x) + P'(x) dx + dP(x).
\end{equation}
Here $P'$ denotes the derivative of 
$P$. The differential at $P$ is then the linear map $(dP, dx) \mapsto 
P'(x) dx + dP(x)$.

As for interpolation, we consider a positive integer $d$ and the partial 
function $f : K^{2d} \to K_{< d}[X]$ which maps the tuple $(x_1, y_1, 
\ldots, x_d, y_d)$ to the polynomial $P$ of degree less than $d$ such 
that $P(x_i) = y_i$ for all $i$. The polynomial $P$ exists and is unique 
as soon as the $x_i$'s are pairwise distinct; the above function $f$ is 
then defined on this open set. Furthermore, if $(x_1, y_1, \ldots, x_d, 
y_d)$ is a point in it and $P$ denotes the corresponding interpolation 
polynomial, \eqref{eq:diffeval} shows that $d y_i = P'(x_i) dx_i + 
dP(x_i)$ for all $i$. For this, we can compute $dP(x_i)$ from $d x_i$ 
and $d y_i$ and finally recover $dP$ by performing a new interpolation.

\subsection*{Euclidean division.}

Let $A$ and $B$ be two polynomials with $B \neq 0$. The Euclidean division of 
$A$ by $B$ is the problem of finding $Q$ and $R$ with $A = BQ + R$ and $\deg R < \deg B$. 
Differentiating the above equality we find
\[
dA - dB \cdot Q = B \cdot dQ + dR,
\]
which implies that $dQ$ and $dR$ are respectively obtained as the 
quotient and the remainder of the Euclidean division of $dA - dB \cdot 
Q$ by $B$. This gives the differential. We note that the discussion 
above extends readily to convergent series (see also 
\cite{caruso-lubicz:14a}).

\subsection*{Greatest common divisors and B\'ezout coefficients.}

We fix two positive integers $n$ and $m$ with $n \geq m$. We consider the 
function $f : K_n[X] \times K_m[X] \to (K_{\leq n}[X])^3$ which sends a 
pair $(A,B)$ to the triple $(D, U, V)$ where $D$ is the \emph{monic} 
greatest common divisor of $A$ and $B$ and $U$ and $V$ are the B\'ezout 
coefficients of minimal degrees, computed by the extended 
Euclidean algorithm.
The nonvanishing of the resultant of $A$ and $B$ defines a Zariski open 
subset $\mathcal V_0$ where the function $\gcd$ takes the constant value 
$1$. On the contrary, outside $\mathcal V_0$, $\gcd(A,B)$ is a polynomial 
of positive degree.  Since $\mathcal V_0$ is dense, $f$ is not continuous outside 
$\mathcal V_0$. On the contrary, the function $f$ is differentiable, and even locally analytic,
on $\mathcal V_0$.

Of course, on $\mathcal V_0$, the first component $D$ is constant and 
therefore $dD = 0$. To compute $dU$ and $dV$, one can simply 
differentiate the B\'ezout relation $AU + BV = 1$. We get:
$$A \cdot dU + B \cdot dV = - (dA \cdot U + dB \cdot V)$$
from which we deduce that $dU$ (resp. $dV$) is obtained as the 
remainder in the Euclidean division of $U{\cdot}dX$ by $B$ (resp. of 
$V{\cdot}dX$ by $A$) where $dX = - (dA \cdot U + dB \cdot V)$.

In order to differentiate $f$ outside $\mathcal V_0$, we 
define the subset $\mathcal V_i$ of $K_n[X] \times 
K_m[X]$ as the locus where the $\gcd$ has degree $i$. The theory of 
subresultants shows that $\mathcal V_i$ is locally closed with respect to 
the Zariski topology. In particular, it defines a $K$-manifold in the 
sense of Appendix \ref{sec:manifold}, and the 
restriction of $f$ to $\mathcal V_i$ is differentiable. To compute
its differential, we proceed along the same lines as before: we 
differentiate the relation $AU + BV = D$ and obtain this way
$A \cdot dU + B \cdot dV - dD = dX$
with $dX = - (dA \cdot U + dB \cdot V)$. In the above relation, the
first two terms $A{\cdot}dU$ and $B{\cdot}dV$ are divisible by $D$ 
whereas the term $dD$ has degree less than $i$. 
Hence if $dX = D \cdot dQ + dR$ is the Euclidean division of $dX$ by $D$, 
we must have $\frac A D \cdot dU + \frac B D \cdot dV = dQ$ and $dD = 
-dR$. These relations, together with bounds on the degree of $U$ and $V$,
imply as before that $dU$ (resp. $dV$) are equal to the remainder in the
Euclidean division of $U{\cdot}dQ$ by $\frac B D$ (resp. of $V{\cdot}dQ$
by $\frac A D$).

\medskip

The lesson we may retain from this study is the following. If we have to 
compute the greatest common divisor of two polynomials $A$ and $B$ known 
with finite precision, we first need to determine what is its degree. 
However, the degree function is not continuous --- it is only upper 
semi-continuous --- and hence cannot be determined with certainty from $A$ and 
$B$, unless the approximation to $(A,B)$ lies entirely within $\mathcal V_0$.

We therefore need to make an extra hypothesis.  The most natural hypothesis to make
is that $\gcd(A, B)$ has the maximal possible degree.  The main reason for choosing
this convention is that if the actual polynomials $A, B \in K[X]$ have a greatest common
divisor of degree $i$ then there is some precision for which the maximal degree will be
equal to $i$, whereas if they have a positive
degree common divisor then no amount of increased precision will eliminate an intersection
with $\mathcal V_0$.  A second justification is that any other choice would yield a result
with no precision, since $A$ and $B$ appear to lie in $\mathcal V_i$ at the known precision.
Once this assumption is made, the computation is possible and one can apply Lemma 
\ref{lem:main} to determine the precision of the result.
Note that with the above convention, the $\gcd$ of $A$ and $A$ is $A$ 
itself although there exist pairs of coprime polynomials in any 
neighborhood of $(A,A)$.

\subsection*{Factorization.}

Suppose that we are given a polynomial $P_0 \in K_d[X]$ written as a 
product $P_0 = A_0 B_0$, where $A_0$ and $B_0$ are monic and coprime. 
Hensel's lemma implies that there exists a small neighborhood $\mathcal 
U$ of $P_0$ in $K_d[X]$ such that any $P \in \mathcal U$ factors 
uniquely as $P = A B$ with $A$ and $B$ monic and close enough to $A_0$ 
and $B_0$ respectively. Thus, we can consider the map $f : P 
\mapsto (A,B)$ defined on the subset of $\mathcal U$ consisting of monic 
polynomials. We want to differentiate $f$ at $P_0$. For this, we 
differentiate the equality $P = A B$ around $P_0$, obtaining
\begin{equation}
\label{eq:difffactor}
dP = A_0 \cdot dB + B_0 \cdot dA.
\end{equation}
where $dP$, $dA$ and $dB$ have degree less than $\deg P$, $\deg A$ and 
$\deg B$ respectively. If $A_0 U_0 + B_0 V_0 = 1$ is a Bezout relation
between $A_0$ and $B_0$, it follows from \eqref{eq:difffactor} that
$dA$ (resp. $dB$) is the remainder in the Euclidean division of $V_0
{\cdot} dP$ by $A_0$ (resp. of $U_0 {\cdot} dP$ by $B_0$).

\subsection*{Finding roots.}

An important special case of the previous study occurs when $A_0$ has 
degree $1$, that is $A_0(X) = X - \alpha_0$ with some $\alpha_0 \in K$. 
The map $P \mapsto A$ is then nothing but the mapping that 
follows the simple root $\alpha_0$. Of course, its differential around 
$P_0$ can be computed by the above method. Nevertheless, we may
obtain it more directly by expanding the equation $(P_0 + 
dP)(\alpha_0 + d\alpha) = 0$, obtaining
$P_0'(\alpha_0) d\alpha + dP(\alpha_0) = 0$.
Since $\alpha_0$ is a simple root, $P_0'(\alpha_0)$ 
does not vanish and we find:
$$d \alpha = - \frac{dP(\alpha_0)}{P'_0(\alpha_0)}.$$

We now address the case of a multiple root. Let $P_0$ be a monic 
polynomial of degree $d$ and $\alpha_0 \in K$ be a root of $P_0$ having 
multiplicity $m > 1$. Due to the multiplicity, it is no longer possible 
to follow the root $\alpha_0$ in a neighborhood of $P_0$. But it is if we 
restrict ourselves to polynomials which have a root of multiplicity $m$ 
close to $\alpha_0$. More precisely, let us consider the locus $\mathcal 
V_m$ consisting of monic polynomials of degree $d$ having a root of 
multiplicity at least $m$. It is a Zariski closed subset of $K_d[X]$ and
contains by assumption the polynomial $P_0$. 
Moreover, the irreducible components of $\mathcal V_m$ that meet at 
$P_0$ are in bijection with the set of roots of $P_0$ of multiplicity 
$\geq m$. In particular, $\alpha_0$ corresponds to one of these 
irreducible components; let us denote it by $\mathcal V_{m,\alpha_0}$. 
The algebraic variety $\mathcal V_{m,\alpha_0}$ is \emph{a fortioti} a
$K$-manifold. Moreover there exists a differentiable function $f$ which 
is defined on a neighborhood of $P_0$ in $\mathcal V_{m,\alpha_0}$ and 
follows the root $\alpha_0$, \emph{i.e.} $f(P_0) = \alpha_0$ and for all 
$P$ such that $f(P)$ is defined, $\alpha = f(P)$ is a root of 
multiplicity (at least) $m$ of $P$. The existence of $f$ follows from 
Hensel's Lemma applied to the factorisation $P_0(X) = (X-\alpha_0)^m 
B_0(X)$ where $B_0(X)$ is a polynomial which is coprime to $X - 
\alpha_0$. We can finally compute the differential of $f$ at $P_0$ 
(along $\mathcal V_{m,\alpha_0}$) by differentiating the equality
$P^{(m-1)}(\alpha) = 0$ where $P^{(i)}$ denotes the $i$-th derivative
of $P$. We find:
\begin{equation}
\label{eq:dalphamult}
d \alpha = - \frac{dP^{(m-1)}(\alpha_0)}{P_0^{(m)}(\alpha_0)}.
\end{equation}

The above computation has interesting consequences for $p$-adic 
precision. For example, consider the monic 
polynomial $P(X) = X^2 + O(p^{2N}) X + O(p^{2N})$ (where $N$ is some 
large integer) and suppose that we are asking a computer to compute one 
of its roots. What is the right answer? We remark that, if $\alpha$ is 
any $p$-adic number divisible by $p^N$, then $P(X)$ can be $X^2 - 
\alpha^2$ whose roots are $\pm \alpha$. Conversely, we can prove that 
the two roots of $P$ are necessarily divisible by $p^N$. Hence, the
right answer is certainly $O(p^N)$. Nevertheless, if we know in addition 
that $P$ has a double root, say $\alpha$, we can write $P(X) = (X - 
\alpha)^2$ and identifying the coefficient in $X$, we get $\alpha =
O(p^{2N})$ if $p > 2$ and $\alpha = O(p^{2N-1})$ if $p=2$.
This is exactly the result we get by applying Lemma \ref{lem:main} and
using \eqref{eq:dalphamult} to simplify the differential.

This phenomenon is general: if $P$ is a polynomial over $\Qp$ known at 
some finite precision $O(p^N)$, a root $\alpha$ of $P$ having possibly 
multiplicity $m$ (\emph{i.e.} $P(\alpha), P'(\alpha), \ldots, P^{(m-1)} 
(\alpha)$ vanish at the given precision) can be computed at precision 
roughly $O(p^{N/m})$. However, if we know in advance that $\alpha$ has 
multiplicity $m$ then we can compute it at precision $O(p^{N-c})$ 
where $c$ is some constant (depending on $P$ and $\alpha$ but not on
$N$).

}


\section{Vector spaces}
\label{sec:vectorspaces}

Vector spaces are generally represented as subspaces of $K^n$ for some 
$n$. Hence they naturally appear as points on grassmannians. Therefore, 
one can use the framework of \cite[Appendix A]{caruso-roe-vaccon:14a} to 
study the $p$-adic in this context.

\subsection{Geometry of grassmannians}

Given $E$ be a finite dimensional vector space over $K$ and $d$ an 
integer in the range $[0, \dim E]$ we denote by $\Grass(E,d)$ the 
set of $d$-dimensional subspaces of $E$. It is the so-called 
\emph{grassmannian}. It is well-known that $\Grass(E,d)$ has a 
natural structure of $K$-manifold. The aim of this subsection is to 
recall standard facts about its geometry. In what follows, we set
$n = \dim E$ and equip $E$ with a distinguished basis $(e_1, \ldots, 
e_n)$.

\medskip

\noindent
{\bf Description and tangent space.}
Let $V$ denote a fixed sub-vector space of $E$ of dimension $d$. The 
grassmannian $\Grass(E,d)$ can be viewed as the quotient of the set 
of linear embeddings $f: V \hookrightarrow E$ modulo the action (by 
precomposition) of $\GL(V)$: the mapping $f$ represents its image 
$f(V)$. It follows from this description that the tangent space of 
$\Grass(E,d)$ is canonically isomorphic to $$\Hom(V, E) / \End(V) 
\simeq \Hom(V, E/V).$$

\medskip

\noindent
{\bf Charts.}
Let $V$ and $V^\c$ be two complementary subspaces of $E$ 
(\emph{i.e.} $V \oplus V^\c = E$). We assume that $V$ has 
dimension $d$ and denote by $\pi$ the projection $E \to V$ 
corresponding to the above decomposition. We introduce the set 
$\mathcal U_{V,V^\c}$ of all embeddings $f : V \hookrightarrow E$ 
such that $\pi \circ f = \id_V$. Clearly it is an affine space over
$\Hom(V,V^\c)$. 
Furthermore, we can embed it into $\Grass(E,d)$ by taking $f$ as
above to its image. This way, $\mathcal U_{V,V^\c}$ appears as
on open subset of $\Grass(E,d)$ consistant exactly of those subspaces 
$W$ such that $W \cap V^\c = 0$. As a consequence, the tangent space 
at each such $W$ becomes isomorphic to $\Hom(V,V^\c)$. The
identification $\Hom(V,V^\c) \to \Hom(W, E/W)$ is given by
$du \mapsto (du \circ f^{-1}) \text{ mod } W$ where $f : V 
\stackrel{\sim}{\to} W$ is the linear mapping defining $W$.

When the couple $(V, V^\c)$ varies, the open subsets $\mathcal 
U_{V,V^\c}$ cover the whole grassmaniann and then define an atlas 
of it. When implementing vector spaces on computes, we usually restrict 
ourselves to the subatlas consisting of all charts of the form $(V_I, 
V_{I^\c})$ where $I$ runs over the family of subsets of $\{1, 
\ldots, n\}$ of cardinality $d$ and $V_I$ is the subspaces spanned by 
the $e_i$'s with $i \in I$. A subspace $W \in E$ then belongs to at 
least one $\mathcal U_{V_I, V_{I^{\text{c}}}}$ and, given a family of
generators of $W$, we can determine such an $I$ together with the
corresponding embedding $f : V_I \hookrightarrow E$ by echelonizing 
the matrix of generators of $W$.

\medskip

\noindent
{\bf A variant.}
Alternatively, one can describe $\Grass(E,d)$ as the set of linear 
surjective morphisms $f : E \to E/V$ modulo the action (by 
postcomposition) of $\GL(E/V)$ as well. This identification presents the 
tangent space at a given point $V$ as the quotient $\Hom(V, E/V) / 
\End(E/V)$, which turns out to be again isomorphic to $\Hom(V, E/V)$. 
Given a decomposition $E = V \oplus V^\c$ as above, we let $\mathcal 
U^\star_{V, V^\c}$ denote the set of surjective linear maps $f : E 
\to V^\c$ whose restriction to $V^\c$ is the identity. It is an 
affine space over $\Hom(V, V^\c)$ which can be identified to an open
subset of $\Grass(E,d)$ \emph{via} the map $f \mapsto \ker f$.

It is actually easily seen that $\mathcal U_{V, V^\c}$ and $\mathcal 
U^\star_{V, V^\c}$ define the same open subset in $\Grass(E,d)$. 
Indeed, given $f \in \mathcal U_{V, V^\c}$, one can write $f = \id_V 
+ h$ with $h \in \Hom(V, V^\c)$ and define the morphism $g = \id_E -
h \circ \pi \in \mathcal U^\star_{V, V^\c}$. The association $f \mapsto
g$ then defines a bijection $\mathcal U_{V, V^\c} \to \mathcal
U^\star_{V, V^\c}$ which commutes with the embeddings into the
grassmannian. We emphasize that the above bijection is quite easy to
compute: it consists in replacing the matrix 
$\Big(\begin{matrix} I \\ H \end{matrix}\Big)$ by $(\begin{matrix} -H & 
I \end{matrix})$ (in a good choice of coordinates).

\medskip

\noindent
{\bf Duality.}
If $E$ is a finite dimension vector space over $K$, we use the notation 
$E^\star$ for its dual (\emph{i.e.} $E^\star = \Hom(E,K)$) and, given in 
addition a subspace $V \subset E$, we denote by $V^\perp$ the subspace 
of $E^\star$ consisting of linear maps that vanish on $V$. We recall
that the dual of $V^\perp$ (resp. $E^\star/V^\perp$) is canonically
isomorphic to $E/V$ (resp. $V$).
For all $d$, the association $V \mapsto V^\perp$ defines a continuous 
morphism $\psi_E : \Grass(E,d) \to \Grass(E^\star, n-d)$. The action of 
$\psi_E$ on tangent spaces is easily described. Indeed, the differential 
of $\psi_E$ at $V$ is nothing but the canonical identification between
$\Hom(V, E/V)$ and $\Hom(V^\perp, E^\star/V^\perp)$ induced by 
``transposition''. Furthermore, we observe that $\psi_E$ respects
the charts we have defined above, in the sense that it maps bijectively 
$\mathcal U_{V, V^\c}$ to $\mathcal U^\star_{V^\perp, (V^\c)^\perp}
\simeq \mathcal U_{V^\perp, (V^\c)^\perp}$.

\subsection{Differential computations}

We compute in this subsection the differential of various operations on 
vector spaces. For brevity, we skip the estimation of the corresponding 
growing functions (but this can be done using Proposition 
\ref{prop:boundLambdaf2} as before).

\medskip

\noindent
{\bf Direct images.}
Let $E$ and $F$ be two finite dimensional $K$-vector spaces of dimension 
$n$ and $m$ respectively. Let $d$ be an integer in $[0,n]$. We are
interested here in the function $\DI$ (for ``direct image'') defined 
over $\mathcal M = \Hom(E,F) \times \Grass(E,d)$ that takes the 
couple $(f,V)$ to $f(V)$. Since the dimension of $f(V)$ may \emph{a 
priori} vary, the map $\DI$ does not take its values in well-defined 
grassmannian. To tackle this problem, we stratify $\mathcal M$ as
follows: for each integer $r \in [0,d]$, we introduce the subset
$\mathcal M_r \subset \mathcal M$ consisting of those couples $(f,V)$
for which $f(V)$ has dimension $r$. The $\mathcal M_r$'s are
locally closed in $\mathcal M$ are therefore are submanifolds. 
Moreover, $\DI$ induces by restriction respectable differentiable 
functions between $K$-manifolds:
$$\DI_r : \mathcal M_r \to \Grass(F,r).$$
We would like to differentiate $\DI_r$ around some point $(f,V) \in 
\mathcal M_r$. To do so, we use the first description of the 
grassmannians we gave above: we shall see points in $\Grass(E,d)$ 
(resp. $\Grass(F,d)$) as embeddings $V \hookrightarrow E$ (resp. $W 
\hookrightarrow F$) modulo the action of $\GL(V)$ (resp. $\GL(W)$).
The point $V \in \Grass(E,d)$ is then represented by the canonical 
inclusion $v : V \to E$ whereas a representative $w$ of $W$ satisfies
$w \circ \varphi = f \circ v$
where $\varphi : V \to W$ is the linear mapping induced by $f$. The 
previous relation still holds if $(f,v)$ is replaced by a couple $(f', 
v') \in \mathcal M_r$ sufficiently close to $(f,v)$.
Differentiating it and passing to the quotient we find, first, that the 
tangent space of $\mathcal M_r$ at $(f,v)$ consists of couples $(df, dv) 
\in \Hom(E,F) \times \Hom(V,E/V)$ such that
$$d\tilde w = \big((df \circ v + f \circ dv) \text{ mod } W\big)
\in \Hom(V, F/W)$$
factors through $\varphi$ (\emph{i.e.} vanishes on $\ker \varphi = V 
\cap \ker f$) and, second, that the differential of $\DI_r$ at $(f,V)$ 
is the linear mapping sending $(df,dv)$ as above to the unique element 
$dw \in \Hom(W,F/W)$ such that $dw \circ \varphi = d \tilde w$.

\medskip

\noindent
{\bf Inverse images.}
We now consider the mapping $\II$ (for ``inverse image'') sending a 
couple $(f,W) \in \mathcal W = \Hom(E,F) \times \Grass(F,d)$ to 
$f^{-1}(W)$. As before, this map does not takes its value in a single 
grassmannian and we need to stratify $\mathcal W$ in order to get 
differentiable functions. For each integer $r \in [0,n]$, we introduce 
the sub-manifold $\mathcal W_r$ of $\mathcal W$ consisting of those 
couples $(f,W)$ such that $\dim f^{-1}(W) = s$. For all $s$, $\II$ 
induces a continuous function
$\II_r : \mathcal W_r \to \Grass(E,s)$.
Pick $(f,W) \in \mathcal W_r$. Set $V = f^{-1}(W)$ and denote by $w : F 
\to F/W$ the canonical projection.
Similarly to what we have done for direct images, one can prove that
the tangent space of $\mathcal W_r$ at some point $(f,W) \in \mathcal
W_r$ is the subspace of $\Hom(E,F) \times \Hom(W,F/W)$ consisting of 
couples $(df,dw)$ such that
$d \tilde v = (w \circ df + dw \circ f)_{|W}$
factors through the linear mapping $\varphi : E/V \to F/W$ induced by 
$f$. Furthermore $\II_r$ is differentiable at $(f,W)$ and its 
differential is the linear mapping that takes $(df,dw)$ as above to the 
unique element $dv \in \Hom(V,E/V)$ satisfying $\varphi \circ dv =
d\tilde v$.

The cases of direct images and inverse images are related by duality
as follows: if $f : E \to F$ is any linear map and $W$ is a subspace
of $F$, then $f^\star(W^\perp) = f^{-1}(W)^\perp$. Using this, one 
can deduce the differential of $\DI_r$ from those of $\II_s$ and 
\emph{vice versa}.

%\begin{rem}
%A particularly interesting case of inverse images is of course those
%of kernels.
%\end{rem}

\medskip

\noindent
{\bf Sums and intersections.}
Let $d_1$ and $d_2$ be two nonnegative integers. We consider the 
function $\Sigma$ defined on the manifold $\mathcal C = \Grass(E,d_1) 
\times \Grass(E,d_2)$ by $\Sigma(V_1, V_2) = V_1 + V_2$. As before, in 
order to study $\Sigma$, we stratify $\mathcal C$ according to the 
dimension of the sum: for each integer $d \in [0, d_1+d_2]$, we define 
$\mathcal C_d$ as the sub-manifold of $\mathcal C$ consisting of those 
couples $(V_1, V_2)$ such that $\dim(V_1 + V_2) = d$. We get the way a 
well-defined mapping $\mathcal C_d \to \Grass(E,d)$ whose differential
can be computed similarly as before. We seek to state the result: the
tangent space of $\mathcal C_d$ at a given point $(V_1, V_2)$ consists 
of couples $(dv_1, dv_2) \in \Hom(V_1, E/V_1)
\times \Hom(V_2, E/V_2)$ such that $dv_1 \equiv dv_2 \pmod{V_1 + V_2}$ 
on the intersection $V_1 \cap V_2$ and the differential of $\Sigma_r$
at $(V_1, V_2)$ maps $(dv_1, dv_2)$ to $dv \in \Hom(V, E/V)$ (with $V
= V_1 + V_2$) defined by $dv(v_1 + v_2) = dv_1(v_1) + dv_2(v_2)$ ($v_1
\in V_1$, $v_2 \in V_2$).

Using duality, we derive from this a similar result for the mapping 
$(V_1, V_2) \mapsto V_1 \cap V_2$ (left to the reader).

\subsection{Application to precision}


\appendix

\section{Proof of Proposition \ref{prop:boundLambdaf}}
\label{app:proof}

\subsection{Composite of locally analytic functions}

Let $U$, $V$ and $W$ be three open subsets is some $K$-Banach spaces 
$E$, $F$ and $G$ respectively. We assume that $0 \in U$, $0 \in V$. Let 
$f : U \to V$ and $g : V \to W$ be two locally analytic functions around 
$0$ with $f(0) = 0$. The composition $h = g \circ f$ is then locally 
analytic around $0$ as well. Let us write the analytic expansion of $f$, 
$g$ and $h$ as follows:
$$f = \sum_{n \geq 0} f_n, \quad 
g = \sum_{n \geq 0} g_n, \quad
h = \sum_{n \geq 0} h_n, \quad$$
where $f_n$, $g_n$ and $h_n$ are the restrictions to the diagonal of 
some symmetric $n$-linear forms $F_n$, $G_n$ and $H_n$ respectively. The 
aim of this subsection is to prove the following intermediate result.

\begin{prop}
\label{prop:boundhr}
With the above notations, we have for all nonnegative integer $r$:
$$\Vert h_r \Vert \leq \sup_{m, (n_i)}
  \Vert g_m \Vert \cdot \Vert f_{n_1} \Vert \cdots \Vert f_{n_m} \Vert$$
where the supremum is taken over all couples $(m, (n_i))$ where $m$
is a nonnegative integer and $(n_i)_{1 \leq i \leq m}$ is a sequence of
length $m$ of nonnegative integers such that $n_1 + \ldots + n_m = r$.
\end{prop}

During the proof, multinomial coefficients will play an important role. 
We recall that they are defined as follows: 
given $s, k_1, \ldots, k_\ell$ some positive integers with $s \geq
k_1 + \cdots + k_\ell$, we set:
$$\binom s {k_1 \,\, k_2 \,\, \cdots \,\, k_\ell} =
  \frac{s!}{k_1!\: k_2! \cdots k_\ell! \: (s{-}k_1{-}\cdots{-}k_\ell)!}.$$
We also recall that these numbers are all positive integers.

We now expand $g \circ f$ as follows:
\begin{align}
g \circ f & = \sum_{m \geq 0} g_m \Big(\sum_{n \geq 1} f_n\Big) \nonumber \\
& = \sum \binom m {\!k_1 \,\, \cdots \,\, k_\ell\!} \:
G_m(f_{n_1}, \ldots, f_{n_1}, \ldots, f_{n_\ell}, \ldots, f_{n_\ell})
\label{eq:expansiongf}
\end{align}
where the latest sum runs over:

\noindent
(a) all finite sequences $(k_i)$ of positive integers whose length 
(resp. sum) is denoted by $\ell$ (resp. $m$), and

\noindent
(b) all finite sequences $(n_i)$ of positive integers of length
$\ell$.

\noindent
Moreover, in the argument of $G_m$, the variable $f_{n_i}$ 
is repeated $k_i$ times.

The degree of $G_m(f_{n_1}, \ldots, f_{n_1}, \ldots, f_{n_\ell},
\ldots, f_{n_\ell})$ is $r = k_1 n_1 + \ldots + k_\ell n_\ell$ and 
then contributes to $h_r$. As a consequence $h_r$ is equal to 
\eqref{eq:expansiongf} where the sum is restricted to sequences
$(k_i)$, $(n_i)$ such that $k_1 n_1 + \ldots + k_\ell n_\ell = r$.
Proposition \ref{prop:boundhr} now follows from the next lemma.

\begin{lem}
Let $\mathcal E$ be a $K$-vector space. Let $\varphi : \mathcal E^n \to 
K$ be a symmetric $s$-linear form and $\psi: E \to K$ defined by 
$\psi(x) = \varphi(x, x, \ldots, x)$.
Given $k_1, \ldots, k_\ell$ some positive integers whose sum is $m$ and 
$x_1, \ldots, x_\ell$ some elements of $\mathcal E$, we have:
$$\begin{array}{l}
\Big\Vert \binom m {k_1 \,\, k_2 \,\, \cdots \,\, k_\ell} \cdot
\varphi(x_1, \ldots, x_1, \ldots, x_\ell, \ldots,
x_\ell) \Big\Vert  \\
\hspace{4.3cm} \leq \Vert \psi \Vert \cdot \Vert x_1 \Vert^{k_1} \cdots
 \Vert x_\ell \Vert^{k_\ell}
\end{array}$$
where, in LHS, the variable $x_i$ is repeated $k_i$ times.
\end{lem}

\begin{proof}
It is enough to prove that:
\begin{equation}
\label{eq:normpolar}
\Big\Vert \binom m {k_1 \,\, k_2 \,\, \cdots \,\, k_\ell} \cdot
\varphi(x_1, \ldots, x_1, \ldots, x_\ell, \ldots,
x_\ell) \Big\Vert \leq \Vert \psi \Vert
\end{equation}
provided that all the $x_i$'s have norm at most $1$. We proceed by 
induction on 
$\ell$. If $\ell = 1$, Eq.~\eqref{eq:normpolar} follows directly from
the definition of $\Vert \psi \Vert$. 
We now pick $(\ell+1)$ integers $k_1, \ldots, k_{\ell+1}$ whose sum 
equals $m$ together with $(\ell+1)$ elements $x_1, \ldots, x_{\ell+1}$
lying in the unit ball of $\mathcal E$.
We also consider a new variable $\lambda$ varying in
$\O_K$. We set $x'_i = x_i$, $k'_i = k_i$ when $i < \ell$ and $x'_\ell 
= x_\ell + \lambda x_{\ell+1}$, $k'_\ell = k_\ell + k_{\ell+1}$. By the 
induction hypothesis, we know that the inequality:
$$\Big\Vert \binom s {k'_1 \,\, \cdots \,\, k'_\ell} \cdot
\varphi(x'_1, \ldots, x'_1, \ldots, x'_\ell, \ldots, x'_\ell) \Big\Vert
\leq \Vert \psi \Vert$$
holds for all $\lambda \in K$. Furthermore, LHS of the above inequality
is a polynomial $P(\lambda)$ of degree $k'_\ell$ whose coefficient in 
$\lambda^j$ is:
$$\binom s {k'_1 \,\, \cdots \,\, k'_\ell} \cdot
\binom {k'_\ell} {j} \cdot
\varphi(\underline x_j) = 
\binom s {k_1 \,\, \cdots \,\, k_{\ell-1} \,\, j} \cdot
\varphi(\underline x_j)$$
with
$$\underline x_j = (x_1, \ldots, x_1, \ldots, x_{\ell+1}, \ldots, 
x_{\ell+1})$$
where $x_i$ is repeated $k_i$ times if $i < \ell$ and $x_\ell$ 
(resp. $x_{\ell+1}$) is repeated $j$ times (resp. $k'_\ell - j$ times).
Since $\Vert P(\lambda) \Vert \leq \Vert \psi \Vert$ for all $\lambda$ in 
the unit ball, the norm of all its coefficients must be at most $\Vert \psi
\Vert$ as well. Looking at the coefficient in $\lambda^{k_{l+1}}$, we find
Eq.~\eqref{eq:normpolar} for the families $(k_1, \ldots, k_{\ell+1})$ 
and $(x_1, \ldots, x_{\ell+1})$. The induction follows.
\end{proof}

\subsection{Bounding a growing function}

We now go back to the setting of Proposition \ref{prop:boundLambdaf}.
We write the analytic expansion of $f$, $g$ and $h$ as follows: 
$$f = \sum_{n \geq 0} f_n, \quad
g = \sum_{n \geq 0} g_n, \quad
h = \sum_{n \geq 0} h_n$$
where $f_n$, $g_n$ and $h_n$ are the restrictions to the diagonal of 
some symmetric $n$-linear forms $F_n$, $G_n$ and $H_n$ respectively.
Following \cite{caruso-roe-vaccon:14a}, we define $\NP(f) : \R \to \R \cup 
\{+\infty\}$ as the greatest convex function such that $\NP(f)(n) \geq - 
\log \Vert f_n \Vert$ for all integer $n$ and denote by $\Lambda(f)$ its 
Legendre transform. We define $\NP(g)$, $\Lambda(g)$, $\NP(h)$ and
$\Lambda(h)$ similarly.
We recall that $\alpha$ is a nonnegative real number such that $\Vert n! 
\Vert \geq e^{-\alpha n}$ for all positive integer $n$.

\begin{lem}
\label{lem:boundLambdaf}
We keep the above notations. If the couple $(a,b)$ satisfies:
\begin{equation}
\label{eq:condAB}
b \geq a + \Lambda(g)\big( \! \max(b, \, \Lambda(h) (a)) \big)
\end{equation}
then $b \geq \Lambda(f)(a - \alpha)$.
\end{lem}

\begin{proof}
We have $f' = \sum_{n \geq 0} f'_n$ where
$$f'_n : U \to \mathcal L(E,F), \quad
x \mapsto \big(h \mapsto n \cdot F_n(h, x, x, \ldots, x)\big).$$
In particular, taking $h = x$, we find:
\begin{equation}
\label{eq:normderivative}
\Vert f'_n \Vert \geq \Vert n f_n \Vert = 
\Vert n \Vert \cdot \Vert f_n \Vert.
\end{equation}
Combining this with Proposition \ref{prop:boundhr}, we get, for all
nonnegative integer $r$:
$$\Vert (r+1) f_{r+1} \Vert \leq
  \sup_{m, (n_i)} \Vert g_m \Vert \cdot 
  \prod_{i=1}^m \max(\Vert f_{n_i} \Vert, \Vert h_{n_i} \Vert)$$
where the supremum runs over all couples $(m, (n_i))$ where $m$
is a nonnegative integer and $(n_i)_{1 \leq i \leq m}$ is a sequence of
length $m$ of nonnegative integers such that $n_1 + \ldots + n_m = r$.
We set $u_r = \Vert r! f_r \Vert$. Multiplying the above inequality by
$\Vert r! \Vert$, we obtain:
\begin{equation}
\label{eq:boundurrec}
u_{r+1} \leq
  \sup_{m, (n_i)} \Vert g_m \Vert \cdot 
  \prod_{i=1}^m \max(u_{n_i}, \Vert n_i! h_{n_i} \Vert)
\end{equation}
since the multinomial coefficient $\binom r {\!n_1 \, \cdots \, n_m\!}$
is an integer and hence as norm at most $1$.

We now pick two real numbers $a$ and $b$ satisfying \eqref{eq:condAB}.
Set $d = \Lambda(h)(a)$. Going back to the definitions of $\Lambda (h)$ and Legendre transform, we remark that
the above equality implies:
\begin{equation}
\label{eq:boundhn}
\Vert h_n \Vert \leq e^{- a n + d},
  \quad \forall n \in \N.
\end{equation}
In the same way, from Eq.~\eqref{eq:condAB}, we get:
\begin{equation}
\label{eq:boundgm}
\Vert g_m \Vert \leq e^{-\!\max(b,d)\cdot m + b - a},
  \quad \forall m \in \N.
\end{equation}
We are now ready to prove $u_r \leq e^{-ar + b}$ by induction on $r$.
When $r = 0$, it is obvious because $u_0$ vanishes. Otherwise, it
follows from \eqref{eq:boundurrec}, \eqref{eq:boundhn}, \eqref{eq:boundgm}, 
and the induction hypothesis that:
\begin{align*}
u_{r+1} 
& \leq \sup_{m, (n_i)}
    e^{ -\max(b,d)\cdot m + b - a + \sum_{i=1}^m (-a n_i + \max(b,d))} \\
& = e^{ b - a - a r } = e^{ -a (r+1) + b}
\end{align*}
and the induction goes.
Finally, keeping in mind the definition of $u_r$, from $u_r \leq 
e^{-a r + b}$, we obtain:
$$\Vert f_r \Vert \leq u_r \cdot \Vert r! \Vert^{-1} \leq
e^{-(a - \alpha) r + b}$$
which means that $b \geq \Lambda(f)(a - \alpha)$.
\end{proof}

\begin{rem}
A simpler proof is possible if $\Z \subset \O_K$, \emph{i.e.} if
one can choose $\alpha = 0$. Indeed, under this extra assumption,
one can prove using Eq.~\eqref{eq:normderivative} that:
$$\Lambda(f') \geq \Lambda(f) - \id.$$
Combining this with Lemma 3.7 of \cite{caruso-roe-vaccon:14a}, we find:
$$\Lambda(f) - \id \leq \Lambda(g) \circ \max(\Lambda(f), \Lambda(h))$$
from which we can easily derive Lemma \ref{lem:boundLambdaf}.
\end{rem}

We can now conclude the proof of Proposition \ref{prop:boundLambdaf} as 
follows. Given $a \in \R$ and $b = a + \Lambda_g \circ \Lambda_h(a)$, we 
have to prove that $\Lambda(f)(a-\alpha) \leq b$ provided that $b \leq 
\nu$. Thanks to Proposition \ref{prop:boundLambdaf}, it is enough to 
check that such couples $(a,b)$ satisfy \eqref{eq:condAB}. Clearly:
$b \geq a + \Lambda(g) \circ \Lambda(h)(a)$
since $\Lambda_g \geq \Lambda(g)$, $\Lambda_h \geq \Lambda(h)$ and
$\Lambda_g$ is nondecreasing. Furthermore, from $b \leq \nu$, we get:
$$\Lambda_g(b) = \min_{x \in \R} \Lambda_g(x) \leq \Lambda_g \circ 
\Lambda_h(a).$$
Thus $a + \Lambda_g(b) \leq a + \Lambda_g \circ \Lambda_h(a) =
b$ and we are done.

\bibliographystyle{plain}
\bibliography{../roebib/Biblio,extras}

\end{document}
