%\documentclass{sig-alternate}
\documentclass{lms}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage{amsrefs}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,french,linesnumbered]{algorithm2e}

\newcommand{\noopsort}[1]{}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
%\theoremstyle{definition}
\newtheorem{deftn}[theo]{Definition}
\newtheorem{rmk}[theo]{Remark}

\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\renewcommand{\O}{\mathcal O}
\newcommand{\OK}{\mathcal{O}_K}
\newcommand{\XX}{\mathbf X}
\newcommand{\trans}{{}^{\text t}}

\newcommand{\GL}{\text{\rm GL}}

\newcommand{\val}{\text{\rm val}}
\newcommand{\pr}{\text{\rm pr}}
\newcommand{\tr}{\text{\rm Tr}}
\newcommand{\com}{\text{\rm Com}}
\newcommand{\Grass}{\text{\rm Grass}}
\renewcommand{\prec}{\text{\rm prec}}

\newcommand{\lb}{\ensuremath{\llbracket}}
\newcommand{\rb}{\ensuremath{\rrbracket}}
\newcommand{\lp}{(\!(}
\newcommand{\rp}{)\!)}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\title{$p$-adic precision}
\author{Xavier Caruso, David Roe \& Tristan Vaccon}
\date\today

\maketitle
\begin{abstract}
We present a new method to handle $p$-adic precision (or more generally
precision over ultrametric fields) based on $p$-adic calculus.
We illustrate it by many examples and give a toy application to the
stable computation of the SOMOS 4 sequence.
\end{abstract}

\setcounter{tocdepth}{1}
\tableofcontents

\section{Introduction}

The last two decades have seen a rise in the popularity of $p$-adic methods in
computational algebra.  For example,
\begin{itemize}
\item Bostan et al. \cite{boston-gonzalez-perdry-schost:05a} used Newton sums for polynomials over $\Zp$ to compute composed products for polynomials over $\Fp$;
\item Gaudry et al. \cite{gaudry-houtmann-weng-ritzenthaler-kohel:06a} used $p$-adic lifting methods to generate genus 2 CM hyperelliptic curves;
\item Kedlaya \cite{kedlaya:01a} used $p$-adic cohomology to count points on hyperelliptic curves over finite fields;
\item Lercier and Survent \cite{lercier-sirvent:08a} computed isogenies between elliptic curves over finite fields using $p$-adic differential equations.
\end{itemize}
Like real numbers, most $p$-adic numbers cannot be represented exactly, but instead must be
stored with some finite precision.  In this paper we focus on methods for handling $p$-adic precision
that apply across many different algorithms.

Two sources of inspiration arise when studying $p$-adic algorithms.
The first relates $\Zp$ to its quotients $\Z/p^n\Z$.  The preimage in
$\Zp$ of an element $a \in \Z / p^n \Z$ is a ball, and these balls cover
$\Zp$ for any fixed $n$.  Since the projection $\Zp \to \Z/p^n\Z$ is a
homomorphism, given unknown elements in two such balls we can
locate the balls in which their sum and product lie.  Working on a computer
we must find a way to write elements using only a finite amount of data.
By lumping elements together into these balls of radius $p^{-n}$, we may
model arithmetic in $\Zp$ using the finite ring $\Z / p^n \Z$.  In this representation,
all $p$-adic elements in have constant \emph{absolute precision} $n$.

The second source draws upon parallels between $\Qp$ and $\R$.  Both
occur as completions of $\Q$ and we represent elements of both in terms
of a set of distinguished rational numbers.
In $\R$, floating point arithmetic provides approximate operations $\oplus$ and $\odot$ on a
subset $S_{\infty,h} \subset \Z[\frac12]$ that model $+$ and $\cdot$ in $\R$ up to a given relative precision $h$:
\[
\lvert \frac{x \circledast y}{x \ast y} - 1 \rvert \le 2^{-h}
\]
for $\ast \in \{+, \cdot\}$ and all $x, y \in S_{\infty,h}$ with $x \ast y \ne 0$.  The $p$-adic analogue
defines floating point operations on $S_{p,h} \subset \Z[\frac1p]$ with
\[
\lvert \frac{x \circledast y}{x \ast y} - 1 \rvert_p \le p^{-h}.
\]
When using floating point arithmetic, elements are represented with a constant \emph{relative precision} $h$.

Neither of these models allows a user to track the precision of an individual element.
Over the reals, interval arithmetic is unwieldy, since arithmetic operations always increase the lengths of the inputs.
As a consequence, most computations in the real numbers rely on
statistical cancelation and external estimates of precision loss, rather than
attempting to track known precision at each step.  This tendency is strengthened
by the ubiquity of floating point arithmetic in scientific applications, where Gaussian
distributions are more common than intervals anyway.

In the $p$-adic world, precision tracking using intervals is much more feasible.
Even a long sequence of operations with such elements may not sacrifice any precision.  Intervals
allow number theorists to provably determine a result modulo a given power of $p$,
and the Gaussian distributions of measurement error over $\R$ have no direct analogue
over $\Qp$ anyway.  As a consequence, interval arithmetic is ubiquitous
in implementations of $p$-adic numbers.  The mathematical software packages
Sage \cite{sage}, PARI \cite{pari} and Magma \cite{magma} all include $p$-adic elements
that track precision in this way.

The approach of propagating precision with each arithmetic operation works well, but does
sometimes underestimate the known precision of a result, as we will discuss in Section
\ref{ssec:separation}.  Moreover, elements of $\Qp$ provide building blocks for generic
implementations of polynomials, vector spaces, matrices and power series.  The practice
of storing the precision within each entry is not flexible enough for all applications.
Sometimes only a rough accounting of precision is needed, in which case storing
and computing the precision of each entry in a large matrix needlessly consumes
space and time.  Conversely, recording the precision of each entry does not allow
a constraint such as specifying the precision of $f(0)$, $f(1)$ and $f(2)$ for a
quadratic polynomial $f$.

For a vector space $V$ over $\Qp$, we propose that the fundamental object used to
store the precision of an element should be a $\Zp$-lattice $H \subset V$.  By using
general lattices one can eliminate needless loss of precision.  Moreover, specifying
the precision of each entry or recording a fixed precision for all entries can both
be interpreted in terms of lattices.  In Section \ref{sec:prec-proposal} we detail our
proposal for how to represent the precision of an element of a vector space.

\todofor{David}{Finish summarizing sections}

\section{Tracking Precision} \label{sec:prec-proposal}

\subsection{Lattices}

We recall some standard definitions from ultrametric analysis, referring to
Schneider \cite{schneider:11a} for a complete exposition.

Let $K$ be a field with absolute value $|\cdot| : K \to \R_{\ge 0}$. 
We assume that the induced metric is an ultrametric -- 
\[ \lvert x + y \rvert \leq \max(\lvert x \rvert, \lvert y \rvert) \]
-- and that $K$ is complete with respect to it.
For example, we may take $K = \Qp$ with the
$p$-adic absolute value or $K = k\lp t \rp$ with the $t$-adic absolute value.
Write $\OK$ for the ring $\{x \in K : \lvert x \rvert \le 1\}$ and assume
that $K$ contains a dense subring $R \subset K$
consisting of elements that can be represented with a finite amount of space.
For $K = \Qp$ we may choose $R = \Z[\frac1p]$ or $R = \Q$;
for $K = \Fp\lb t \rb$ we may choose $R = \Fp[t, t^{-1}]$ or $R = \Fp(t)$.

If $E$ is a $K$-vector space, possibly of infinite dimension, then an
\emph{ultrametric norm} on $E$ is a map $\Vert\cdot\Vert : E \to \R^+$ satisfying:
\begin{enumerate}[(i)]
\item $\Vert x\Vert = 0$ if and only if $x = 0$;
\item $\Vert \lambda x\Vert = |\lambda| \cdot \Vert x\Vert$;
\item $\Vert x+y\Vert \leq \max(\Vert x\Vert, \Vert y\Vert)$.
\end{enumerate}
A \emph{$K$-Banach space} is a complete normed $K$-vector space.
A \emph{lattice} in a $K$-Banach space $E$ is an open bounded sub-$\OK$-module of $E$.

For a $K$-Banach space $E$ and $r \in \R_{\ge 0}$, write
\begin{align*}
B_E(r) &= \{ x \in E : \Vert x \Vert \le r \}, \\
B^-_E(r) &= \{ x \in E : \Vert x \Vert < r\}.
\end{align*}
Note that $B_E(r)$ and $B^-_E(r)$ are both lattices.

Suppose $E$ is a $K$-Banach space and $I$ a set.
A family $(x_i)_{i \in I} \subset E$ is a \emph{Banach basis} for $E$ if
every element $x \in E$ can be written
\[
x = \sum_{i \in I} \alpha_i x_i
\]
for scalars $\alpha_i \in K$ with $\alpha_i \to 0$, and $\Vert x \Vert = \sup_{i \in I} \lvert \alpha_i \rvert$.
Note that if $E$ is finite dimensional then the condition $\alpha_i \to 0$ is vacuous.

Given a basis $(x_i)_{i \in I}$ and a sequence $(r_i)_{i \in I}$ with $r_i \in \R_{>0}$, the sets
\begin{align*}
B_E((x_i),(r_i)) &= \Big\{ \sum_{i \in I} \alpha_i x_i : \lvert \alpha_i \rvert \le r_i \Big\}, \\
B^-_E((x_i),(r_i)) &= \Big\{ \sum_{i \in I} \alpha_i x_i : \lvert \alpha_i \rvert < r_i \Big\}
\end{align*}
are lattices precisely when the $r_i$ are bounded.  If we have equipped $E$ with a distinguished basis
then we may drop $(x_i)$ from the notation for $B^{(-)}_E((x_i),(r_i))$.
% Is every lattice of this shape for some Banach basis?

\subsubsection*{Lattices and computers.}

Suppose that $E \simeq K^d$ is finite dimesional.  Then if $H \subset E$
is a lattice then there exist $a, b \in \Q_{>0}$ with
\begin{equation}
\label{eq:incllattice}
B_K(a)^d \subset H \subset B_K(b)^d.
\end{equation}
Set $r = \frac a b$ and $R_r = \OK/B_K(r)$.  
Then a lattice $H$ satisfying \eqref{eq:incllattice} is uniquely 
determined by its image in the quotient $B_K(b)^d / B_K(a)^d \simeq 
R_r^d$.  Since $R \cap \OK$ is dense in $\OK$, elements of $R_r$ may be
represented exactly.  Thus $H$ may be encoded as a 
$(d \times d)$ matrix with coefficients in $R_r$.  For example, when
$K = \Qp$ the ring $R_r$ is just $(\Z / p^n\Z)$ for $n = \lfloor - \log_p r \rfloor$.

\subsection{Approximate elements}

Suppose that $E$ is a $K$-Banach space with basis $(x_i)_{i \in I}$.
\begin{deftn}
\begin{itemize}
\item An element $x \in E$ is \emph{exact} if there is a finite subset $J \subseteq I$ and scalars $\alpha_j \in R$ with
\begin{equation} \label{eq:exact_elt}
x = \sum_{j \in J} \alpha_j x_j.
\end{equation}

\item An \emph{approximate element} is a pair $(x, H)$ where $x \in E$ is an exact element
and $H$ is a lattice in $E$.
\end{itemize}
\end{deftn}

The pair $(x, H)$ represents an undetermined element of the coset
$x + H$.  We will frequently write $x + O(H)$ to emphasize the fact that $H$ represents
the uncertainty in the value of the approximate element.  In the special case that $E = K = \Qp$,
we recover the standard notation $a + O(p^n)$ for an approximate $p$-adic element.
Note that the set of exact elements is dense in $E$, so every element of $E$ can be approximated.

This definition encapsulates the main practical suggestion of the paper with regards to representing
vector spaces, matrices, polynomials and power series over $K$: one should \textbf{separate} the
approximation from the precision.  In the rest of this section we discuss some of the benefits made
possible by such a separation.

Note first that using an arbitrary lattice to represent the precision of an approximate element
can reduce precision loss when compared to storing the precision of each coefficient
$\alpha_i$ in \eqref{eq:exact_elt} separately.  Consider the function 
$f : \Qp^2 \to \Qp^2$ mapping $(x,y)$ to $(x+y, x-y)$ and the problem of computing
$f \circ f(a + O(p^n), b + O(p^m))$.

Write $(e_1, e_2)$ for the standard basis of $E=\Qp^2$.  Since $f$ is linear, the image of
the approximation $\bigl((a,b), B_E\bigl((e_1,e_2),(p^{-n}, p^{-m})\bigr)\bigr)$ is
$\bigl((a+b, a-b), B_E\bigl((e_1+e_2, e_1-e_2), (p^{-n}, p^{-m})\bigr)\bigr)$.
For $p \ne 2$, applying $f$ again yields $\bigl((2a, 2b), B_E\bigl((e_1, e_2), (p^{-n}, p^{-m})\bigr)\bigr)$.
Storing precisions on each coordinate of a vector amounts to only allowing precisions of the form
$B_E((r_i))$ with respect to the standard basis.  One would be forced to truncate the intermediate precision
to $B_E(p^{-\min(m, n)})$, yielding an artificial loss of precision in the final result.

\subsection{Separating precision from approximation}
\label{ssec:separation}

In addition to allowing for a more flexible representation of the precision of an element,
the separation of precision from approximation has other benefits as well.  If the precision
is encoded with the approximation, certain algorithms become unusable because of their
numerical instability.  For example, the Karatsuba algorithm for polynomial multiplication \cite{karatsuba-ofman:62a}
can needlessly lose precision when operating on polynomials with inexact coefficients.
However, it works perfectly well on exact approximations, leaving the question of the precision of
the product to be solved separately.  By separating the precision, more algorithms become available.

There are also 

\todofor{David}{
Explain here why it might be useful to work separately with 
approximation on the one hand and precision on the other hand.
}

\subsection{Step-by-step tracking is not sharp}


In this subsection, we illustrate by some examples that tracking
$p$-adic precision step by step may lead to important numerical
instability.

\subsubsection*{The SOMOS 4 sequence}

The SOMOS 4 sequence is a four-term inductive sequence satisfying:
$$u_{n+4} = \frac{u_{n+1} u_{n+3} + u_{n+2}^2}{u_n}.$$
In this example, we shall consider the situation where the initial
terms $u_0$, $u_1$, $u_2$ and $u_3$ are \emph{invertible} $p$-adic 
integers, \emph{i.e.} lie in $\Zp^\times$.

Let us first examine what happens (regarding to the precision) when
we are computing the sequence $(u_n)$ naively. The computation of
$u_{n+4}$ involves a division by $u_n$ and hence, roughly speaking,
decreases the precision by a factor $p^{\val(u_n)}$ where $\val$
stands for the $p$-adic valuation. Hence, if we assume that $u_0$,
$u_1$, $u_2$ and $u_3$ are given with precision $O(p^N)$ then the
naive computation returns the value of $u_n$ with precision
\begin{equation}
\label{eq:SOMOS}
O(p^{N-v_n})
\quad \text{with}\quad
v_n = \val(u_0) + \cdots + \val(u_{n-4}).
\end{equation}

However, on the other hand, one can prove that the SOMOS 4 sequence 
exhibits the \emph{Laurent phenomenon}. This means that, for all integer 
$n$, there exists a polynomial $P_n$ in $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 
1}, T^{\pm 1}]$ such that $u_n = P_n(u_0, u_1, u_2, u_3)$ (see \cite{fomin-zelevinsky:02a}).
From the latter formula, it follows directly that if $u_0$, $u_1$,
$u_2$ and $u_3$ are known up to precision $O(p^N)$ then all $u_n$'s
are also known with the same precision. Thus, the term $v_n$ that 
appears in \eqref{eq:SOMOS} does not reflect an intrinsic loss of
precision but some numerical instability related to the algorithm ---
which includes the model precision --- we used to compute to SOMOS 
sequence.

\begin{rmk}
From the above discussion, one can easily derive a numerically stable 
algorithm that computes the SOMOS 4 sequence: (1)~we compute the 
Laurent polynomials $P_n$ using the induction formula (and working in 
the ring $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 1}, T^{\pm 1}]$) and (2)~we 
evaluate $P_n$ at the point $(u_0, u_1, u_2, u_3)$.
However this algorithm is not efficient at all since computing the 
$P_n$'s is very time-consuming for two reasons: first, it requires 
division in a polynomial ring with $4$ variables and, second, the
size of the coefficients of $P_n$ explodes as $n$ grows.

In \S \ref{ssec:SOMOS-solution}, we shall design an algorithm computing 
the SOMOS 4 sequence which turns out to be, at the same time, efficient 
and numerically stable.
\end{rmk}

\subsubsection*{LU factorization}

Let us first recall that a square matrix $M$ with coefficients in $\Qp$ 
admits a LU factorization if it can be written as a product $LU$ where 
$L$ and $U$ is lower triangular and upper triangular respectively. 
The computation of a LU factorization appears as an important tool to 
tackle many classical questions about matrices or linear systems.

We first note that a LU decomposition of a given matrix $M$ is unique
if you require further that the $L$-part of this decomposition is
unipotent (\emph{i.e.} all diagonal entries equal $1$). Let us write
$L(M) U(M)$ the LU factorization of $M$ satisfying the above extra
condition (when it exists). There exists Cramer-type formulas that
gives each individual entries of $L(M)$and $U(M)$ in a closed form
(see \cite{caruso:12a	}).

If $M$ is random square matrix over $\Zp$ of side $d$ whose every
coefficient is known up to precision $O(p^N)$, the two following
results are proved in \cite{caruso:12a}:
\begin{itemize}
\item using usual Gauss elimination and tracking $p$-adic precision 
step-by-step, the smallest precision on an entry of $L(M)$ is about
$O(p^{N - \frac{2d}{p-1}})$ on average;
\item computing $L(M)$ by evaluating Cramer-type formulas yields a
result whose every entry is known up to precision $O(p^{N - 2 \log_p 
d})$.
\end{itemize}
If $d$ is large compared to $p$, the second precision is much more
accurate than the first one. This shows once again that ``artifical''
losses of precision appear when one performs Gauss elimination by
tracking precision step-by-step.

\section{The main Lemma}
\label{sec:mainlemma}

The theory of $p$-adic precision we want to develop is mainly based on a 
single lemma (Lemma \ref{lem:main}) of $p$-adic analysis. The aim of 
this Section is to state and prove this lemma. Consequences will be 
discussed after in Section \ref{sec:tracking}.

From now, we generalize a bit the setting and work over a general 
ultrametric complete field (see definition below). This includes 
$p$-adic fields and Laurent series fields.

%\subsection{Ultrametric analysis}
%\label{ssec:ultrametric}



%Similarly to the real case, any finite dimensional normed $K$-vector 
%space is complete and all norms over such a space are equivalent 
%(\emph{i.e.} they define the same topology). Many usual Theorems 
%concerning Banach spaces (\emph{e.g.} open mapping Theorem, closed graph 
%Theorem) remain true in the ultrametric setting.

%\begin{deftn} \label{deftn : diff}
%Let $V, W$ be two normed $K$-vector spaces, let $U \subset V$ be an open 
%subset of $V$ and let $f : U \rightarrow W$ be a map. Then $f$ is called 
%\emph{differentiable} at $v_0 \in U$ if there exists a 
%continuous linear map $f'(v_0) : U \rightarrow W$ such that for any 
%$\varepsilon >0$, there exists an open neighborhood $U_\varepsilon 
%\subset U$ containing $v_0$ with:
%\[ 
%\forall v, w \in U_\varepsilon, \quad
%\Vert f(v)-f(w)-f'(v_0) \cdot \left( v-w \right) \Vert 
%\leq \varepsilon \Vert v-w \Vert. 
%\]
%The linear map $f'(v_0)$ is called the \emph{differential} of $f$ at $v_0$.
%\end{deftn}

%One can derive all usual properties of the differential from this 
%definition: unicity, chain rules, \emph{etc}.



%The following Lemma shows that the notion of lattices behaves rather 
%well under linear\footnote{We shall see in the sequel (\emph{cf} Lemma 
%\ref{lem:main}) that, under some extra hypothesis, the linearity 
%assumption can be drastically relaxed.} transformations.

%\begin{lem}
%\label{lem:morlat}
%Let $E$ and $F$ be two $K$-Banach and $f : 
%E \to F$ be a continuous \emph{surjective} linear map.
%If $H$ is a lattice in $E$, then $f(H)$ is a lattice in $F$.
%\end{lem}

%\begin{proof}
%It is clear that $f(H)$ is a module over $B_K(1)$ which is bounded.
%The fact that it is open is a direct consequence of the open mapping
%Theorem.
%\end{proof}

%To each lattice $H$ in $E$, one can attach a norm $\Vert \cdot \Vert_H$ 
%on $E$ defined as follows. We pick $x \in E$, $x \neq 0$. We introduce
%the set $I_x$ consisting of scalars $\lambda \in K$ such that $\lambda x 
%\in H$. It follows from the fact that $H$ is open and bounded in $E$
%that $I_x$ is nonempty and bounded in $K$. Therefore, the image $|I_x|$ 
%of $I_x$ under the absolute value map is bounded and contains a positive
%element. We can then define:
%$$\Vert x \Vert_H = (\sup |I_x|)^{-1}.$$
%It is an exercise to check that $\Vert \cdot \Vert_H$ is a norm over 
%$E$.

%\begin{lem} \label{lem:reductionball}
%\begin{enumerate}
%\item The norms $\Vert \cdot \Vert_H$ and $\Vert \cdot \Vert$ are 
%equivalent.
%\item Let $B^-_{E,H}(1)$ (resp. by $B_{E,H}(1)$) the set of elements $x 
%\in E$ such that $\Vert x \Vert_H < 1$ (resp. $\Vert x \Vert_H \leq 1$).
%Then:
%$$B^-_{E,H}(1) \subset H \subset B_{E,H}(1)$$
%and the last inclusion is an equality if the absolute value on $K$ is
%discrete.
%\end{enumerate}
%\end{lem}

%\begin{proof}
%Left to the reader.
%\end{proof}

\subsection{Lattices}

%The following Lemma shows that the notion of lattices behaves rather 
%well under linear\footnote{We shall see in the sequel (\emph{cf} Lemma 
%\ref{lem:main}) that, under some extra hypothesis, the linearity 
%assumption can be drastically relaxed.} transformations.
%
%\begin{lem}
%\label{lem:morlat}
%Let $E$ and $F$ be two $K$-Banach and $f : 
%E \to F$ be a continuous \emph{surjective} linear map.
%If $H$ is a lattice in $E$, then $f(H)$ is a lattice in $F$.
%\end{lem}
%
%\begin{proof}
%It is clear that $f(H)$ is a module over $B_K(1)$ which is bounded.
%The fact that it is open is a direct consequence of the open mapping
%Theorem.
%\end{proof}
%
%To each lattice $H$ in $E$, one can attach a norm $\Vert \cdot \Vert_H$ 
%on $E$ defined as follows. We pick $x \in E$, $x \neq 0$. We introduce
%the set $I_x$ consisting of scalars $\lambda \in K$ such that $\lambda x 
%\in H$. It follows from the fact that $H$ is open and bounded in $E$
%that $I_x$ is nonempty and bounded in $K$. Therefore, the image $|I_x|$ 
%of $I_x$ under the absolute value map is bounded and contains a positive
%element. We can then define:
%$$\Vert x \Vert_H = (\sup |I_x|)^{-1}.$$
%It is an exercise to check that $\Vert \cdot \Vert_H$ is a norm over 
%$E$.
%
%\begin{lem} \label{lem:reductionball}
%\begin{enumerate}
%\item The norms $\Vert \cdot \Vert_H$ and $\Vert \cdot \Vert$ are 
%equivalent.
%\item Let $B^-_{E,H}(1)$ (resp. by $B_{E,H}(1)$) the set of elements $x 
%\in E$ such that $\Vert x \Vert_H < 1$ (resp. $\Vert x \Vert_H \leq 1$).
%Then:
%$$B^-_{E,H}(1) \subset H \subset B_{E,H}(1)$$
%and the last inclusion is an equality if the absolute value on $K$ is
%discrete.
%\end{enumerate}
%\end{lem}
%
%\begin{proof}
%Left to the reader.
%\end{proof}
%
%The first part of Lemma \ref{lem:reductionball} asserts that the 
%identity map $\iota_H : (E, \Vert \cdot \Vert_H) \to (E, \Vert \cdot 
%\Vert)$ is an homeomorphism. Furthermore, by the second of the Lemma, 
%$H$ is the image under $\iota_H$ of a lattice satisfying an extra 
%assumption: it is ``between the open unit ball and the closed unit 
%ball''.

\subsection{Images of lattices under differentiable functions}

\begin{deftn}
\label{def:firstorder}
Let $E$ and $F$ be two $K$-Banach and $f : U \rightarrow F$ be a 
function defined on an open subset $U$ of $E$. Let also $v_0$ be a 
point in $U$.
A lattice $H$ in $E$ is called a \emph{first order lattice} for $f$ at 
$v_0$ if the equality
\begin{equation}
\label{eq:firstorder}
f(v_0 + H) = f(v_0) + f'(v_0) (H)
\end{equation}
holds.
\end{deftn}

We emphasize that we require an equality in Eq.~\eqref{eq:firstorder}, 
and not just an inclusion! It will be very important for the 
applications we have in mind.
Here comes our main Lemma.

\begin{lem} \label{lem:main}
Let $E$ and $F$ be two $K$-Banach and $f : U 
\rightarrow F$ be a function defined on an open subset $U$ of $E$.
We assume that $f$ is differentiable at some point $v_0 \subset 
U$ and that the differential $f'(v_0)$ is surjective. 

Then, for all $\rho \in (0, 1]$, there exists a positive real 
number $\delta$ such that, for all $r \in (0, \delta)$, any lattice
$H$ such that $B^-_E(\rho r) \subset H \subset B_E(r)$ is a first
order lattice for $f$ at $v_0$.
\end{lem}

\begin{proof}
We may assume that $v_0=0$ and $f(0)=0$. Since $f'(0)$ is surjective, by 
the open mapping Theorem, there exists $C>0$ such that $B_F(1) \subset 
C  f'(0)(B_E(1))$. Let $\varepsilon>0$ be such that 
$\varepsilon C < \rho$. Let $U_\varepsilon \subset E$ given by Definition 
\ref{deftn : diff} and the fact that $f$ is differentiable at $0$. We 
may assume $U_\varepsilon = B_E(\delta)$ for some $\delta >0$.

Let $r \in (0, \delta)$. Let $B^-_E(r) \subset H \subset B_E(r).$ Let us 
show that $f$ maps surjectively $H$ onto $f'(0) (H)$. We first prove 
that $f(H) \subset f'(0) (H)$. Let $x \in H$. By differentiability at 
$0$, $\Vert f(x)-f'(0)(x) \Vert \leq \varepsilon \Vert x \Vert $. 
Therefore, if $y=f(x)-f'(0)(x)$, then $\Vert y \Vert \leq 
\varepsilon r$. By definition of $C$, we have $B_F(\varepsilon r) 
\subset \varepsilon C f'(0) (B_E(r))$. Thus there exists $x_1 \in 
\varepsilon C \cdot B_E(r)$ such that $f'(0) (x_1) =y$. Since 
$\varepsilon C < \rho$, we get $x_1 \in B^-_E(\rho r) \subset H$ and 
then $f(x)= f'(0) (x-x_1) \in f'(0) (H)$.

We shall now prove surjectivity. Let $y \in f'(0) (H)$. Let $x_0 \in H$ 
be such that $y = f'(0) (x_0)$. We define inductively two sequences 
$(x_n)$ and $(z_n)$ as follows:
\begin{itemize}
\item $z_n$ is an element of $E$ satisfying $f'(0)(z_n) = y - 
f(x_n)$ and $\Vert z_n \Vert \leq C \cdot \Vert y - f(x_n) \Vert$ (such 
an element exists by definition of $C$), and
\item $x_{n+1}=x_n+z_n$.
\end{itemize}
For convenience, let us also define $x_{-1} = 0$. We claim that the 
sequences $(x_n)$ and $(z_n)$ are well defined and take their values in 
$H$. To establish this, it is enough to prove that, assuming that 
$x_{n-1}$ and $x_n$ belongs to $H$, so do $z_n$ and $x_{n+1}$. Noticing
that 
\begin{equation}
\label{eq:mainlemma}
y - f(x_n) 
= f(x_{n-1}) + f'(0)(z_{n-1}) - f(x_n)
= f(x_{n-1}) - f(x_n) - f'(0)(x_{n-1} - x_n)
\end{equation}
we deduce using differentiability that
$\Vert y - f(x_n) \Vert \leq \varepsilon \cdot \Vert x_n - x_{n-1}
\Vert$.
Since we are assuming that $x_{n-1}$ and $x_n$ lie in $H \subset
B_E(r)$, we find $\Vert y - f(x_n) \Vert \leq \varepsilon r$. Thus
$\Vert z_n \Vert \leq C \cdot \varepsilon r < \rho r$ and then
$z_n \in H$. From the relation $x_{n+1} = x_n + z_n$, we finally
deduce $x_{n+1} \in H$.

Now using again Eq.~\eqref{eq:mainlemma} and differentiability at $0$,
we get, for all positive integer $n$,
$$\Vert y  - f(x_n) \Vert 
\leq \varepsilon \cdot \Vert z_{n-1} \Vert \leq \varepsilon C \cdot \Vert 
y - f(x_{n-1}) \Vert,$$
from what we deduce that $\Vert y - f(x_n) \Vert = O(a^n)$ and 
$\Vert z_n \Vert = O(a^n)$ with $a = \varepsilon C < \rho \leq 1$. 
This shows that $(x_n)$ is a Cauchy sequence and, since $E$ is complete, 
it converges. Because $H$ is closed, if $x$ denotes the limit of 
$(x_n)$, we have $x \in H$. Moreover, as a consequence of the 
differentiability, $f$ is continuous over $U_\varepsilon$. Therefore, 
since $x \in H \subset U_\varepsilon$, we get $y=f(x)$ and we are done.
\end{proof}

Let us conclude this paragraph by a remark on the assumption made in 
Lemma \ref{lem:main} about the surjectivity of $f'(v_0)$. First let us 
emphasize that this hypothesis is definitely necessary. Indeed, it would 
implies otherwise that the image of $f$ is locally contained in a proper 
sub vector space around each point where the differential of $f$ is not 
surjective, which is certainly not true! Nevertheless, one can get 
around this issue in many situations. Indeed, consider the situation of 
Lemma \ref{lem:main} without assuming that $f'(v_0)$ is surjective. Now, 
we pick a closed sub vector space $K$ of $F$ such that $K + f'(v_0)(E) = 
F$. Then, denoting by $\pr_K$ the canonical projection of $F$ onto 
$F/K$, the composite $\pr_K \circ f$ is certainly differentiable at 
$v_0$ at its differential is surjective. Hence Lemma \ref{lem:main}
applies and shows that
\begin{equation}
\label{eq:notsurjective}
f(v_0 + H) \subset f(v_0) + f'(v_0)(H) + K
\end{equation}
for all lattices $H$ satisfying a certain condition. Now we can take
the intersection of the right hand side of \eqref{eq:notsurjective} 
over all possible $K$ and hope that we get this way a sharp affine upper 
bound of $f(v_0 + H)$.

\subsection{The case of locally analytic functions}
\label{ssec:locanalytic}

\todofor{Tristan}{Improve this Subsection}

Before discussing applications of Lemma \ref{lem:main} to ultrametric 
precision, we would like to describe a little bit more restrictive 
setting where the constant $\delta$ appearing in this Lemma can be 
computed explicitly.


Indeed, when $f$ is a locally analytic function, we prove that we can recover $\delta$ from a Newton polygon defined with the Taylor series expansion of $f.$

We refer to Schneider \cite{schneider:11a}, \S 6, for a definition of a locally analytic function.

\begin{deftn} \label{deftn :  serie and norm}
Let $f(x)= \sum_{n \geq 0} f_n (x)$ be a serie over $E,$ a complete normed $K$-vector space. We assume that for any $\lambda  \in K,$ $x \in E,$ $n \in \mathbb{N},$ $\vert f_n(\lambda x) \vert = \vert \lambda \vert \vert f_n (x) \vert.$ 
We define $\vert f_n \vert = \sup_{x \in B_E(0,1)} \vert f_n(x) \vert,$ and we assume that all the $\vert f_n \vert $ are finite. We define $R((f_n)_n)$ to be the radius of convergence of the serie.

We also define : \[\Lambda((f_n)_n) (v) = \log \sup_{x \in B(0,e^v)} \vert f(x) \vert.\]
We define  $NP((f_n)_n)$ to be the Newton polygon of $f$, that is to say, the lower convex hull in $\mathbb{R}^2$ of the $(k, - \log \vert f_k \vert ),$ $k\geq 0.$ 
\end{deftn}

The following proposition is a classical consequence of convex analysis.

\begin{prop} \label{prop : conjugate Newton}
We define $v_\infty \in \mathbb{R} \cup \left\lbrace \infty \right\rbrace$ to be the limit of the slopes of $NP((f_n)_n).$
We denote by $NP((f_n)_n)^*$ the convex conjugate of $NP((f_n)_n).$ $NP((f_n)_n)^*$ is defined over $\left] -\infty,v_{\infty} \right[$ (at least)
\end{prop}

$\Lambda((f_n)_n)$ and $NP((f_n)_n)$ are connected :

\begin{prop} \label{prop : connexion sup Newton}
We have $R((f_n)_n) = e^{v_{\infty}}.$
Moreover, for any $v < \log \left( R((f_n)_n) \right),$ \[ \Lambda ((f_n)_n) (v) = NP((f_n)_n)^*(v).\]
\end{prop}
\begin{proof}
Let $v \in \left] -\infty,v_{\infty} \right[,$ and let us assume that $v$ is not a slope of $NP((f_n)_n).$
Let $u = NP((f_n)_n)^*(v).$ Then the intersection of the line defined by $y=vx-u$ and $NP((f_n)_n)$ is an extreme point of $NP((f_n)_n),$ say $(n, - \log ( \vert f_n \vert ))$ for some $n \geq 0,$ and $u = vn + \log \vert f_n \vert.$

$v$ is not a slope of $NP((f_n)_n)$ and by convexity of $NP((f_n)_n)$, its difference quotients at $n$ are increasing, therefore, there exists some $c>0$ such that for any $m\geq 0,$ $vm-u \leq - \log \vert f_m \vert - c \vert n-m \vert.$ Since $vm-u=vm-vn-log \vert f_n \vert,$ we get $-vn-\log \vert f_n \vert + c \vert n-m \vert  \leq -vm-\log \vert f_m \vert .$

Now, let $x \in  B(0,e^v),$ and $m \geq 0.$ Definition of $\vert f_m$ yields $ \vert f_m (x) \vert \leq \vert f_m \vert e^{vm}.$ Therefore \[\vert f_m (x) \vert \leq e^{-c \vert n-m \vert } \vert f_n \vert  e^{vn}< \vert f_n \vert e^{vn}.\]
Therefore, the serie converges and by ultrametricity $ \vert f(x) \vert \leq \vert f_n \vert e^{vn},$ and $\Lambda ((f_l)_l)(v) \leq \log \left( \vert f_n \vert e^{vn} \right).$ Furthermore,  $R((f_n)_n)  \geq e^v.$
 

Now, definition of $\vert f_n \vert$  provides $(x_i)_{i\geq 0}$ in $B(0,e^v)$ such that $\lim \vert f_n (x_i)= \vert f_n \vert e^{vn}.$ 

Since for any $i,$ $\vert f_m (x) \vert \leq e^{-c \vert n-m \vert } \vert f_n \vert  e^{vn},$ then for $i$ large enough, $\vert f_m (x) \vert < \vert f_n (x) \vert $ and $\vert f(x) \vert = \vert f_n (x) \vert.$

As a consequence, $\lim \vert f(x_i) \vert = \vert f_n \vert e^{vn},$ and  $\Lambda ((f_l)_l)(v) = \log ( \vert f_n \vert e^{vn}) = u= NP((f_n)_n)^*(v).$

Therefore, for any $v \in \left] -\infty,v_{\infty} \right[,$ with $v$ not a slope of $NP((f_n)_n),$ we get $\Lambda ((f_l)_l)(v) =NP((f_n)_n)^*(v).$
Since both $\Lambda ((f_l)_l)$ is increasing and $NP((f_n)_n)^*$ is continuous (since it is convex), we deduce that $R((f_n)_n) \geq  e^{v_{\infty}}$ and both functions coincide on $v \in \left] -\infty,v_{\infty} \right[,$

One can show similarily that $R((f_n)_n) \leq  e^{v_{\infty}}.$
\end{proof}

\begin{cor} Let $f(x)= \sum_{n \geq 0} f_n (x)$ and $g(x)= \sum_{n \geq 0} g_n (x)$ be two series as in definition \ref{deftn :  serie and norm}. Let $f(x)g(x)=\sum_{n \geq 0} \mu_n (x) $ and $f \circ g(x)=\sum_{n \geq 0} \tau_n (x) .$ Then :
$\Lambda((f_n+g_n)_n) \leq \max (\Lambda((f_n)_n),\Lambda((g_n)_n))$
$\Lambda((\mu_n)_n) \leq \Lambda((f_n)_n)+\Lambda((g_n)_n)$
$\Lambda((\tau_n)_n) \leq \Lambda((f_n)_n) \circ \Lambda((g_n)_n)$
\end{cor}

\begin{prop} \label{prop : legendre transform}
If $f$ and $g$ are two real convex functions such that $f \geq g,$ then $f^* \leq g^*.$
Furthermore, slopes of $f^*$ corresponds to extreme points of $f$.
\end{prop}

\begin{cor} \label{cor : trunc series}
Let $f(x)= \sum_{n \geq 0} f_n (x)$ be a serie as in definition \ref{deftn :  serie and norm}, $n_0 \geq 0$ and $g(x)= \sum_{n \geq n_0} f_n (x).$ Then $\Lambda((g_n)_n) \leq \Lambda((f_n)_n)$ and the slopes of $\Lambda((g_n)_n)$ are bigger than $n_0$. 
As a consequence, a minoration of $\Lambda((g_n)_n)$ can be obtain by extending $\Lambda((f_n)_n)(v)$ with a slope $n_0$ for $v \leq n_0$.
\end{cor}


Now, let $E$ and $F$ be two complete normed $K$-vector spaces, and let $f$ and the $L_k$'s be as in definition \ref{deftn:analyticNP}. We assume $L_1$ is surjective. We define, for any $n \geq 1$, $f_n(x)=L_n(x,\dots,x)$ for any $x \in E.$ 
Let $C$ be given by the Banach-Schauder theorem for $L_1$, $\varepsilon = \frac{1}{C}$. We provide three methods to find a sufficient $r$ for \ref{lem:main}.

For the first one, we assume that we have know a function that majorates $\Lambda((f_n)_{n \geq 1})$.

\begin{prop} \label{prop : maj Lambda}
If $m_1$ is a real convex function above $\Lambda((f_n)_{n \geq 1})$.
We define $m_2$ by extending $m_1$ with slope $2$.
Let $r$ be the abscissa of the intersection of $m_2$ and the line defined by $y=-\log \varepsilon+ x.$ Then $e^r$ is enough for \ref{lem:main}.
\end{prop}
\begin{proof}
$r$ is well-defined thanks to corollary \ref{cor : trunc series}, and $m_2 \geq \Lambda((f_n)_{n \geq 2}).$

One only has to prove that for all $x \in B(0,e^r),$ $\vert \sum_{n\geq 2} f_n(x) \vert \leq \varepsilon \vert x \vert.$

If $x \in B(0,e^r),$ $u = \log \vert x \vert,$ then $\Lambda((f_n)_{n \geq 2})(u) \geq -\log \varepsilon+ u.$
With proposition \ref{prop : connexion sup Newton}, for any $m \geq 2,$ $-\log \vert f_m \vert +m u \geq -\log \varepsilon+ u.$

Therefore, for any $m \geq 2,$ $\vert f_m \vert  \vert x \vert^m \leq \varepsilon \vert x \vert.$
By ultrametricity,   $\vert \sum_{n\geq 2} f_n(x) \vert \leq \varepsilon \vert x \vert,$ which concludes the proof.
\end{proof}

For the second one, we only assume that we have majorations on the $\vert f_n \vert.$

\begin{prop}
Let $(M_n)_{n \geq 2}$ be real numbers such that for all $n \geq 2$, $\vert f_n \vert \leq M_n.$

If we denote by $NP((M_{n+1})_{n \geq 1})$ the lower convex hull of the $(n, -\log M_{n+1})$ for $n \geq 1$, then $r$ such that $-\log \varepsilon=NP((M_n)_{n \geq 2})^*(r)$ is enough.
\end{prop}
\begin{proof}
Same idea as proposition \ref{prop : maj Lambda}. One can define $g_n$ with $g_n (x) =\frac{f_n(x)}{\vert x \vert}$ and $g_n(0)=0$ and work with $\Lambda ((g_n)_n).$
\end{proof}

We finally provide a simple method when working with finite dimension and $p$-adic fields.

\begin{prop} \label{prop : locanalyticfinitedim}
If $f$ is a locally analytic map $K^n \rightarrow K^m$, with $K$ a $p$-adic field, if $C$ and $\varepsilon$ are as above. It is enough to work with the Newton polygon $NP_F$ of $F(t) = \sum_{k\geq 0}^{+\infty} \interleave f^{(k+1)}(0) \interleave  t^k \in \mathbb{R}[[t]]$ : let $r$ be such that $NP_F^* \left( \log_p r - \frac{1}{p-1} \right) \leq \log_p \varepsilon.$ Then $r$ is enough for lemma \ref{lem:main}. 
\end{prop}
\begin{proof}
Same as before, with $L_n=\frac{f^{(n)}(0)}{n!}$ and Legendre formula. The result follows \textit{mutatis mutandis}.
\end{proof}

\todo{Explain how Newton polygons considered above behave under
usual operations (sum, multiplication and composition of functions).}

%Using Lemma \ref{lem:reductionball}, one can extend Proposition 
%\ref{prop:locanalytic} to arbitrary lattices $H$ as follows. Given $H$
%a lattice in some $K$-Banach space $E$, we first define the two
%following quantities:
%\begin{itemize}
%\item $\max(H)$ is the smallest $r$ such that $H \subset B_E(r)$, and
%\item $\min(H)$ is the greatest $r$ such that $B_E(r) \subset H$.
%\end{itemize}
%
%\begin{cor}
%Let $U$ are $V$ be open subsets in $K$-Banach spaces $E$ and $F$ 
%respectively and $f : U \to V$ be a locally analytic function. Let 
%also $v_0$ be a point in $U$ such that $f'(v_0)$ is surjective. 
%Then a lattice $H$ in $E$ is a first order lattice for $f$ at $v_0$ as 
%soon as $\max(H)^2 < C_{f, v_0} \cdot \min(H)$ where $C_{f, v_0}$ is a 
%constant depending only on $f$ and $v_0$ which can be determined 
%explicitely from the Newton polygon of $f$ at $v_0$.
%\end{cor}
%
%\begin{proof}
%We apply Proposition \ref{prop:locanalytic} to the composite $f \circ 
%\iota_H$ where $\iota_H : (E, \Vert \cdot \Vert_H) \to (E, \Vert \cdot 
%\Vert)$ is induced by identity map.
%\end{proof}

\section{Tracking precision using differentials}
\label{sec:tracking}

Lemma \ref{lem:main} and Proposition \ref{prop:locanalytic} have 
important applications to effective computations with $p$-adic numbers 
or power series, that we discuss in this Section. 

\subsection{Automatic sharp track of precision}

We consider a function {\tt f} (in the sense of computer 
science) that takes as input an (approximate) element lying in an open 
subset $U$ of a $K$-Banach space $E$ and outputs another (approximate) 
element lying in an open subset $V$ of another $K$-Banach space $F$. Of 
course, this function can be modelised by a \emph{continuous} 
mathematical function $f : U \to V$. The relation is the following: when 
{\tt f} is called on the input $x + O(H)$, it outputs $x' + O(H')$ with 
$f(x+H) \subset x' + H'$. We say that {\tt f} does not generate 
artificial loss of precision if the above inclusion is an equality; it is 
often not the case as shown in \S \ref{ssec:separation}.

Let us assume now that $f$ is locally analytic on $U$ and in addition 
that $f'(x)$ is surjective. Proposition \ref{prop:locanalytic} then 
yields a rather simple sufficient condition to decide if a given lattice 
$H$ is a first order lattice for $f$ at $x$. For such a lattice, by 
definition, we have $f(x+H) = f(x) + f'(x)(H)$. Thus the smallest 
possible $H'$ is $f'(x)(H)$. In other words, the function {\tt f} 
generates artificial loss of precision if the precision $H'$ it outputs 
is strictly larger that $f'(x)(H)$. The aim of this section is to 
explain how, under the above hypothesis, one can implement the function 
{\tt f} so that it always outputs the optimal precision $O(H')$ with $H' 
= f'(x)(H)$.

\subsubsection*{Forward computation}

The execution of the function {\tt f} yields a factorization:
$$f = f_n \circ f_{n-1} \circ \cdots \circ f_1$$
where the $f_i$'s correspond to each individual basic step (like 
addition, multiplication or affectation of variables); it is then a
``nice'' (in particular locally analytic) function. For all 
$i$, let $U_i$ denote the codomain of $f_i$. Concretely, $U_i$ is the 
set of all possible values of all variables which are defined in the 
program after the execution of $i$-th step. Mathematically, it is an 
open subset in some $K$-Banach space $E_i$. We have $U_n = V$ and the 
domain of $f_i$ is $U_{i-1}$ where, by convention, we have set $U_0 = 
U$.
For all $i$, we set $g_i = f_i \circ \cdots \circ f_1$ and $x_i = 
g_i(x)$. 

When we execute the function {\tt f} on the input $x + O(H)$, we apply 
first $f_1$ to this input obtaining this way a first result $x_1 + 
O(H_1)$ and then go on with $f_2, \ldots, f_n$. At each step, we obtain 
a new intermediate result that we denote by $x_i + O(H_i)$. A way to 
guarantee that no artificial loss of precision are generated is then to 
ensure $H_i = f'_i(x)(H_{i-1}) = g_i'(x)(H)$ at each step. This can be 
achieved by reimplementing all primitives (addition, multiplication, 
\emph{etc.}) and make them compute at the same time the function $f_i$
they implement together with its differential and apply the latter to
the ``current'' lattice $H_i$.

There is nevertheless an important issue with this approach: in order to 
be sure that Lemma \ref{lem:main} applies, we need \emph{a priori} to 
compute the exact values of all $x_i$'s, which is of course not 
possible! Assuming that $g'_i(x)$ is surjective for all $i$, we can fix 
it as follows. For each $i$, we fix a first order lattice $\tilde H_i$ 
for $g_i$ at $x$. Under our assumption, such lattices always exist and 
can be computed dynamically using Proposition \ref{prop:locanalytic}. 
Now, the equality $g_i(x + \tilde H_i) = x_i + g'_i(x)(\tilde H_i)$ 
means that any perturbation of $x_i$ by an element in $g'_i(x) (\tilde 
H_i)$ is induced by a perturbation of $x$ by an element in $\tilde H_i 
\subset H$. Hence, we can freely compute $x_i$ modulo $g'_i(x) (\tilde 
H_i)$ without changing the final result. Since $g'_i(x)(\tilde H_i)$ is 
a lattice in $E_i$, this remark makes possible the computation of $x_i$.

\begin{rmk}
In some cases, it is actually possible to determine suitable lattices 
$\tilde H_i$ together with their images under $g'_i(x)$ (or, at
least, good approximations of them) before starting the computation 
by using mathematical arguments. If possible, this generally helps a
lot. We shall present in \S \ref{ssec:SOMOS-solution} an example of 
this.
\end{rmk}

\subsubsection*{Backward computation}

The previous approach works only if the $g'_i(x)$'s are all surjective. 
Unfortunately, this assumption is in general not fulfilled. Indeed, 
remember that the dimension of $E_i$ is roughly the number of used 
variables after the step $i$. It all $g_i'(x)$ were surjective, this 
would mean that the function {\tt f} never initializes a new variable!
In what follows, we propose another solution that does not assume the
surjectivity of $g'_i(x)$.

For $i \in \{1, \ldots, n\}$, define $h_i = f_n \circ \cdots \circ 
f_{i+1}$, so that we have $f = h_i \circ g_i$. On differentials, we 
have $f'(x) = h_i'(x_i) \circ g'_i(x)$. Since $f'(x)$ is surjective (by 
assumption), we deduce that all $h'_i(x_i)$ are surjective. Let $H'_i$ 
be a lattice in $E_i$ such that:
\begin{enumerate}[(a)] 
\item \label{item:Hi1}
$H'_i$ is contained in $H_i + \ker h'_i(x_i) = h'_i(x_i)^{-1}
\big(f'(x)(H)\big)$;
\item \label{item:Hi2}
$H'_i$ is a first order lattice for $h_i$ at $x_i$.
\end{enumerate}
By definition, we have
$h_i(x_i + H'_i) = x_n + h'_i(x_i)(H'_i) \subset x_n + f'(x)(H)$.
Therefore, if after the $i$-th step of the execution of the function
{\tt f}, we modify the intermediate value $x_i$ by an element of
$H'_i$, the final result remains unchanged. In other words, it is
enough to compute $x_i$ modulo $H'_i$.

It is nevertheless not obvious to implement these ideas in practice
because when we enter in the $i$-th step of the execution of {\tt f},
we have not computed $h_i$ yet and hence are \emph{a priori} not able
to determine a lattice $H'_i$ satisfying the axioms \eqref{item:Hi1}
and \eqref{item:Hi2} above.
A possible solution to tackle this problem is to proceed in several
stages as follows: 
\begin{enumerate}[(1)]
\item \label{item:smallprec}
for $i$ going from $1$ to $n$, we compute $x_i$, $f'_i(x_{i-1})$ 
at small precision (but enough for the second step) together with an
approximation of the Newton polygon of the higher order derivatives;
\item \label{item:determineHi}
for $i$ going from $n$ to $1$, we compute $h'_i(x_i)$ and
determines a lattice $H'_i$ satisfying \eqref{item:Hi1} and 
\eqref{item:Hi2};
\item \label{item:finalcomp}
for $i$ going from $1$ to $n$, we recompute $x_i$ modulo $H'_i$
and finally outputs $x_n + O\big(f'(x)(H)\big)$.
\end{enumerate}
Using relaxed algorithms for computing with elements in $K$ (\emph{cf} 
\cite{}), we can reuse in Step~\eqref{item:finalcomp} the computations 
already performed in Step~\eqref{item:smallprec}. The ``backward'' method 
we have just presented is then probably not much more expansive than the 
``forward'' method, although it is certainly much more difficult to 
implement. 

We conclude this Section by remarking that the ``backward'' method seems 
to be particularly well suited to computations with lazy $p$-adic. Indeed 
in this setting, a target precision (\emph{i.e.} a precision on the 
output) is fixed and the software determines automatically the precision 
it needs on the input to perform the computation. To do this, it first 
builds the ``skeleton'' of the computation (\emph{i.e.} it determines the 
functions $f_i$ and eventually computes the $x_i$ at small precision when 
branching points occur and it needs to decide which branch it follows) 
and then runs over this skeleton in the reverse direction in order to 
determine (an upper bound of) the needed precision at each step. The 
similarity with our approach is noticeably strong.

\subsubsection*{Additional remarks}

From the beggining, we have assumed that $f'(x)$ is surjective. Let us 
discuss shortly what happens when this assumption is relaxed. As it is 
explained after the proof of Lemma \ref{lem:main}, the first thing we 
can do is to project the result onto different quotients, \emph{i.e.} to 
work with the composites $\pr_K \circ f$ for a sufficiently large family 
of closed sub vector spaces $K \subset F$ such that $K + f'(x)(E) = F$. 
(We recall that here $\pr_K$ denotes the canonical projection to $F$ 
onto the quotient $F/K$.) If $F$ has a natural system of coordinates 
(which is the case in almost all settings), we can generally take the 
$\pr_K$'s as the projections on each coordinate. Doing this, we end up 
with a precision on each individual coordinate. Furthermore, we have the 
guarantee that all these precisions are sharp (even if the lattice build
from them is not).
Let us illustrate the above discussion by an example: suppose that we 
want to compute the function $f : (K^n)^n \to M_n(K)$ that takes a 
family of $n$ vectors to its Graam matrix. The differential of $f$ is
clearly never surjective because $f$ takes its values in the subspace
consisting of symmetric matrices. Nevertheless for all couple $(i,j)
\in \{1, \ldots, n\}^2$, one can consider the composite $f_{ij} = 
\pr_{ij} \circ f$ where $\pr_{ij} : M_n(K) \to K$ that takes a matrix 
$M$ to its $(i,j)$-th entry. The maps $f_{ij}$'s are differentiable and 
their differentials generally are surjective (\emph{i.e.} they do not 
vanish). Let $M$ be a matrix known at some finite precision such that 
$f'_{ij} (M) \neq 0$ for all $(i,j)$. We can then apply the forward 
and/or the backward computation and get $f_{ij}(M)$ together with its 
precision. Putting this together, we get the whole matrix $f(M)$ 
together with a sharp precision datum on each entry.

The study of the this example actually suggests another solution to 
tackle the issue of non-surjectivity. Indeed, remark that our $f$ above 
had not a surjective differential simply because its codomain was too 
large: if we had replaced $f : (K^n)^n \to M_n(K)$ by $g : (K^n)^n \to 
S_n(K)$ (where $S_n(K)$ denotes the $K$-vector space of symmetric matrix 
over $K$ of size $n$) defined in the same way, our problem would 
disappear. Of course the image of a general $f$ is rarely a sub vector 
space of $F$ but it is often a sub-$K$-manifold of $F$. 
% Using the results of Appendix 


\medskip

We conclude this Section by remarking that the two strategies designed 
above (\emph{i.e.} forward and backward computations) share some 
similarities with usual floating point arithmetics over the reals. 
Indeed, roughly speaking, in each setting, we begin by choosing a large 
precision, we do all our computations up to this precision understood 
that when we are not sure about some digit, we choose it ``at random'' 
or using good heuristics. The main difference is that, in the 
ultrametric setting, we are able (under some mild hypothesis) to 
quantify the precision we need at each individual step in order to be 
sure that the final result is correct up to the required precision.

\subsection{Application to SOMOS sequence}
\label{ssec:SOMOS-solution}

We illustrate the theory developed in this paper by giving a simple 
toy application. Other applications will be discussed in subsequent 
articles. More precisely, we study the SOMOS 4 sequence introduced in \S 
\ref{ssec:separation}. Making a crucial use of Lemma \ref{lem:main} and 
Proposition \ref{prop:locanalytic}, we design a \emph{stable} algorithm 
for computing it.

Recall (\emph{cf} \S \ref{ssec:separation}) that a SOMOS 4 sequence is a 
four-term inductive sequence defined by $u_{n+4} = \frac{u_{n+2} u_{n+4} 
+ u_{n+3}^3}{u_n}$. We recall also that SOMOS sequences exhibit the 
Laurent phenomenon: its means that the four initial terms $u_0, u_1, 
u_2, u_3$ are some indeterminates, that each $u_n$ is a Laurent 
polynomial with coefficients in $\Z$ in these indeterminates
(see \cite{fomin-zelevinsky:02a}).

From now, we will always consider SOMOS sequences with values in $\Q_p$ 
(for some prime number $p$) and assume that $u_0, u_1, u_2, u_3$ are 
chosen such that all terms $u_n$'s are determined (\emph{i.e.} there is 
no division by zero). We assume also for simplicity that $u_0, u_1, u_2, 
u_3$ are all units in $\Z_p$. By Laurent phenomenon, this implies first 
that all $u_n$'s lie in $\Z_p$ and second that if $u_0, u_1, u_2, u_3$ 
are known with finite precision $O(p^N)$ then all terms $u_n$'s are 
known with the same precision. Algorithm \ref{algo:SOMOS} presented on 
page \pageref{algo:SOMOS} performs this computation.

\begin{algorithm}[t]
\SetKwInOut{Assumption}{Assumption}
\KwIn{$a,b,c,d$ --- four initial terms of a SOMOS 4 sequence $(u_n)_{n \geq 0}$}
\KwIn{$n, N$ --- two integers}
\Assumption{$a$, $b$, $c$ and $d$ lie in $\Z_p^\times$ and are known at 
precision $O(p^N)$}
\Assumption{None of $u_i$ $(0 \leq i \leq n$) is divisible by $p^N$}
\KwOut{$u_n$ at precision $O(p^N)$}
\BlankLine
$\prec$ $\leftarrow$ $N$\;
\For {$i$ from $1$ to $n-3$}{
  $\prec$ $\leftarrow$ $\prec + v_p(bd + c^2) - v_p(a)$\;
  {\bf lift} (arbitrary) 
    $b$, $c$ and $d$ to precision $O(p^{\prec})$\label{line:lift}\;
  $\prec$ $\leftarrow$ $\prec - v_p(a)$\;
  $e$ $\leftarrow$ $\frac{bd+c^2} a$; \hspace{1cm}
   \tcp{$e$ is known at precision $O(p^\prec)$}
  $a, b, c, d$ $\leftarrow$ $b + O(p^{\prec}), c + O(p^{\prec}), d + O(p^{\prec}), e$\;
}
\Return{$d + O(p^N)$}\;
\caption{\sc SOMOS$(a, b, c, d, n, N)$}\label{algo:SOMOS}
\end{algorithm}

We now prove that it is correct.
We introduce the (partial) function $f : \Q_p^4 \to \Q_p^4$ defined by 
$f(a,b,c,d) = (b,c,d,\frac{bd+c^2}a)$, so that, for all $i$, we have 
$(u_i, u_{i+1}, u_{i+2}, u_{i+3}) = f_i(u_0, u_1, u_2, u_3)$ where $f_i
= f \circ \cdots \circ f$ ($i$ times). Clearly, $f$ is  
differentiable on $\Q_p^\times \times \Q_p^3$ and its differential in the 
canonical basis is given by the matrix:
$$D(a,b,c,d) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-\frac{bd+c^2}{a^2} & \frac d a & \frac {2c} a & \frac b a
\end{pmatrix}$$
whose determinant is $\frac{bd+c^2}{a^2}$. Thus, if the $(i+4)$-th term 
of the SOMOS sequence is defined, the mapping $f_i$ is differentiable
at $(u_0, u_1, u_2, u_3)$ and its differential $\varphi_i = 
f'_i(u_0, u_1, u_2, u_3)$ is given by the matrix:
$$D_i = D(u_{i-1}, u_i, u_{i+1}, u_{i+2}) \cdots
D(u_1, u_2, u_3, u_4) \cdot D(u_0, u_1, u_2, u_3).$$
Moreover thanks to the Laurent phenomenon, we know that $D_i$ has
integral coefficients, \emph{i.e.} $\varphi_i$ stabilizes the lattice
$\Z_p^4$.
We are now going to prove by induction on $i$ that, at the end of the 
$i$-th iteration of the loop, we have $\prec = N + v_p(\det D_i)$ and
\begin{equation}
\label{eq:congrSOMOS}
(a, b, c, d) \equiv (u_i, u_{i+1}, u_{i+2}, u_{i+3}) \pmod
{p^N \varphi_i(\Z_p^4)}.
\end{equation}
The first point is easy. Indeed, from $D_i = D(u_{i-1}, u_i, u_{i+1}, 
u_{i+2}) D_{i-1}$, we deduce $\det D_i = \det D_{i-1} \cdot 
\frac{u_i}{u_{i-3}}$ and the assertion follows by taking determinants
and using the induction hypothesis. 
Let us now establish Eq.~\eqref{eq:congrSOMOS}. To avoid confusion, 
let us agree to denote $a'$, $b'$, $c'$, $d'$ and $\prec'$ the values of 
$a$, $b$, $c$, $d$ and $\prec$ respectively at the \emph{beginning} of 
the $i$-th iteration of the loop. By induction hypothesis (or by 
initialization if $i = 1$), we have:
\begin{equation}
\label{eq:congrSOMOS2}
(a', b', c', d') \equiv (u_{i-1}, u_i, u_{i+1}, u_{i+2}) \pmod
{p^N \varphi_{i-1}(\Z_p^4)}.
\end{equation}
Moreover, we know that the determinant of $\varphi_{i-1}$ has valuation
$\prec'$. Hence \eqref{eq:congrSOMOS2} remains true if $a'$, $b'$, $c'$
and $d'$ are replaced by other values which are congruent to them modulo
$p^{\prec'}$. In particular it holds if $a'$, $b'$, $c'$ and $d'$ denotes
the values of $a$, $b$, $c$ and $d$ after the execution of line
\ref{line:lift}. Now applying Lemma \ref{lem:main} and Proposition
\ref{prop:locanalytic} to $\varphi_{i-1}$ and $\varphi_i$ (at the point 
$(u_0, u_1, u_2, u_3)$), we get:
$$f\big((u_{i-1}, u_i, u_{i+1}, u_{i+2}) + p^N \varphi_{i-1}(\Z_p^4)\big)
= (u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4).$$
By the discussion above, this implies in particular that $f(a',b',c',d')$
belongs to $(u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4)$. We
conclude by remarking first that $(a,b,c,d) \equiv f(a',b',c',d') \pmod 
{p^\prec \Z_p^4}$ by construction and second that $p^\prec \Z_p^4 \subset
p^N \varphi_i(\Z_p^4)$.

Finally Eq.~\eqref{eq:congrSOMOS} applied with $i = n-3$ together with
the fact that $\varphi_i$ stabilizes $\Z_p^4$ imply that, when we exit
the loop, the value of $d$ is congruent to $u_n$ modulo $p^N$. Hence,
our algorithm returns the correct value.

\medskip

Let us conclude this subsection by remarking that Algorithm 
\ref{algo:SOMOS} performs computations at precision at most $O(p^{N+v})$ 
where $v$ is the sum of the valuations of five consequence terms among 
the first $n$ terms of the SOMOS sequence we are considering. 
Experiments show that the value of $v$ varies like $c \cdot \log n$ 
where $c$ is some constant. Assuming that we are using a FFT-like 
algorithm to compute products of integers, the complexity of Algorithm 
\ref{algo:SOMOS} is then expected to be $\tilde O(N n)$ where the 
notation $\tilde O$ means that we hide logarithmic factors.

We can compare this with the complexity of the more naive algorithm 
consisting of lifting the initial terms $u_0, u_1, u_2, u_3$ to enough 
precision and then do the computation using a naive step-by-step 
tracking of precision. In this setting, the required original precision 
is $O(p^{N+2v'})$ where $v'$ is the sum of the valuation of the $u_i$'s 
for $i$ varying between $0$ and $n$. Experiments show that $v'$ is about 
$c' \cdot n \log n$ (where $c'$ is a constant), which leads to a 
complexity in $\tilde O (N n + n^2)$. Our approach is then interesting
when $n$ is large compared to $N$: under this hypothesis, it saves
roughly a factor $n$.

%\section{Conclusions}

\appendix

\section{Generalization to manifolds}
\label{sec:manifold}

Many natural $p$-adic objects do not lie in vector spaces:
points in projective spaces or elliptic curves,
subspaces of a fixed vector space (which lie in Grassmannians),
classes of isomorphism of certain curves (which lie in various moduli spaces),
\emph{etc.}  In this section we extend the formalism 
developed in Section \ref{sec:mainlemma} to a more general setting:
we consider the quite general case of 
differentiable manifolds locally modeled on ultrametric Banach spaces.
This covers all the 
aforementioned examples.

In what follows, the letter $K$ always refers to an ultrametric field 
(\emph{cf} \S \ref{ssec:ultrametric}). The absolute value over $K$ is 
denoted by $|\cdot|$.

\subsection{Differentiable $K$-manifolds}
\label{ssec:manifold}

The theory of finite dimensional $K$-manifolds is presented for example 
in \cite{schneider:11a}*{Ch. 8-9}. In this Section, we shall work with
a slightly different notion of manifolds which allows also Banach vector 
spaces of infinite dimension.
More precisely, for us, a \emph{ differentiable $K$-manifold} 
(or just \emph{$K$-manifold} for short) is the data of a topological 
space $V$ together with an open covering $V = \bigcup_{i \in I} V_i$ 
(where $I$ is some set) and, for all $i \in I$, an homeomorphism 
$\varphi_i : V_i \to U_i$ where $U_i$ is an open subset of a $K$-Banach 
$E_i$ such that for all $i,j \in I$ for which $V_i \cap V_j$ is 
nonempty, the composite map
\begin{equation}
\label{eq:psiij}
\psi_{ij} : 
\varphi_i(V_{ij}) \stackrel{\varphi_i^{-1}}{\longrightarrow} 
V_{ij} \stackrel{\varphi_j}{\longrightarrow} \varphi_j(V_{ij})
\quad \text{(with } V_{ij} = V_i \cap V_j \text{)}
\end{equation}
is differentiable at all points in its domain. We recall that
the mappings $\varphi_i$ above are the so-called \emph{charts}. The
$\psi_{ij}$'s are the transition maps. The collection of $\varphi_i$'s
and $\psi_{ij}$'s is called an \emph{atlas} of $V$. In the sequel, we 
shall assume further that the open covering $V = \bigcup_{i \in I} V_i$ 
is locally finite, which means that every point $x \in V$ lies only in a 
finite number of $V_i$'s. Trivial examples of $K$-manifolds are 
$K$-Banach themselves.

If $V$ is a $K$-manifold and $x$ is a point of $V$, we define the 
\emph{tangent space} $T_x V$ of $V$ at $x$ as the space $E_i$ for some 
$i$ such that $x \in V_i$. We note that if $x$ belongs to $V_i$ and 
$V_j$, the linear map $\psi'_{ij}(\varphi_i(x))$ defines an isomorphism 
between $E_i$ and $E_j$. Furthermore these isomorphisms are compatible 
in an obvious way. This implies that the definition of $T_x V$ given 
above does not depend (up to some canonical isomorphism) on the index 
$i$ such that $x \in V_i$ and then makes sense.

As usual, we can define the notion of strict differentiability (at some 
point) for a continuous mapping between two $K$-manifolds by viewing it 
through the charts. A  differentiable map $f : V \to V'$ induces 
a linear map on tangent spaces $f'(x) : T_x V \to T_{f(x)} V'$ for all 
$x$ in the domain $V$. It is called the \emph{differential} of $f$ at
$x$.

\subsection{Precision data}

Going back to our problem of precision, given $V$ a $K$-manifold as 
above, we would like to be able to deal with ``approximations up to some 
precision'' of elements in $V$, \emph{i.e.} expressions of the form $x + 
O(H)$ where $x$ belongs to a dense \emph{computable} subset of $V$ and 
$H$ is a ``precision data''.
The aim of this paragraph is to give a precise meaning to the expression 
$x + O(H)$ and to prove several basic properties of it.
For now, we fix a $K$-manifold $V$ and we use freely the notations $I$, 
$V_i$, $\varphi_i$, \emph{etc.} introduced in \S \ref{ssec:manifold}.

\begin{deftn}
Let $x \in V$.
A \emph{precision datum} at $x$ is a lattice in the tangent space 
$T_x V$ such that for all indices $i$ and $j$ with $x \in U_i \cap
U_j$, the image of $T_x V$ in $E_i$ is a first order lattice for
$\psi_{ij}$ at $\varphi_i(x)$ (\emph{cf} Definition \ref{def:firstorder}.
\end{deftn}

\begin{rmk}
The definition of a precision datum at $x$ depends not only on $x$
and the manifold $V$ where it lies but also on the chosen atlas that
defines $V$.
\end{rmk}

\begin{prop}
\label{prop:independence}
Let $x \in V$ and $H$ be a precision datum at $x$.
The subset:
$$\varphi_i^{-1}\big(\varphi_i(x) + \varphi_i'(x)(H)\big) 
\subset V$$
does not depend on the index $i$ such that $x \in V_i$.
\end{prop}

\begin{proof}
Let $i$ and $j$ be two indices such that $x$ belongs to $V_i$ and $V_j$. 
Set $x_i = \varphi_i(x) \in E_i$ and $H_i = \varphi'_i(x)(H)$. The 
equality 
$$\varphi_i^{-1}\big(\varphi_i(x) + \varphi_i'(x)(H)\big) 
\subset V = \varphi_j^{-1}\big(\varphi_j(x) + \varphi_j'(x)(H)\big) 
\subset V$$
is clearly equivalent to
$\psi_{ij}(x_i + H_i) = \psi_{ij}(x_i) + \psi'_{ij}(x_i)(H_i)$.
and the latter holds because $H_i$ is a first order lattice
of $\psi_{ij}$ at the point $x_i$ by definition.
\end{proof}

We are now in position to define $x + O(H)$.

\begin{deftn}
Let $x \in V$ and $H$ be a precision datum at $x$. We set:
$$x + O(H) = \varphi_i^{-1}\big(\varphi_i(x) + \varphi_i'(x)(H)\big)
\subset V$$
for some (equivalenty, all) $i$ such that $x \in V_i$.
\end{deftn}

\subsubsection*{Change of base point}

In order to restrict ourselves to elements $x$ lying in a dense 
computable subset, we need to compare $x_0 + O(H_0)$ with varying $x + 
O(H)$ when $x$ and $x_0$ are close enough. Let us first examine the 
situation in a fixed given chart: we fix some index $i \in I$ and pick 
two elements $x_0$ and $x$ in $V_i$. We consider in addition a lattice 
$\tilde H_0$ in $E_i$ --- which should be think as $\varphi'_i(x_0) 
(H_0)$ --- and we want to produce a lattice $\tilde H$ such that 
$\varphi_i(x_0) + \tilde H_0 = \varphi_i(x) + \tilde H$. Of course 
$\tilde H = \tilde H_0$ does the job as soon as 
$\varphi_i(x) - \varphi_i(x_0) \in \tilde H_0$. 
Now, we remark that the tangent spaces $T_{x_0} V$ and $T_x V$ are both 
isomorphic to $E_i$ via the maps $\varphi'_i(x_0)$ and $\varphi'_i(x)$ 
respectively. A natural candidate for $H$ is then:
\begin{equation}
\label{eq:Hprime}
H = \left(\varphi'_i(x)^{-1} \circ \varphi'_i(x_0)\right) (H_0).
\end{equation}
With this choice, $x + O(H) = x_0 + O(H_0)$ as soon as $x$ and $x_0$ are 
close enough in the following sense: the difference $\varphi_i(x) - 
\varphi_i(x_0)$ lies in the lattice $\varphi'_i(x_0)(H_0)$. We 
furthermore have a property of independence on $i$.

\begin{prop}
Let $x_0 \in V$ and $H_0$ be a precision datum at $x_0$.
Then, for all $x$ sufficiently close to $x_0$,
\begin{enumerate}[(i)]
\item the lattice $H$ defined by \eqref{eq:Hprime} does not depend 
on $i$ and is a precision datum at $x$, and
\item we have $x + O(H) = x_0 + O(H_0)$.
\end{enumerate}
\end{prop}

\begin{proof}
We first prove (i). For an index $i$ such that $x, x_0 \in V_i$, let us 
denote by $f_i : T_{x_0} V \to T_x V$ the composite $\varphi'_i(x)^{-1} 
\circ \varphi'_i(x_0)$. Given an extra index $j$ satisfying the same
assumption, we know by \todo{(find a reference)} that the difference
$f_i - f_j$ goes to $0$ when $x$ converges to $x_0$. Since $H_0$ is open
in $T_{x_0} V$, this implies that $(f_j - f_i)(H_0)$ contains $f_i(H_0)$ and 
$f_j(H_0)$ if $x$ and $x_0$ are close enough. Now, pick $w \in f_j(H_0)$ and 
write it $w = f_j(v)$ with $v \in H_0$. Then $w$ is equal to $f_i(v) + 
(f_j - f_i)(v)$ and then belongs to $f_i(H_0)$ because each summand does. 
Therefore $f_j(H_0) \subset f_i(H_0)$. The inverse inclusion can be proved 
in the same way. The fact that $H$ is a precision datum at $x$ is easy
and left to the reader.
Finally, if $x$ is close enough to $x_0$, we can check (ii) in the 
charts but this was already done.
\end{proof}

\subsection{Generalization of the main Lemma}

With above definitions, Lemma \ref{lem:main} extends readily to 
manifolds. To state it the context of manifolds, we first need to define 
a norm on the tangent space $T_x V$ (where $V$ is some $K$-variety and 
$x$ is a point in $V$). There is actually in general no canonical choice 
for this. Indeed, let us consider a $K$-manifold $V$ covered by charts 
$U_i$'s ($i \in I$) which are open subset of $K$-Banach $E_i$'s. If $x$ 
is a point in $V$, the tangent space $T_x V$ is by definition isomorphic 
to $E_i$ for each index $i$ such that $x \in V_i$. A natural norm on 
$T_x V$ is then the one obtained by pulling back the norm on $E_i$. 
However, since the transition maps are not required to be isometries, 
this norm do depend on the choice on $i$. They are nevertheless all
equivalent because the transition maps are required to be continuous.

In the next Lemma, we choose to endow $T_x V$ by one of the above norms.

\begin{lem}
Let $V$ and $W$ be two $K$-manifolds. 
Suppose that we are given a differentiable function $f : V \to W$, 
together with a point $x \in V$ such that $f'(x) : T_x V \to T_{f(x)} W$ 
is surjective. 

Then for all $\rho \in (0,1]$, there exists $\delta \in (0,1)$ such 
that for all $r \in (0, \delta)$, the equality
$$f(x + O(H)) = f(x) + O(f'(x)(H))$$
holds for all lattices $H$ in $T_x V$ such that $B^{\phantom -}_{T_x V}(\rho r)
\subset H \subset B^-_{T_x V}(r)$.
\end{lem}

\begin{proof}
Apply Lemma \ref{lem:main} in charts.
\end{proof}

\begin{rmk}
The constant $\delta$ that appears in the above Lemma depends (up to
some multiplicative constant) on the norm that we have chosen on $T_x V$. 
However, once this norm is fixed, and assuming further that $V$ and $W$ 
are locally analytic $K$-manifolds and the mapping $f$ is locally 
analytic as well, the constant $\delta$ can be made explicit using the 
method of \S \ref{ssec:locanalytic}.
\end{rmk}

\subsection{Examples}

\todofor{David}{Elliptic curves and grassmannians}

\section{Differential of usual operations}
\label{ssec:differentials}

We continue to work over a fixed ultrametric field $K$, whose absolute 
value is denoted by $|\cdot|$. We recall that usual examples of such $K$ 
are the field of $p$-adic numbers $\Q_p$ and the field of Laurent series 
$k((X))$ over a base field $k$. The aim of this appendix is to compute 
the differential of many usual operations on numbers, polynomials and 
matrices. Surprisingly, we observe that all differentials we will 
consider are rather easy to compute even if the underlying operation is 
quite involved (\emph{e.g.} Gr\"obner basis).

In what follows, we use freely the ``method of physicists'' to compute 
differentials: given a function $f$ differentiable at some point $x$, we 
consider a small perturbation $dx$ of $x$ and write $f(x+dx) = y + dy$ 
by expanding LHS and neglecting terms of order $2$. The differential of 
$f$ at $x$ is then the linear mapping $dx \mapsto dy$.

\subsection{Numbers}

The most basic operations are, of course, sums, products and inverses of 
elements of $K$. Their differential are well-known and quite easy to 
compute: if $z = x + y$ (resp. $z = xy$, resp $z = \frac 1 x$), we have 
$dz = dx + dy$ (resp. $dz = x \cdot dy + dx \cdot y$, resp $dz = - 
\frac{dx}{x^2}$).

A little bit more interesting is the $n$-th power map $f$ from $K$ to 
itself. Its differential $f'(x)$ is obtained by differentiate the 
equality $y = x^n$: we obtain $dy = n x^{n-1} dx$.
Hence $f'(x)$ maps to ball $B_K(r)$ (where $r$ is some positive real 
number) to $B_K(|n|{\cdot} r)$. According to Lemma \ref{lem:main}, this 
means that the behaviour of the precision depends on the absolute value 
of $n$. By the ultrametric inegality, we always have $|n| \leq 1$ but 
this inegality might be strict. It happens for instance if the 
characteristic of $K$ is divisible by $p$, in which case $n$ vanishes in 
$K$. In that case $f'(x)$ also vanishes and Lemma \ref{lem:main} does 
not apply. 
Another situation (which is more interesting) is that of $p$-adic 
numbers: let us take $K = \Q_p$ and $n = p^k$ for some integer $k$. Then
$|n| = p^{-k}$ and Lemma \ref{lem:main} reflects the well-known fact that
raising a $p$-adic number to the $p^k$-th power increases the precision 
by $k$ extra digits.

\subsection{Univariate polynomials}
\label{ssec:polynomials}

For any integer $d$, let us denote by $K_{< d}[X]$ the set of 
polynomials over $K$ of degree $< d$. It is a finite dimensional vector 
space of dimension $d+1$. The affine space $X^d + K_{< d}[X]$ is then 
the set of monic polynomials over $K$ of degree $d$. We denote it by 
$K_d[X]$.

\subsubsection*{Evaluation and interpolation}

Beyond sums and products (which can be treated as before), two basic 
operations involving polynomials are evaluation and interpolation.
Evaluation is modelised by the function $(P,x) \mapsto P(x)$ where
$P$ is some polynomial and $x$ some element in $K$. Differentiating
it can be done by computing
\begin{equation}
\label{eq:diffeval}
(P + dP)(x + dx) = P(x + dx) + dP(x + dx) = P(x) + P'(x) dx + dP(x)
\end{equation}
by neglecting terms of order $2$. Here $P'$ denotes the derivative of 
$P$. The differential at $P$ is then the linear map $(dP, dx) \mapsto 
P'(x) dx + dP(x)$.

As for interpolation, we consider a positive integer $d$ and the partial 
function $f : K^{2d} \to K_{< d}[X]$ which maps the tuple $(x_1, y_1, 
\ldots, x_d, y_d)$ to the polynomial $P$ of degree less than $d$ such 
that $P(x_i) = y_i$ for all $i$. The polynomial $P$ exists and is unique 
as soon as the $x_i$'s are pairwise distinct; the above function $f$ is 
then defined on this open set. Furthermore, if $(x_1, y_1, \ldots, x_d, 
y_d)$ is a point in it and $P$ denotes the corresponding interpolation 
polynomial, Eq.~\eqref{eq:diffeval} shows that $d y_i = P'(x_i) dx_i + 
dP(x_i)$ for all $i$. For this, we can compute $dP(x_i)$ from $d x_i$ 
and $d y_i$ and finally recover $dP$ by performing a new interpolation.

\subsubsection*{Euclidean division}

Let $A$ and $B$ be two polynomials $B \neq 0$. The Euclidean division of 
$A$ by $B$ is written $A = BQ + R$ with $\deg R < \deg B$. 
Differentiating the above equality we find:
$dA - dB \cdot Q = B \cdot dQ + dR$,
which implies that $dQ$ and $dR$ are respectively obtained as the 
quotient and the remainder of the Euclidean division of $dA - dB \cdot 
Q$ by $B$. This gives the differential. We note that the discussion 
above extends readily to convergent series (see also 
\cite{caruso-lubicz:14a}).

\subsubsection*{Greatest common divisors and B\'ezout coefficients}

We fix two positive integers $n$ and $m$ with $n \geq m$. We consider the 
function $f : K_n[X] \times K_m[X] \to (K_{\leq n}[X])^3$ which sends a 
couple $(A,B)$ to the triple $(D, U, V)$ where $D$ is the \emph{monic} 
greatest common divisor of $A$ and $B$ and $U$ and $V$ are the B\'ezout 
coefficients of minimal degrees (they are those computed by extended 
Euclide algorithm).
The nonvanishing of the resultant of $A$ and $B$ defines a Zariski open 
subset $\mathcal V_0$ where the function $\gcd$ takes the constant value 
$1$. On the contrary, outside $\mathcal V_0$, $\gcd(A,B)$ is a polynomial 
of positive degree. This implies that $f$ is not continuous outside 
$\mathcal V_0$. On the contrary, on $\mathcal V_0$, the function $f$ is
differentiable, and actually locally analytic.

Of course, on $\mathcal V_0$, the first component $D$ is constant and 
therefore $dD = 0$. To compute $dU$ and $dV$, one can simply 
differentiate the B\'ezout relation $AU + BV = 1$. We get:
$$A \cdot dU + B \cdot dV = - (dA \cdot U + dB \cdot V)$$
from which we deduce that $dU$ (resp. $dV$) is obtained as the 
remainder in the Euclidean division of $U{\cdot}dX$ by $B$ (resp. of 
$V{\cdot}dX$ by $A$) where $dX = - (dA \cdot U + dB \cdot V)$.

In order to go further and differentiate $f$ outside $\mathcal V_0$, we 
define, for all integer $i$, the subset $\mathcal V_i$ of $K_n[X] \times 
K_m[X]$ as the locus where the $\gcd$ has degree $i$. The theory of 
subresultants shows that $\mathcal V_i$ is locally closed with respect to 
the Zariski topology. In particular, it defines a $K$-manifold in the 
sense of Appendix \ref{sec:manifold}. This is interesting because the 
restriction of $f$ to $\mathcal V_i$ is now differentiable. To compute
its differential, we proceed along the same lines as before: we 
differentiate the relation $AU + BV = D$ and obtain this way
$A \cdot dU + B \cdot dV - dD = dX$
with $dX = - (dA \cdot U + dB \cdot V)$. In the above relation, the
first two terms $A{\cdot}dU$ and $B{\cdot}dV$ are divisible by $D$ 
whereas the term $dD$ has degree less than $i$. 
Hence if $dX = D \cdot dQ + dR$ is the Euclidean division of $dX$ by $D$, 
we must have $\frac A D \cdot dU + \frac B D \cdot dV = dQ$ and $dD = 
-dR$. These relations, together with bounds on the degree of $U$ and $V$,
imply as before that $dU$ (resp. $dV$) are equal to the remainder in the
Euclidean division of $U{\cdot}dQ$ by $\frac B D$ (resp. of $V{\cdot}dQ$
by $\frac A D$).

\medskip

The lesson we may retain from this study is the following. If we have to 
compute the greatest common divisor of two polynomials $A$ and $B$ known 
with finite precision, we first need to determine what is its degree. 
However, the degree function is not continuous --- it is only upper 
semi-continuous --- and hence cannot be determined from sure from $A$ and 
$B$ (expect if they are coprime). We then need to make an extra 
hypothesis and the more natural one is arguably to assume that 
$\gcd(A,B)$ has actually the maximal degree that it can have. Once this
assumption is made, the computation is possible and one can apply Lemma 
\ref{lem:main} to determine the precision of the result.
Note that with the above convention, the $\gcd$ of $A$ and $A$ is $A$ 
itself although there exist pair of coprime polynomials in any 
neighborhood of $(A,A)$.

\subsubsection*{Factorization}

Suppose that we are given a polynomial $P_0 \in K_d[X]$ written as a 
product $P_0 = A_0 B_0$ where $A_0$ and $B_0$ are monic and coprime. 
Hensel's lemma implies that there exists a small neighborhood $\mathcal 
U$ of $P_0$ in $K_d[X]$ such that any $P \in \mathcal U$ factors 
uniquely as $P = A B$ with $A$ and $B$ monic and close enough to $A_0$ 
and $B_0$ respectively. Thus, we can consider the map $f : P 
\mapsto (A,B)$ defined on the subset of $\mathcal U$ consisting of monic 
polynomials. We want to differentiate $f$ at $P_0$. For this, we 
differentiate the equality $P = A B$ around $P_0$, obtaining
\begin{equation}
\label{eq:difffactor}
dP = A_0 \cdot dB + B_0 \cdot dA.
\end{equation}
where $dP$, $dA$ and $dB$ have degree less than $\deg P$, $\deg A$ and 
$\deg B$ respectively. If $A_0 U_0 + B_0 V_0 = 1$ is a Bezout relation
between $A_0$ and $B_0$, it follows from Eq.~\eqref{eq:difffactor} that
$dA$ (resp. $dB$) is the remainder in the Euclidean division of $V_0
{\cdot} dP$ by $A_0$ (resp. of $U_0 {\cdot} dP$ by $B_0$).

\subsubsection*{Finding roots}

An important special case of the previous study occurs when $A_0$ has 
degree $1$, that is $A_0(X) = X - \alpha_0$ with some $\alpha_0 \in K$. 
The map $P \mapsto A$ is then nothing but the mapping that 
follows the simple root $\alpha_0$. Of course, its differential around 
$P_0$ can be computed by the above method. Nevertheless there is a more 
direct way to obtain it in this particular case: we write $(P_0 + 
dP)(\alpha_0 + d\alpha) = 0$ and expand this. We obtain this way 
$P_0'(\alpha_0) d\alpha + dP(\alpha_0) = 0$ where $P'$ denotes the 
derivative of $P$. Since $\alpha_0$ is a simple root, $P_0'(\alpha_0)$ 
does not vanish and we find:
$$d \alpha = - \frac{dP(\alpha_0)}{P'_0(\alpha_0)}.$$

We now address the case of a multiple roots. Let $P_0$ be a monic 
polynomial of degree $d$ and $\alpha_0 \in K$ be a root of $P_0$ having 
multiplicity $m > 1$. Due to the multiplicity, it is no longer possible 
to follow to root $\alpha_0$ in a neighborhood of $P_0$. But it is if we 
restrict ourselves to polynomials which have a root of multiplicity $m$ 
close to $\alpha_0$. More precisely, let us consider the locus $\mathcal 
V_m$ consisting of monic polynomials of degree $d$ having a root of 
multiplicity at least $m$. It is a Zariski close subset of $K_d[X]$ and 
it contains by assumption the polynomial $P_0$ we started with. 
Moreover, the irreducible components of $\mathcal V_m$ that meet at 
$P_0$ are in bijection with the set of roots of $P_0$ of multiplicity 
$\geq m$. In particular $\alpha_0$ correspond to one of these 
irreducible components; let us denote it by $\mathcal V_{m,\alpha_0}$. 
The algebraic variety $\mathcal V_{m,\alpha_0}$ is a \emph{a fortioti} 
$K$-manifold. Moreover there exists a differentiable function $f$ which 
is defined on a neighborhood of $P_0$ in $\mathcal V_{m,\alpha_0}$ and 
follows the root $\alpha_0$, \emph{i.e.} $f(P_0) = \alpha_0$ and for all 
$P$ such that $f(P)$ is defined, $\alpha = f(P)$ is a root of 
multiplicity (at least) $m$ of $P$. The existence of $f$ follows from 
Hensel's Lemma applied to the factorisation $P_0(X) = (X-\alpha_0)^m 
B_0(X)$ where $B_0(X)$ is a polynomial which is coprime to $X - 
\alpha_0$. We can finally compute the differential of $f$ at $P_0$ 
(along $\mathcal V_{m,\alpha_0}$) by differentiating the equality
$P^{(m-1)}(\alpha) = 0$ where $P^{(i)}$ denotes the $i$-th derivative
of $P$. We find:
\begin{equation}
\label{eq:dalphamult}
d \alpha = - \frac{dP^{(m-1)}(\alpha_0)}{P_0^{(m)}(\alpha_0)}.
\end{equation}

The above computation has actually interesting consequences on $p$-adic 
precision. Let us illustrate this with an exemple. Consider the monic 
polynomial $P(X) = X^2 + O(p^{2N}) X + O(p^{2N})$ (where $N$ is some 
large integer) and assume that we are asking a softwate to compute one 
of its roots. What is the right answer? We remark that, if $\alpha$ is 
any $p$-adic number divisible by $p^N$, then $P(X)$ can be $X^2 - 
\alpha^2$ whose roots are $\pm \alpha$. Conversely, we can prove that 
the two roots of $P$ are necessarily divisible by $p^N$. Hence, the
right answer is certainly $O(p^N)$. Nevertheless, if we know in addition 
that $P$ has a double root, say $\alpha$, we can write $P(X) = (X - 
\alpha)^2$ and identifying the coefficient in $X$, we get $\alpha =
O(p^{2N})$ if $p > 2$ and $\alpha = O(p^{2N-1})$ if $p=2$.
This is exactly the result we get by applying Lemma \ref{lem:main} and
using Eq.~\eqref{eq:dalphamult} to simplify the differential.

This phenomenon is general: if $P$ is a polynomial over $\Q_p$ known at 
some finite precision $O(p^N)$, a root $\alpha$ of $P$ having possibly 
multiplicity $m$ (\emph{i.e.} $P(\alpha), P'(\alpha), \ldots, P^{(m-1)} 
(\alpha)$ vanish at the given precision) can be computed at precision 
roughly $O(p^{N/m})$. However, if we know in advance that $\alpha$ has 
indeed multiplicity $m$ then we can compute it at precision $O(p^{N-c})$ 
where $c$ is some constant (depending on $P$ and $\alpha$ but not on
$N$).

\subsection{Multivariate polynomials}

We consider the ring $K[\XX] = K[X_1, \ldots, X_n]$ of polynomials in $n$
variables over $K$ and fix a monomial order on it. 

\subsubsection*{Division}

We then have a notion of division in $K[\XX]$: if $f, f_1, \ldots, f_s$ 
are polynomials in $K[\XX]$, there exists a writing:
$$f = q_1 f_1 + \cdots + q_s f_s + r$$
where $q_1, \ldots, q_s, r \in K[\XX]$ and no term of $r$ is divisible
by the leading term of some $f_i$ ($1 \leq i \leq s$). Moreover, assuming
that $(f_1, \ldots, f_s)$ is a Gr\"obner basis\footnote{Without this
assumption, there still exists a canonical choice of $r$. We do not know
if the computation of the differential that follows extends to a more
general setting.} (of the ideal generated by these polynomials), the 
polynomial $r$ is uniquely determined and called the \emph{remainder} of 
the division of $f$ by the family $(f_1, \ldots, f_s)$. The map 
$(f, f_1, \ldots, f_s) \mapsto r$ is then well defined and similarly to 
the case of univariate polynomials, we can compute its differential: we 
find that $dr$ is obtained as the remainder of the division of $f - (q_1 
\cdot d f_1 + \cdots + q_s \cdot d f_s)$ by $(f_1, \ldots, f_s)$.

\subsubsection*{Gr\"obner basis}

Recall that any 
ideal $I \subset K[\XX]$ admits a unique reduced Gr\"obner basis. We can 
then consider the map sending a family $(f_1, \ldots, f_s)$ of 
\emph{homogeneous}\footnote{This restriction will be convenient for us 
mainly because we are using the article \cite{vaccon:14a}, which deals only 
with homogeneous polynomials. It is however probably not essential.} 
polynomials of fixed degrees to the reduced Gr\"obner basis $(g_1, 
\ldots, g_t)$. It follows from Theorem 1.1 of \cite{vaccon:14a} that this 
map is continuous at $(f_1, \ldots, f_s)$ provided that:
\begin{itemize}
\item the sequence $(f_1, \ldots, f_s)$ is regular
\item for all $i \in \{1, \ldots, s\}$, the ideal generated by
$f_1, \ldots, f_i$ is wearly-$w$ where $w$ denotes the fixed monomial
order (\emph{cf} \cite{vaccon:14a} for more precisions).
\end{itemize}
A similar argument proves that it is actually differentiable at such
points. We are now going to compute the differential. For this we
remark that since the families $(f_1, \ldots, f_s)$ and $(g_1, \ldots,
g_t)$ generate the same ideal, we have a relation:
\begin{equation}
\label{eq:prodgrobner}
(g_1, \ldots, g_t) = (f_1, \ldots, f_s) \cdot A
\end{equation}
where $A$ is a $(s \times t)$ matrix with coefficients in $K[\XX]$.
Moreover following Vaccon's construction, we see that the entries of $A$ 
can be chosen in such a way that they define differentiable functions.
Differentiating Eq.~\eqref{eq:prodgrobner}, we get:
$$(d g_1, \ldots, d g_t) =
(d f_1, \ldots, d f_s) \cdot A + (f_1, \ldots, f_s) \cdot dA.$$
Finally, using that we are working with \emph{reduced} Gr\"obner
basis, the above equality implies that $d g_i$ is the remainder in
the division of the $i$-th coefficient of the product
$(d f_1, \ldots, d f_s) \cdot A$ by the family $(g_1, \ldots, g_t)$.

\subsection{Matrices}

Differentiating ring operations (sum, products, inverse) over matrices 
is again straightforward; we just need to be careful that matrix 
algebras are (in general) noncommutative.

\subsubsection*{Determinants and characteristic polynomials}

Let us first compute the differential of the function $\det : M_n(K) \to 
K$. It is a standard computation. To begin with, we consider an 
invertible matrix $M$ and write:
\begin{eqnarray*}
\det(M + dM) &=& \det(M) \cdot \det(I + M^{-1} \cdot dM)  \\
&=& \det(M) \cdot \big(1 + \tr(M^{-1} \cdot dM)\big) \\
&=& \det(M) + \tr(\com(M) \cdot dM)
\end{eqnarray*}
where $\com(M)$ denotes the comatrix of $M$. The differential of 
$\det$ at $M$ is then $dM \mapsto \tr(\com(M) \cdot dM)$. It turns
out that this formula is still valid when $M$ is not invertible.
The same computation extends readily to characteristic polynomials,
since they are defined as determinants. More precisely, let us 
consider the function $\chi : M_n(K) \to K_n[X]$ taking a matrix 
$M$ to its monic characteristic polynomial $\det(X-M)$.
Then $\chi$ is differentiable at each point $M \in M_n(K)$ and its 
differential is given by $dM \mapsto \tr(\com(X{-}M) \cdot dM)$.

\subsubsection*{LU factorization}

Let us agree to define a 
LU factorization of a square matrix $M \in M_n(K)$ as a writing $M = LU$ 
where $L$ is lower triangular and unipotent and $U$ is upper triangular. 
Such a writing exists and is unique provided that all principal minors 
of $M$ do not vanish. We can then consider the mapping $M \mapsto (L,U)$ 
defined over the Zariski-open set of matrices satisfying the above 
condition. In order to differentiate it, we differentiate the relation 
$M = LU$ and rewrite the result as follows:
$$L^{-1} dM \: U^{-1} = L^{-1} \cdot dL + dU \cdot U^{-1}.$$
We remark that in the right hand side of the above formula, the first
summand is lower triangular and all its diagonal entries vanish whereas 
the second summand is upper triangular. Hence in order to compute $dL$
and $dU$, one can proceed as follows: (1)~we compute the product $dX = 
L^{-1} dM \: U^{-1}$, (2)~we separate the lower and upper part of $dX$
obtaining this way $L^{-1} \cdot dL$ and $dU \cdot U^{-1}$ and (3)~we
recover $dL$ and $dU$ by multiplying the above matrices by $L$ on the 
left and $U$ on the right respectively. 
The above discussion extends almost \emph{verbatim} to LUP 
factorization; the only difference is that LUP factorizations are not 
unique but they are on a small neighborhood of $M$ if we fix the matrix 
$P$.

\subsubsection*{QR factorization}

For us, a QR factorization of a square matrix $M \in M_n(K)$ will be
a writing $M = QR$ where $R$ is unipotent upper triangular and $Q$ is
orthogonal in the sense that $\trans Q \cdot Q$ is diagonal. As before, such 
a writing exists and is unique on a Zariski-open subset of $M_n(K)$. The
mapping $f : M \mapsto (Q,R)$ is then well defined on this subset. We 
would like to emphasize at this point that the orthogonality condition
defines a sub-manifold of $M_n(K)$ which is a apparently \emph{not} a
vector space (it is defined by equations of degree $2$). The codomain
of $f$ is then also a manifold; this example then fits to the setting
of \S \ref{sec:manifold} but not to those of \S \ref{sec:mainlemma}. 
The generalization made in \S \ref{sec:manifold} is then needed.
Anyway, we can differentiate $f$ by following the method we used for LU 
factorization: differentiating the relation $M = QR$, we obtain:
\begin{equation}
\label{eq:diffQR}
\trans Q \cdot dM \cdot R^{-1} = \trans Q \cdot dQ + \Delta \cdot dR 
\cdot R^{-1}
\end{equation}
where $\Delta = \trans Q \cdot Q$ is a diagonal matrix by definition.
Moreover by differentiating $\trans Q \cdot Q = \Delta$, we find that
$\trans Q \cdot dQ$ can be written as the sum of an antisymmetric 
matrix and a diagonal one. Since moreove $dR \cdot R^{-1}$ is upper
triangular with all diagonal entries equal to $0$, we see that 
Eq.~\eqref{eq:diffQR} is enough to compute $dQ$ and $dR$ from $Q$, $R$ 
and $dM$.

\subsection{Vector spaces}

Let $d$ and $n$ be two nonnegative integers such that $d \leq n$. The 
Grassmannian $\Grass(d,n)$ is the set of all sub-vector spaces of $K^n$ 
of dimension $d$. It defines an algebraic variety over $K$ and hence
\emph{a fortiori} a $K$-manifold in the sense of \S \ref{ssec:manifold}.
Concretely, a vector space $V \subset K^n$ of dimension $d$ is given by 
a rectangular matrix $M \in M_{d,n}(K)$ whose rows form a basis of $V$ 
and two such matrices $M$ and $M'$ define the same vector space if there 
exists $P \in \GL_d(K)$ such that $M = P M'$. Performing row echelon, we 
find that we can always choose the above matrix $M$ in the particular
shape:
$$M = \begin{pmatrix} I_d & M' \end{pmatrix} \cdot P$$
where $I_d$ denotes the $(d \times d)$ identity matrix, $M' \in M_{d, 
n-d}(K)$ and $P$ is a permutation matrix of size $n$. Moreover two 
such writings with the same $P$ necessarily coincide. Hence each 
permutation matrix $P$ defines a chart $U_P \subset \Grass(d,n)$ which
is canonically diffeomorphism to $M_{d, n-d}(K) \simeq K^{d(n-d)}$.

\subsubsection*{Left kernels}

We consider the open subset $\mathcal V_{n-d}$ of $M_{n,n-d}(K)$ 
consisting of matrices of full rank, \emph{i.e.} rank $n-d$. The left 
kernel defines a mapping $\text{LK} : M_{n,n-d}(K) \to \Grass(d,n)$. Let 
us prove that it is differentiable and compute its differential around 
some point $M \in M_{n,n-d}(K)$. Of course, there exists a neighborhood 
$\mathcal U$ of $M$ whose image is entirely contained in a given chart 
$U_P$. We assume for simplicity that $P$ is the identity. On $\mathcal 
U$, the map $\text{LK}$ corresponds in our chart to the map that 
takes $M$ to the unique matrix $N \in M_{d, n-d}(K)$ such that 
$\begin{pmatrix} I_d & N \end{pmatrix} \cdot M = 0$. The implicit 
function theorem then implies that $\text{LK}$ is differentiable. 
Furthermore, its differential satisfies the relation
$\begin{pmatrix} 0 & dN \end{pmatrix} \cdot M +
\begin{pmatrix} I_d & N \end{pmatrix} \cdot dM = 0$,
from which we can compute $dN$ by projecting on the $(n-d)$ last
columns.

Following what we have already done in \S \ref{ssec:polynomials} for 
greatest common divisors of polynomials, we can develop further this 
example and study what happens on the closed subset of $M_{n,n-d}(K)$ 
where matrices have not full rank. On this subspace, the left kernel has 
dimension $< d$ and then no longer defines a point in $\Grass(n,d)$. 
Nevertheless, for all integer $r < n-d$, we can consider the subset 
$\mathcal V_r \subset M_{n,n-d}(K)$ of matrices whose rank are exactly 
rank $r$. It is locally closed in $M_{n,n-d}(K)$ with respect to the 
Zariski topology and hence defines a $K$-manifold. Furthermore, we have a 
mapping $\text{LK}_r : \mathcal M_r \to \Grass(n,n-r)$ which is 
differentiable and whose differential can be computed as before.

\subsubsection*{Intersections}

We pick $n$, $d_1$ and 
$d_2$ three nonnegative integers such that $d_1 \leq n$, $d_2 \leq n$ 
and $d_1 + d_2 \geq n$. Two subspaces of $K^n$ of dimension $d_1$ and 
$d_2$ respectively meet along a subspace of dimension at most $d_1 + d_2 
- n$. For all $d \leq d_1 + d_2 -n$, we can then define the subspace 
$\mathcal V_d$ of $\Grass(n,d_1) \times \Grass(n,d_2)$ consisting of 
pairs $(E_1, E_2)$ such that $\dim (E_1 \cap E_2) = d$ and consider the 
function $f_d : \mathcal V_d \to \Grass(n, d)$ which sends $(E_1, E_2)$ 
to $E_1 \cap E_2$. In charts, the function $f_d$ can be interpreted as
the kernel of a matrix simply because $E_1 \cap E_2$ appears as the
kernel of the canonical linear map $E_1 \oplus E_2 \to K^n$. We deduce
for this that $f_d$ is differentiable and that its differential can be
computed similarly to what we have done before.

\subsubsection*{Sums and images}

Finally, we note that similar results hold for images of matrices and,
consequently, sums of subspaces.

\bibliographystyle{plain}
\bibliography{roebib/Biblio}

\end{document}
