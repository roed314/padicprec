%\documentclass{sig-alternate}
\documentclass{lms}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,french,linesnumbered]{algorithm2e}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
%\theoremstyle{definition}
\newtheorem{deftn}[theo]{Definition}
\newtheorem{rmk}[theo]{Remark}

\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\R}{\mathbb R}
\renewcommand{\O}{\mathcal O}
\newcommand{\XX}{\mathbf X}
\newcommand{\trans}{{}^{\text t}}

\newcommand{\GL}{\text{\rm GL}}

\newcommand{\val}{\text{\rm val}}
\newcommand{\tr}{\text{\rm Tr}}
\newcommand{\com}{\text{\rm Com}}
\newcommand{\Grass}{\text{\rm Grass}}
\renewcommand{\prec}{\text{\rm prec}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\title{$p$-adic precision}
\author{Xavier Caruso}
\author{David Roe}
\author{Tristan Vaccon}
\date\today

\maketitle
\begin{abstract}
We present a new method to handle $p$-adic precision (or more generally
precision over ultrametric fields) based on $p$-adic calculus.
We illustrate it by many examples and give a toy application to the
stable computation of the SOMOS 4 sequence.
\end{abstract}

\setcounter{tocdepth}{1}
\tableofcontents

\section{Introduction}

During the last two decades, $p$-adic methods have become a tremendous tool in computer algebra, since it allows computation not feasible on finite fields, can control the growth of the size of the coefficients when dealing with rationnal numbers, or is needed when performing algorithm of $p$-adic nature.

Yet, $p$-adic algorithms are necessarily numeric or symbolic-numeric: one can not avoid working with finite precision, and thus, keeping track of loss in precision during the computation.

Here are some examples of algorithm that rely on $p$-adic computations:
\begin{itemize}
\item Bostan \cite{Bostan}, on how polynomials in $\mathbb{F}_p$ can be computed from theirs Newton series lifted in $\mathbb{Z}_p$;
\item Gaudry \cite{Gaudry}, on generation of genus $2$ curves via $p$-adic lifting;
\item Kedlaya \cite{Kedlaya}, on point counting on hyperelliptic curves using Monsky--Washnitzer cohomology;
\item Lercier \cite{Lercier}, on computing isogenies between elliptic curves using $p$-adic differential equations.
\end{itemize}

Many usual softwares (\emph{e.g.} {\tt magma} and {\tt sage}) include 
primitives to handle $p$-adic numbers. Since $p$-adic numbers are by 
nature infinite, one needs to truncate them up to some finite level (we 
generally say \emph{finite precision}) in order to manipulate them on a 
computer. The usual technics to achieve this are inspired from case of 
real numbers: the computer works with ``truncated $p$-adic numbers'' 
represented as $x + O(p^n)$ and performs all basic operations (addition, 
product) on this representation. For instance, it computes: 
$$(x + O(p^n)) + (y + O(p^m)) = (x+y) + O(p^{\min(n,m)}).$$
However, in many important cases (\emph{e.g.} those discussed just 
above), this approach is too naive since it creates artificially 
important losses of precision (see \S\ref{ssec:stepbystep}). 
It is the so-called \emph{numerical instability} issue.

The aim of the current paper is to provide a systematical theorical 
study of $p$-adic precision (and more generally nonarchimedian 
precision) and to derive from this study new methods to handle 
precision, that turns out to give nearly optimal results in a quite 
large setting.

\section{Motivations and aims}

Two sources of inspiration arise when studying $p$-adic algorithms.
The first relates $\Zp$ to its quotients $\Z/p^n\Z$.  The preimage in
$\Zp$ of an element $a \in \Z / p^n \Z$ is a ball, and these balls cover
$\Zp$ for any fixed $n$.  Since the projection $\Zp \to \Z/p^n\Z$ is a
homomorphism, given unknown elements in two such balls we can
locate the balls in which their sum and product lie.  Working on a computer
we must find a way to write elements using only a finite amount of data.
By lumping elements together into these balls of radius $p^{-n}$, we may
model arithmetic in $\Zp$ using the finite ring $\Z / p^n \Z$.  In Sage
this method for approximating elements is known as ``fixed modulus''.

The second source draws upon parallels between $\Qp$ and $\R$.  Both
occur as completions of $\Q$, and both have dense subrings:
the localizations $\Z[\frac1p] \subset \Qp$ and $\Z[\frac12] \subset \R$.
In $\R$, floating point arithmetic provides approximate operations $\oplus$ and $\odot$ on a
subset $S \subset \Z[\frac12]$ that model $+$ and $\cdot$ in $\R$ up to a given relative precision $h$:
\[
\frac{x \circledast y}{x \ast y} - 1 \le 2^{-h}
\]
for all $x, y \in S$ and $\ast \in \{+, \cdot\}$.  Interval arithmetic in the reals
is unwieldy, since arithmetic operations always increase the lengths of the inputs.
As a consequence, most computations in the real numbers rely on
statistical cancelation and external estimates of precision loss, rather than
attempting to track known precision at each step.  This tendency is strengthened
by the ubiquity of floating point arithmetic in scientific applications, where Gaussian
distributions are more common than intervals anyway.

In the $p$-adic world, these features are reversed.

\todofor{David}{Add a general introduction to this section.}

\subsection{Separate precision and approximation}

\todofor{David}{
Explain here why it might be useful to work separately with 
approximation on the one hand and precision on the other hand.
}

\subsection{Step-by-step tracking is not sharp}
\label{ssec:stepbystep}

In this subsection, we illustrate by some examples that tracking
$p$-adic precision step by step may lead to important numerical
instability.

\subsubsection*{A first stupid example}

Let $f : \Qp^2 \to \Qp^2$ be the function mapping $(x,y)$ to $(x+y, 
x-y)$. We consider in addition two $p$-adic numbers $x$ and $y$, 
which are known up to precision $O(p^n)$ and $O(p^m)$ respectively.
The question we would like to address in this example is the
computation of $f \circ f(x,y)$.

Two approaches are possible. The first one consists in doing the 
computation step-by-step: we first compute $(z,t) = f(x,y)$ and we 
compute then $f(z,t)$ which gives the result. Proceeding this way, we 
see that $z$ and $t$ are both known up to precision $O(p^{\min(n,m)})$ 
are consequently the same holds for the coordinates of the result.
The second approach consists in remarking that $f \circ f$ maps
$(x,y)$ to $(2x, 2y)$. Computing $f \circ f(x,y)$ using the latter
formula, it is clear that the result we get is known up to precision
$(O(p^n), O(p^m))$ --- and even $(O(p^{n+1}), O(p^{m+1}))$ if $p = 2$.
As a conclusion, the second approach yields a more accurate result.
In other words, computing step-by-step creates ``artificial'' losses 
of precision.

\subsubsection*{The SOMOS 4 sequence}

The SOMOS 4 sequence is a four-term inductive sequence satisfying:
$$u_{n+4} = \frac{u_{n+1} u_{n+3} + u_{n+2}^2}{u_n}.$$
In this example, we shall consider the situation where the initial
terms $u_0$, $u_1$, $u_2$ and $u_3$ are \emph{invertible} $p$-adic 
integers, \emph{i.e.} lie in $\Zp^\times$.

Let us first examine what happens (regarding to the precision) when
we are computing the sequence $(u_n)$ naively. The computation of
$u_{n+4}$ involves a division by $u_n$ and hence, roughly speaking,
decreases the precision by a factor $p^{\val(u_n)}$ where $\val$
stands for the $p$-adic valuation. Hence, if we assume that $u_0$,
$u_1$, $u_2$ and $u_3$ are given with precision $O(p^N)$ then the
naive computation returns the value of $u_n$ with precision
\begin{equation}
\label{eq:SOMOS}
O(p^{N-v_n})
\quad \text{with}\quad
v_n = \val(u_0) + \cdots + \val(u_{n-4}).
\end{equation}

However, on the other hand, one can prove that the SOMOS 4 sequence 
exhibits the \emph{Laurent phenomenon}. This means that, for all integer 
$n$, there exists a polynomial $P_n$ in $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 
1}, T^{\pm 1}]$ such that $u_n = P_n(u_0, u_1, u_2, u_3)$ (see \cite{Fomin-Zelevinsky}).
From the latter formula, it follows directly that if $u_0$, $u_1$,
$u_2$ and $u_3$ are known up to precision $O(p^N)$ then all $u_n$'s
are also known with the same precision. Thus, the term $v_n$ that 
appears in \eqref{eq:SOMOS} does not reflect an intrinsic loss of
precision but some numerical instability related to the algorithm ---
which includes the model precision --- we used to compute to SOMOS 
sequence.

\begin{rmk}
From the above discussion, one can easily derive a numerically stable 
algorithm that computes the SOMOS 4 sequence: (1)~we compute the 
Laurent polynomials $P_n$ using the induction formula (and working in 
the ring $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 1}, T^{\pm 1}]$) and (2)~we 
evaluate $P_n$ at the point $(u_0, u_1, u_2, u_3)$.
However this algorithm is not efficient at all since computing the 
$P_n$'s is very time-consuming for two reasons: first, it requires 
division in a polynomial ring with $4$ variables and, second, the
size of the coefficients of $P_n$ explodes as $n$ grows.

In \S \ref{ssec:SOMOS-solution}, we shall design an algorithm computing 
the SOMOS 4 sequence which turns out to be, at the same time, efficient 
and numerically stable.
\end{rmk}

\subsubsection*{LU factorization}

Let us first recall that a square matrix $M$ with coefficients in $\Qp$ 
admits a LU factorization if it can be written as a product $LU$ where 
$L$ and $U$ is lower triangular and upper triangular respectively. 
The computation of a LU factorization appears as an important tool to 
tackle many classical questions about matrices or linear systems.

We first note that a LU decomposition of a given matrix $M$ is unique
if you require further that the $L$-part of this decomposition is
unipotent (\emph{i.e.} all diagonal entries equal $1$). Let us write
$L(M) U(M)$ the LU factorization of $M$ satisfying the above extra
condition (when it exists). There exists Cramer-type formulas that
gives each individual entries of $L(M)$and $U(M)$ in a closed form
(see \cite{Caruso}).

If $M$ is random square matrix over $\Zp$ of side $d$ whose every
coefficient is known up to precision $O(p^N)$, the two following
results are proved in \cite{Caruso}:
\begin{itemize}
\item using usual Gauss elimination and tracking $p$-adic precision 
step-by-step, the smallest precision on an entry of $L(M)$ is about
$O(p^{N - \frac{2d}{p-1}})$ on average;
\item computing $L(M)$ by evaluating Cramer-type formulas yields a
result whose every entry is known up to precision $O(p^{N - 2 \log_p 
d})$.
\end{itemize}
If $d$ is large compared to $p$, the second precision is much more
accurate than the first one. This shows once again that ``artifical''
losses of precision appear when one performs Gauss elimination by
tracking precision step-by-step.

%\subsubsection*{Algorithms \emph{\`a la} Kedlaya}

\section{A new philosophy}
\label{sec:philosophy}

The aim of this section is to develop a new point of view on $p$-adic 
precision based on $p$-adic analysis. Actually, we shall generalize a 
bit the setting and work over a general ultrametric field (see definition
below).

\subsection{Ultrametric analysis}
\label{ssec:ultrametric}

In this section, we recall usual definitions and properties related to
ultrametric analysis. We refer to Schneider \cite{Schneider} for a 
complete exposition.

An \emph{absolute value} on a field $K$ is a multiplicative map $|\cdot| 
: K \to \R_+$ satisfying the triangle inequality and sending every 
nonzero element of $K$ to a strictly positive real number. It is said 
\emph{ultrametric} if it satisfies the following strong from of the 
triangle inequality:
$$|x+y| \leq \max(|x|, |y|).$$
Given an absolute value $|\cdot|$ on $K$, we note that $|K^\times|$ is a 
multiplicative subgroup of $\R_+^\times$. It is then either discrete or 
dense. We shall say that $|\cdot|$ is \emph{discrete} if $|K^\times|$ is
discrete. The valued field $(K, |\cdot|)$ is said \emph{complete} if all 
Cauchy sequences (with respect to the distance $d$ defined by $d(x,y) = 
|x-y|$) converge.

The formula $|x| = p^{-\val(x)}$ defines an ultrametric absolute value on 
the field $\Qp$ for which it is complete. More generally, the $p$-adic 
absolute value extends uniquely to any finite extension $K$ of $\Qp$ and 
turns $K$ into a complete ultrametric field. Another classical example of 
complete ultrametric field is the field of Laurent series over a finite
field, the absolute value being defined from the valuation of a series.

From now, we fix a field $K$ equipped with an ultrametric absolute value 
denoted by $|\cdot|$ and we assume that $K$ is complete. Given $v$ a 
$K$-vector space (of possibly infinite dimension), an \emph{ultrametric 
norm} on $V$ is a map $\Vert\cdot\Vert : V \to \R^+$ satisfying:
\begin{enumerate}[(i)]
\item $\Vert x\Vert = 0$ if and only if $x = 0$;
\item $\Vert \lambda x\Vert = |\lambda| \cdot \Vert x\Vert$;
\item $\Vert x+y\Vert \leq \max(\Vert x\Vert, \Vert y\Vert)$.
\end{enumerate}
A norm gives rise to a distance and a normed $K$-vector space $V$ is 
said \emph{complete} when any Cauchy sequence with values in $V$ 
converges. A normed $K$-vector space which is complete is called a 
\emph{$K$-Banach}.

Similarly to the real case, any finite dimensional normed $K$-vector 
space is complete and all norms over such a space are equivalent 
(\emph{i.e.} they define the same topology). Many usual Theorems 
concerning Banach spaces (\emph{e.g.} open mapping Theorem, closed graph 
Theorem) remain true in the ultrametric setting.

\begin{deftn} \label{deftn : diff}
Let $V, W$ be two normed $K$-vector spaces, let $U \subset V$ be an open 
subset of $V$ and let $f : U \rightarrow W$ be a map. Then $f$ is called 
\emph{differentiable} at $v_0 \in U$ if there exists a 
continuous linear map $f'(v_0) : U \rightarrow W$ such that for any 
$\varepsilon >0$, there exists an open neighborhood $U_\varepsilon 
\subset U$ containing $v_0$ with:
\[ 
\forall v, w \in U_\varepsilon, \quad
\Vert f(v)-f(w)-f'(v_0) \cdot \left( v-w \right) \Vert 
\leq \varepsilon \Vert v-w \Vert. 
\]
The linear map $f'(v_0)$ is called the \emph{differential} of $f$ at $v_0$.
\end{deftn}

One can derive all usual properties of the differential from this 
definition: unicity, chain rules, \emph{etc}.

\subsection{Lattices}

Let $E$ be a $K$-normed vector space. Given $r \in \R^+$, 
we denote by $B^-_E(r)$ (resp. by $B_E(r)$) the set of elements $x 
\in E$ such that $\Vert x \Vert < r$ (resp. $\Vert x \Vert \leq r$). 
We insist on the following maybe unusual fact: $B^-_E(r)$ and 
$B_E(r)$ are both open and closed for the norm topology on $E$.
Clearly, $B_K(0,1)$ is stable under multiplication. Moreover, the 
ultrametric triangle inequality ensures that $B_K(1)$ is stable under 
addition as well. Thus, $B_K(1)$ is a subring of $K$: it is often 
called the \emph{integer ring} of $K$.

\begin{deftn}
Let $E$ be a $K$-Banach.
A \emph{lattice} in $E$ is an open bounded sub-$B_K(1)$-module of $E$.
\end{deftn}

\begin{rmk}
Any lattice $H$ in $E$ is also closed since its complement is the
union of all cosets $a + H$ (with $a \not\in H$) which are all open.
\end{rmk}

The following Lemma shows that the notion of lattices behaves rather 
well under linear\footnote{We shall see in the sequel (\emph{cf} Lemma 
\ref{lem:main}) that, under some extra hypothesis, the linearity 
assumption can be drastically relaxed.} transformations.

\begin{lem}
\label{lem:morlat}
Let $E$ and $F$ be two $K$-Banach and $f : 
E \to F$ be a continuous \emph{surjective} linear map.
If $H$ is a lattice in $E$, then $f(H)$ is a lattice in $F$.
\end{lem}

\begin{proof}
It is clear that $f(H)$ is a module over $B_K(1)$ which is bounded.
The fact that it is open is a direct consequence of the open mapping
Theorem.
\end{proof}

To each lattice $H$ in $E$, one can attach a norm $\Vert \cdot \Vert_H$ 
on $E$ defined as follows. We pick $x \in E$, $x \neq 0$. We introduce
the set $I_x$ consisting of scalars $\lambda \in K$ such that $\lambda x 
\in H$. It follows from the fact that $H$ is open and bounded in $E$
that $I_x$ is nonempty and bounded in $K$. Therefore, the image $|I_x|$ 
of $I_x$ under the absolute value map is bounded and contains a positive
element. We can then define:
$$\Vert x \Vert_H = (\sup |I_x|)^{-1}.$$
It is an exercise to check that $\Vert \cdot \Vert_H$ is a norm over 
$E$.

\begin{lem} \label{lem:reductionball}
\begin{enumerate}
\item The norms $\Vert \cdot \Vert_H$ and $\Vert \cdot \Vert$ are 
equivalent.
\item Let $B^-_{E,H}(1)$ (resp. by $B_{E,H}(1)$) the set of elements $x 
\in E$ such that $\Vert x \Vert_H < 1$ (resp. $\Vert x \Vert_H \leq 1$).
Then:
$$B^-_{E,H}(1) \subset H \subset B_{E,H}(1)$$
and the last inclusion is an equality if the absolute value on $K$ is
discrete.
\end{enumerate}
\end{lem}

\begin{proof}
Left to the reader.
\end{proof}

The first part of Lemma \ref{lem:reductionball} asserts that the 
identity map $\iota_H : (E, \Vert \cdot \Vert_H) \to (E, \Vert \cdot 
\Vert)$ is an homeomorphism. Furthermore, by the second of the Lemma, 
$H$ is the image under $\iota_H$ of a lattice satisfying an extra 
assumption: it is ``between the open unit ball and the closed unit 
ball''.

\subsubsection*{Lattices and computers}

\todofor{Xavier}{Explain that if $E$ is finite dimensional and $K$ (and 
the absolute value is discrete), a lattice can be encoded by a finite 
amount of informations.}

\subsection{The main Lemma}

\begin{lem} \label{lem:main}
Let $E$ and $F$ be two $K$-Banach and $f : U 
\rightarrow F$ be a function defined on an open subset $U$ of $E$.
We assume that $f$ is differentiable at some point $v_0 \subset 
U$ and that the differential $f'(v_0)$ is surjective. 

Then, there exists a positive real number $\delta$ such that, for all $r 
\in (0, \delta)$, the equality
\begin{equation}
\label{eq:main}
f(v_0 + H) = f(v_0) + f'(v_0) \cdot H.
\end{equation}
holds for each lattice $H$ such that $B^-_E(r) \subset H \subset
B_E(r)$.
\end{lem}

\begin{rmk}
Given a lattice $H_0$ in $E$, the equality \eqref{eq:main} holds for $H 
= \lambda H_0$ with $\lambda$ small enough. To prove this, it is enough 
to apply Lemma \ref{lem:main} to the map $f \circ \iota_{H_0}$ where 
$\iota_{H_0} : (E, \Vert \cdot \Vert_{H_0}) \to (E, \Vert \cdot \Vert)$ 
is the ``identity map'' (which is an homeomorphism by Lemma 
\ref{lem:reductionball}).
\end{rmk}

We may assume that $v_0=0$ and $f(0)=0$. Since $f'(0)$ is surjective, by 
the open mapping Theorem, there exists $C>0$ such that $B_F(1) \subset 
C  f'(0) \cdot B_E(1)$. Let $\varepsilon>0$ be such that 
$\varepsilon C < 1$. Let $U_\varepsilon \subset E$ be given by by definition \ref{deftn : diff} and the fact that $f$ is differentiable at $0$. We may assume $U_\varepsilon = 
B_E(\delta)$ for some $\delta >0$.

Let $r \in (0, \delta)$. Let $B^-_E(r) \subset H \subset
B_E(r).$
Let us show that $f$ maps surjectively $H$ onto $f'(0) \cdot H$.

We first prove that $f(H) \subset f'(0) \cdot H$.
Let $x \in H$. By differentiability at $0$, $\Vert f(x)-f'(0) \cdot x \Vert \leq 
\varepsilon \Vert x \Vert $. Therefore, if $y=f(x)-f'(0) \cdot x,$ then $\Vert y \Vert \leq \varepsilon r$. As a consequence of the open mapping Theorem, $B_F(\varepsilon r) \subset 
\varepsilon C  f'(0) \cdot B_E(r),$ and there exists $x_1 \in \varepsilon C  \cdot B_E(r)$ such that $f'(0) \cdot x_1 =y.$ Since $\varepsilon C <1,$  $x_1 \in B^-_E(r) \subset H.$ and finally, $f(x)= f'(0) \cdot (x-x_1) \in f'(0) \cdot H$.

We shall now prove surjectivity. Let $y \in f'(0) \cdot H$. 
Let $x_0 \in H$ be such that $y = f'(0) \cdot x_0$. Since $f(x_0) \in f'(0) \cdot H,$ we can define $z_0 \in E$ to be such that $f'(0) \cdot z_0 = y - f(x_0)$ and $\Vert z_0 \Vert \leq C \cdot \Vert y - f(x_0) \Vert$.
We then define by induction two sequences $(x_n)$ and $(z_n)$ by
\begin{itemize}
\item $x_{n+1}=x_n+z_n$ and 
\item $z_n \in E$ is such that
$f'(0) \cdot z_n = y - f(x_n)$ and
$\Vert z_n \Vert \leq C \cdot \Vert y - f(x_n) \Vert$.
\end{itemize}
By definition, $\Vert z_0 \Vert < C \Vert y_0\Vert$ and $y 
= -f(x_0)+f'(0) \cdot x_0$. Therefore, $\Vert y \Vert < \varepsilon 
\Vert x_0 \Vert \leq \varepsilon r$. Thus $\Vert z_0 \Vert \leq 
\varepsilon C \cdot r < r$. Thus, $z_0$ and $x_1=x_0+z_0$  lie in $B^-_E(r) 
\subset H$. An easy induction involving an analoguous computation
shows that the sequences $(x_n)$ and $(z_n)$ are well defined and
take their values in $H$.

We notice that $y - f(x_{n+1}) = f(x_n) - f(x_{n+1}) + f'(0) \cdot z_n$.
Since $z_n=x_{n+1}-x_n,$, if we use differentiability, we get:
$$\Vert y  - f(x_{n+1}) \Vert 
\leq \varepsilon \Vert z_n \Vert \leq \varepsilon C \cdot \Vert 
y - f(x_n) \Vert,$$
from what we deduce that $\Vert y - f(x_{n+1}) \Vert = 
O(a^n)$ and $\Vert z_n \Vert = O(a^n)$ with $a = \varepsilon C < 1$.
This shows that $(x_n)$ is a Cauchy sequence and, since $E$ is complete, it converges. If $x$ denotes the limit of $(x_n)$, we have $x \in H.$ Yet, as a consequence of the differentiability, $f$ is continuous over $U_\varepsilon.$ Therefore, since $x \in H \subset U_\varepsilon,$ then  $y=f(x)$. The result is proven.
\end{proof}

\todofor{Tristan}{Improve the end of this Subsection}

Before discussing applications of Lemma \ref{lem:main} to ultrametric 
precision, we would like to describe a little bit more restrictive 
setting where the constant $\delta$ appearing in this Lemma can be 
computed explicitly.
We refer to Schneider \cite{Schneider}, \S 6, for a definition of a 
locally analytic function.

\begin{rmk}
Let $f$ be locally analytic function $K^n \rightarrow K^m$. Let $x \in K^n$ be such that $f'(x)$ is surjective. Then the radius of the ball needed can be read from the Newton polygon given by the norms of the Taylor series expansion of $f$ at $x$. We assume the $C$ given by the Banach-Schauder is given, $\varepsilon = \frac{1}{C}$ and now, we try to recover the link between $\varepsilon$ and $\delta$.
\end{rmk}

\begin{prop}
\label{prop:locanalytic}
\todo{Write a proposition.}
\end{prop}

\begin{proof}
Let us assume that $x=0$, $f(0)=0$, and for $r>0$, $f(x)=f'(0) \cdot x +\sum_{n \geq 2} \frac{f^{(n)}(0)}{n!} \cdot (x, \dots,x)$ is the Taylor series expansion of $f$ at $0$, coinciding on $B_E (0,r)$.

We shall now find $l$ such that if $x \in B_{K^n}(0,l)$, then $\Vert \sum_{n \geq 2} \frac{f^{(n)}(0)}{n!}(x,\dots,x) \Vert \leq \varepsilon$.

By continuity and ultrametricity, it is enough for $l$ to be such that, if $x \in B_{K^n}(0,l)$, then  \[ \sup_{n \geq 2} \interleave f^{(n)}(0) \interleave \frac{\Vert x \Vert^n }{ \vert n! \vert } \leq \varepsilon \Vert x \Vert .\] 

Therefore, \[ \sup_{n \geq 2} \interleave f^{(n)}(0) \interleave \frac{l^{n-1} }{\vert n! \vert } \leq \varepsilon . \] 

In other words, $\forall n \geq 2$, $\interleave f^{(n)}(0) \interleave \frac{l^{n-1} }{\vert n! \vert } \leq \varepsilon.$

By passing to logarithm and valuation, we get for all $n \geq 2$: \[ \log_p (\interleave f^{(n)}(0) \interleave )-(n-1) \log_p l-val (n!)\geq - \log_p \varepsilon.\]

By Legendre formula, it is enough that for all $n \geq 2$:
\[
\log_p (\interleave f^{(n)}(0) \interleave )-(n-1) (\log_p l-(n-1)) \geq - \log_p \varepsilon.
\]

If we denote by $NP_F$ the Newton polygon application of the power series 
\[
F(t) = \sum_{k\geq 0}^{+\infty} \log_p (\interleave f^{(k+1)}(0) \interleave ) t^k
\]
then it is enough that:
\[
\sup_{ x \in \mathbb{R}_+} \left( NP_F(x) -x  \left( \log_p l - \frac{1}{p-1} \right) \right) \geq - \log_p \varepsilon.
\]

If $NP_F^*$ is the Legendre-Fanchel transform of $NP_F$, then it is enough that:
\[ NP_F^* \left( \log_p l - \frac{1}{p-1} \right) \leq \log_p \varepsilon.\]

Therefore, by definition of the Legendre-Fanchel transform, it is enough to:
\begin{itemize}
\item take $M=(0,\log_p \varepsilon)$,
\item draw the line passing by $M$ tangent to the graph of $NP_F$,
\item if we denote by $\mu$ the slope of this line, then take $l$ such that $\mu + \frac{1}{p-1}=\log_p l$.
\end{itemize}
\end{proof}

\subsection{Automatic sharp track of precision}

Lemma \ref{lem:main} and Proposition \ref{prop:locanalytic} have 
important applications to effective computations with $p$-adic numbers 
or power series, that we are going to discuss now. Let us recall first 
that a datum involving elements of a complete ultrametric field $K$ 
(\emph{e.g.} $p$-adic numbers or power series) cannot be stored entirely 
on a computed because they basically convey an infinite amount of 
informations. That is why, instead of this datum itself, we generally 
work with neighborhoods of it. In what follows, we shall consider 
neighborhoods of the form $x + H$ where $x$ is some element lying in a 
$K$-Banach $E$ and $H$ is a lattice in $E$. A very basic example is the 
approximate $p$-adic number $1 + O(p^N)$ which corresponds to the 
neighborhood of $1$ in $\Z_p$, which is the set of all $p$-adic numbers 
congruent to $1$ modulo $p^N$. We nevertheless emphasize that our 
definition is quite flexible and allows many neighborhoods that we do 
not consider in general. For instance, an ``admissible'' neighborhood 
around a monic polynomial $P_0 \in \Q_p[X]$ of degree $d$ may be defined 
by congruences of the form $P(a_i) \equiv P_0(a_i) \pmod {p^{n_i}}$ ($1 
\leq i \leq d$) for some fixed elements $a_i \in \Q_p$ and relative 
integers $n_i$.

In the context of computer science, we will often use the notation $x + 
O(H)$ to refer to $x + H$.

\medskip

Now, let us consider a function {\tt f} (in the sense of computer 
science) that takes as input an (approximate) element lying in an open 
subset $U$ of a $K$-Banach space $E$ and outputs another (approximate) 
element lying in an open subset $V$ of another $K$-Banach space $F$. Of 
course, this function can be modelised by a \emph{continuous} 
mathematical function $f : U \to V$. The relation is the following: when 
{\tt f} is called on the input $x + O(H)$, it outputs $x' + O(H')$ with 
$f(x+H) \subset x' + H'$. We say that {\tt f} does not generate 
artificial loss of precision if the above inclusion is an equality; it is 
often not the case as shown in \S \ref{ssec:stepbystep}.

Let us assume now that $f$ is locally analytic on $U$ and that $x$ and 
$H$ are chosen so that Lemma \ref{lem:main} applies. Then $f(x+H) = f(x) 
+ f'(x)(H)$ and the smallest possible $H'$ is $f'(x)(H)$. In other 
words, the function {\tt f} generates artificial loss of precision if 
the precision $H'$ it outputs is strictly larger that $f'(x)(H)$.
The aim of this section is to explain how, under the above hypothesis, 
one can implement the function {\tt f} so that it always outputs the
optimal precision $O(H')$ with $H' = f'(x)(H)$.

\subsubsection*{Forward computation}

The execution of the function {\tt f} yields a factorization:
$$f = f_n \circ f_{n-1} \circ \cdots \circ f_1$$
where the $f_i$'s correspond to each individual basic step (like 
addition, multiplication or affectation of variables); it is then a
``nice'' (in particular locally analytic) function. For all 
$i$, let $U_i$ denote the codomain of $f_i$. Concretely, $U_i$ is the 
set of all possible values of all variables which are defined in the 
program after the execution of $i$-th step. Mathematically, it is an 
open subset in some $K$-Banach space $E_i$. We have $U_n = V$ and the 
domain of $f_i$ is $U_{i-1}$ where, by convention, we have set $U_0 = 
U$.
For all $i$, we set $g_i = f_i \circ \cdots \circ f_1$ and $x_i = 
g_i(x)$. 

When we execute the function {\tt f} on the input $x + O(H)$, we apply 
first $f_1$ to this input obtaining this way a first result $x_1 + 
O(H_1)$ and then go on with $f_2, \ldots, f_n$. At each step, we obtain 
a new intermediate result that we denote by $x_i + O(H_i)$. A way to 
guarantee that no artificial loss of precision are generated is then to 
ensure $H_i = f'_i(x)(H_{i-1}) = g_i'(x)(H)$ at each step. This can be 
achieved by reimplementing all primitives (addition, multiplication, 
\emph{etc.}) and make them compute at the same time the function $f_i$
they implement together with its differential and apply the latter to
the ``current'' lattice $H_i$.

There is nevertheless an important issue with this approach: in order to 
be sure that Lemma \ref{lem:main} applies, we need \emph{a priori} to 
compute the exact values of all $x_i$'s, which is of course not possible! 
Assuming that $g'_i(x)$ is surjective for all $i$, we can fix it as 
follows. For each $i$, we fix a lattice $\tilde H_i \subset H$ such that 
Lemma \ref{lem:main} applies to $g_i$, $x$ and $\tilde H_i$. Under our 
assumption, such lattices always exist and can be computed dynamically 
using Proposition \ref{prop:locanalytic}. Now, the equality $g_i(x + 
\tilde H_i) = x_i + g'_i(x)(\tilde H_i)$ means that any perturbation of 
$x_i$ by an element in $g'_i(x) (\tilde H_i)$ is induced by a 
perturbation of $x$ by an element in $\tilde H_i \subset H$. Hence, we 
can freely compute $x_i$ modulo $g'_i(x) (\tilde H_i)$ without changing 
the final result. Since $g'_i(x)(\tilde H_i)$ is a lattice in $E_i$, this 
remark makes possible the computation of $x_i$.

\begin{rmk}
In some cases, it is actually possible to determine suitable lattices 
$\tilde H_i$ together with their images under $g'_i(x)$ (or, at
least, good approximations of them) before starting the computation 
by using mathematical arguments. If possible, this generally helps a
lot. We shall present in \S \ref{ssec:SOMOS-solution} an example of 
this.
\end{rmk}

\subsubsection*{Backward computation}

The previous approach works only if the $g'_i(x)$'s are all surjective. 
Unfortunately, this assumption is in general not fulfilled. Indeed, 
remember that the dimension of $E_i$ is roughly the number of used 
variables after the step $i$. It all $g_i'(x)$ were surjective, this 
would mean that the function {\tt f} never initializes a new variable!
In what follows, we propose another solution that does not assume the
surjectivity of $g'_i(x)$.

For $i \in \{1, \ldots, n\}$, define $h_i = f_n \circ \cdots \circ 
f_{i+1}$, so that we have $f = h_i \circ g_i$. On differentials, we 
have $f'(x) = h_i'(x_i) \circ g'_i(x)$. Since $f'(x)$ is surjective (by 
assumption), we deduce that all $h'_i(x_i)$ are surjective. Let $H'_i$ 
be a lattice in $E_i$ such that:
\begin{enumerate}[(a)] 
\item \label{item:Hi1}
$H'_i$ is contained in $H_i + \ker h'_i(x_i) = h'_i(x_i)^{-1}
\big(f'(x)(H)\big)$;
\item \label{item:Hi2}
Lemma \ref{lem:main} applies with the function $h_i$, the point
$x_i$ and the lattice $H'_i$.
\end{enumerate}
Applying Lemma \ref{lem:main}, we then get:
$$h_i(x_i + H'_i) = x_n + h'_i(x_i)(H'_i) \subset x_n + f'(x)(H).$$
Therefore, if after the $i$-th step of the execution of the function
{\tt f}, we modify the intermediate value $x_i$ by an element of
$H'_i$, the final result remains unchanged. In other words, it is
enough to compute $x_i$ modulo $H'_i$.

It is nevertheless not obvious to implement these ideas in practice
because when we enter in the $i$-th step of the execution of {\tt f},
we have not computed $h_i$ yet and hence are \emph{a priori} not able
to determine a lattice $H'_i$ satisfying the axioms \eqref{item:Hi1}
and \eqref{item:Hi2} above.
A possible solution to tackle this problem is to proceed in several
stages as follows: 
\begin{enumerate}[(1)]
\item \label{item:smallprec}
for $i$ going from $1$ to $n$, we compute $x_i$, $f'_i(x_{i-1})$ 
at small precision (but enough for the second step) together with an
approximation of the Newton polygon of the higher order derivatives;
\item \label{item:determineHi}
for $i$ going from $n$ to $1$, we compute $h'_i(x_i)$ and
determines a lattice $H'_i$ satisfying \eqref{item:Hi1} and 
\eqref{item:Hi2};
\item \label{item:finalcomp}
for $i$ going from $1$ to $n$, we recompute $x_i$ modulo $H'_i$
and finally outputs $x_n + O\big(f'(x)(H)\big)$.
\end{enumerate}
Using relaxed algorithms for computing with elements in $K$ (\emph{cf} 
\cite{}), we can reuse in Step~\eqref{item:finalcomp} the computations 
already performed in Step~\eqref{item:smallprec}. The ``backward'' method 
we have just presented is then probably not much more expansive than the 
``forward'' method, although it is certainly much more difficult to 
implement. 

We conclude this Section by remarking that the ``backward'' method seems 
to be particularly well suited to computations with lazy $p$-adic. Indeed 
in this setting, a target precision (\emph{i.e.} a precision on the 
output) is fixed and the software determines automatically the precision 
it needs on the input to perform the computation. To do this, it first 
builds the ``skeleton'' of the computation (\emph{i.e.} it determines the 
functions $f_i$ and eventually computes the $x_i$ at small precision when 
branching points occur and it needs to decide which branch it follows) 
and then runs over this skeleton in the reverse direction in order to 
determine (an upper bound of) the needed precision at each step. The 
similarity with our approach is noticeably strong.

\subsection{Application to SOMOS sequence}
\label{ssec:SOMOS-solution}

We illustrate the theory developed in this paper by giving a simple 
toy application. Other applications will be discussed in subsequent 
articles. More precisely, we study the SOMOS 4 sequence introduced in \S 
\ref{ssec:stepbystep}. Making a crucial use of Lemma \ref{lem:main} and 
Proposition \ref{prop:locanalytic}, we design a \emph{stable} algorithm 
for computing it.

Recall (\emph{cf} \S \ref{ssec:stepbystep}) that a SOMOS 4 sequence is a 
four-term inductive sequence defined by $u_{n+4} = \frac{u_{n+2} u_{n+4} 
+ u_{n+3}^3}{u_n}$. We recall also that SOMOS sequences exhibit the 
Laurent phenomenon: its means that the four initial terms $u_0, u_1, 
u_2, u_3$ are some indeterminates, that each $u_n$ is a Laurent 
polynomial with coefficients in $\Z$ in these indeterminates.

From now, we will always consider SOMOS sequences with values in $\Q_p$ 
(for some prime number $p$) and assume that $u_0, u_1, u_2, u_3$ are 
chosen such that all terms $u_n$'s are determined (\emph{i.e.} there is 
no division by zero). We assume also for simplicity that $u_0, u_1, u_2, 
u_3$ are all units in $\Z_p$. By Laurent phenomenon, this implies first 
that all $u_n$'s lie in $\Z_p$ and second that if $u_0, u_1, u_2, u_3$ 
are known with finite precision $O(p^N)$ then all terms $u_n$'s are 
known with the same precision. Algorithm \ref{algo:SOMOS} presented on 
page \pageref{algo:SOMOS} performs this computation.

\begin{algorithm}[t]
\SetKwInOut{Assumption}{Assumption}
\KwIn{$a,b,c,d$ --- four initial terms of a SOMOS 4 sequence $(u_n)_{n \geq 0}$}
\KwIn{$n, N$ --- two integers}
\Assumption{$a$, $b$, $c$ and $d$ lie in $\Z_p^\times$ and are known at 
precision $O(p^N)$}
\Assumption{None of $u_i$ $(0 \leq i \leq n$) is divisible by $p^N$}
\KwOut{$u_n$ at precision $O(p^N)$}
\BlankLine
$\prec$ $\leftarrow$ $N$\;
\For {$i$ from $1$ to $n-3$}{
  $\prec$ $\leftarrow$ $\prec + v_p(bd + c^2) - v_p(a)$\;
  {\bf lift} (arbitrary) 
    $b$, $c$ and $d$ to precision $O(p^{\prec})$\label{line:lift}\;
  $\prec$ $\leftarrow$ $\prec - v_p(a)$\;
  $e$ $\leftarrow$ $\frac{bd+c^2} a$; \hspace{1cm}
   \tcp{$e$ is known at precision $O(p^\prec)$}
  $a, b, c, d$ $\leftarrow$ $b + O(p^{\prec}), c + O(p^{\prec}), d + O(p^{\prec}), e$\;
}
\Return{$d + O(p^N)$}\;
\caption{\sc SOMOS$(a, b, c, d, n, N)$}\label{algo:SOMOS}
\end{algorithm}

We now prove that it is correct.
We introduce the (partial) function $f : \Q_p^4 \to \Q_p^4$ defined by 
$f(a,b,c,d) = (b,c,d,\frac{bd+c^2}a)$, so that, for all $i$, we have 
$(u_i, u_{i+1}, u_{i+2}, u_{i+3}) = f_i(u_0, u_1, u_2, u_3)$ where $f_i
= f \circ \cdots \circ f$ ($i$ times). Clearly, $f$ is  
differentiable on $\Q_p^\times \times \Q_p^3$ and its differential in the 
canonical basis is given by the matrix:
$$D(a,b,c,d) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-\frac{bd+c^2}{a^2} & \frac d a & \frac {2c} a & \frac b a
\end{pmatrix}$$
whose determinant is $\frac{bd+c^2}{a^2}$. Thus, if the $(i+4)$-th term 
of the SOMOS sequence is defined, the mapping $f_i$ is differentiable
at $(u_0, u_1, u_2, u_3)$ and its differential $\varphi_i = 
f'_i(u_0, u_1, u_2, u_3)$ is given by the matrix:
$$D_i = D(u_{i-1}, u_i, u_{i+1}, u_{i+2}) \cdots
D(u_1, u_2, u_3, u_4) \cdot D(u_0, u_1, u_2, u_3).$$
Moreover thanks to the Laurent phenomenon, we know that $D_i$ has
integral coefficients, \emph{i.e.} $\varphi_i$ stabilizes the lattice
$\Z_p^4$.
We are now going to prove by induction on $i$ that, at the end of the 
$i$-th iteration of the loop, we have $\prec = N + v_p(\det D_i)$ and
\begin{equation}
\label{eq:congrSOMOS}
(a, b, c, d) \equiv (u_i, u_{i+1}, u_{i+2}, u_{i+3}) \pmod
{p^N \varphi_i(\Z_p^4)}.
\end{equation}
The first point is easy. Indeed, from $D_i = D(u_{i-1}, u_i, u_{i+1}, 
u_{i+2}) D_{i-1}$, we deduce $\det D_i = \det D_{i-1} \cdot 
\frac{u_i}{u_{i-3}}$ and the assertion follows by taking determinants
and using the induction hypothesis. 
Let us now establish Eq.~\eqref{eq:congrSOMOS}. To avoid confusion, 
let us agree to denote $a'$, $b'$, $c'$, $d'$ and $\prec'$ the values of 
$a$, $b$, $c$, $d$ and $\prec$ respectively at the \emph{beginning} of 
the $i$-th iteration of the loop. By induction hypothesis (or by 
initialization if $i = 1$), we have:
\begin{equation}
\label{eq:congrSOMOS2}
(a', b', c', d') \equiv (u_{i-1}, u_i, u_{i+1}, u_{i+2}) \pmod
{p^N \varphi_{i-1}(\Z_p^4)}.
\end{equation}
Moreover, we know that the determinant of $\varphi_{i-1}$ has valuation
$\prec'$. Hence \eqref{eq:congrSOMOS2} remains true if $a'$, $b'$, $c'$
and $d'$ are replaced by other values which are congruent to them modulo
$p^{\prec'}$. In particular it holds if $a'$, $b'$, $c'$ and $d'$ denotes
the values of $a$, $b$, $c$ and $d$ after the execution of line
\ref{line:lift}. Now applying Lemma \ref{lem:main} and Proposition
\ref{prop:locanalytic} to $\varphi_{i-1}$ and $\varphi_i$ (at the point 
$(u_0, u_1, u_2, u_3)$), we get:
$$f\big((u_{i-1}, u_i, u_{i+1}, u_{i+2}) + p^N \varphi_{i-1}(\Z_p^4)\big)
= (u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4).$$
By the discussion above, this implies in particular that $f(a',b',c',d')$
belongs to $(u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4)$. We
conclude by remarking first that $(a,b,c,d) \equiv f(a',b',c',d') \pmod 
{p^\prec \Z_p^4}$ by construction and second that $p^\prec \Z_p^4 \subset
p^N \varphi_i(\Z_p^4)$.

Finally Eq.~\eqref{eq:congrSOMOS} applied with $i = n-3$ together with
the fact that $\varphi_i$ stabilizes $\Z_p^4$ imply that, when we exit
the loop, the value of $d$ is congruent to $u_n$ modulo $p^N$. Hence,
our algorithm returns the correct value.

\medskip

Let us conclude this subsection by remarking that Algorithm 
\ref{algo:SOMOS} performs computations at precision at most $O(p^{N+v})$ 
where $v$ is the sum of the valuations of five consequence terms among 
the first $n$ terms of the SOMOS sequence we are considering. 
Experiments show that the value of $v$ varies like $c \cdot \log n$ 
where $c$ is some constant. Assuming that we are using a FFT-like 
algorithm to compute products of integers, the complexity of Algorithm 
\ref{algo:SOMOS} is then expected to be $\tilde O(N n)$ where the 
notation $\tilde O$ means that we hide logarithmic factors.

We can compare this with the complexity of the more naive algorithm 
consisting of lifting the initial terms $u_0, u_1, u_2, u_3$ to enough 
precision and then do the computation using a naive step-by-step 
tracking of precision. In this setting, the required original precision 
is $O(p^{N+2v'})$ where $v'$ is the sum of the valuation of the $u_i$'s 
for $i$ varying between $0$ and $n$. Experiments show that $v'$ is about 
$c' \cdot n \log n$ (where $c'$ is a constant), which leads to a 
complexity in $\tilde O (N n + n^2)$. Our approach is then interesting
when $n$ is large compared to $N$: under this hypothesis, it saves
roughly a factor $n$.

%\section{Conclusions}

\appendix

\section{Generalization to manifolds}
\label{sec:manifold}

Many natural $p$-adic objects do not lie in vector spaces:
points in projective spaces or elliptic curves,
subspaces of a fixed vector space (which lie in Grassmannians),
classes of isomorphism of certain curves (which lie in various moduli spaces),
\emph{etc.}  In this section we extend the formalism 
developed in Section \ref{sec:philosophy} to a more general setting:
we consider the quite general case of 
differentiable manifolds locally modeled on ultrametric Banach spaces.
This covers all the 
aforementioned examples.

In what follows, the letter $K$ always refers to an ultrametric field 
(\emph{cf} \S \ref{ssec:ultrametric}). The absolute value over $K$ is 
denoted by $|\cdot|$.

\subsection{Differentiable $K$-manifolds}
\label{ssec:manifold}

The theory of finite dimensional $K$-manifolds is presented for example 
in \cite{Schneider}, parts 8 and 9. In this Section, we shall work with
a slightly different notion of manifolds which allows also Banach vector 
spaces of infinite dimension.
More precisely, for us, a \emph{ differentiable $K$-manifold} 
(or just \emph{$K$-manifold} for short) is the data of a topological 
space $V$ together with an open covering $V = \bigcup_{i \in I} V_i$ 
(where $I$ is some set) and, for all $i \in I$, an homeomorphism 
$\varphi_i : V_i \to U_i$ where $U_i$ is an open subset of a $K$-Banach 
$E_i$ such that for all $i,j \in I$ for which $V_i \cap V_j$ is 
nonempty, the composite map
\begin{equation}
\label{eq:psiij}
\psi_{ij} : 
\varphi_i(V_{ij}) \stackrel{\varphi_i^{-1}}{\longrightarrow} 
V_{ij} \stackrel{\varphi_j}{\longrightarrow} \varphi_j(V_{ij})
\quad \text{(with } V_{ij} = V_i \cap V_j \text{)}
\end{equation}
is  differentiable at all points in its domain. We recall that
the mappings $\varphi_i$ above are the so-called \emph{charts} and that
their collection defines an \emph{atlas} of $V$. In the sequel, we shall
assume further that the open covering $V = \bigcup_{i \in I} V_i$ is
locally finite, which means that every point $x \in V$ lies only in a
finite number of $V_i$'s. Trivial examples of $K$-manifolds are 
$K$-Banach themselves.

If $V$ is a $K$-manifold and $x$ is a point of $V$, we define the 
\emph{tangent space} $T_x V$ of $V$ at $x$ as the space $E_i$ for some 
$i$ such that $x \in V_i$. We note that if $x$ belongs to $V_i$ and 
$V_j$, the linear map $\psi'_{ij}(\varphi_i(x))$ defines an isomorphism 
between $E_i$ and $E_j$. Furthermore these isomorphisms are compatible 
in an obvious way. This implies that the definition of $T_x V$ given 
above does not depend (up to some canonical isomorphism) on the index 
$i$ such that $x \in V_i$ and then makes sense.

As usual, we can define the notion of strict differentiability (at some 
point) for a continuous mapping between two $K$-manifolds by viewing it 
through the charts. A  differentiable map $f : V \to V'$ induces 
a linear map on tangent spaces $f'(x) : T_x V \to T_{f(x)} V'$ for all 
$x$ in the domain $V$. It is called the \emph{differential} of $f$ at
$x$.

\subsection{Precision data}

\todofor{David/Xavier}{Complete this Subsection}

Going back to our problem of precision, given $V$ a $K$-manifold as 
above, we would like to be able to deal with ``approximations up to some 
precision'' of elements in $V$, \emph{i.e.} expressions of the form $x + 
O(H)$ where $x$ belongs to a dense \emph{computable} subset of $V$ and 
$H$ is a ``precision data''.
The aim of this paragraph is to give a precise meaning to the expression 
$x + O(H)$ and to prove several basic properties of it.
For now, we fix a $K$-manifold $V$ and we use freely the notations $I$, 
$V_i$, $\varphi_i$, \emph{etc.} introduced in \S \ref{ssec:manifold}.

\begin{deftn}
Let $x \in V$. 
A \emph{precision datum} at $x$ is a lattice in the tangent space 
$T_x V$.
\end{deftn}

\begin{deftn}
Let $x \in V$ and $H$ be a precision datum at $x$.
For all $i \in I$ such that $x \in V_i$, we set:
$$x + O_i(H) = \varphi_i^{-1}\big(\varphi_i(x) + \varphi_i'(x)(H)\big) 
\subset V.$$
\end{deftn}

We emphasize that if $x$ belongs to $V_i$ and $V_j$ at the same time,
the subsets $x + O_i(H)$ and $x + O_j(H)$ differ in general. It is 
however not the case if $H$ is small enough.

\begin{prop}
\label{prop:independence}
Let $x \in V$ and $H$ be a precision data at $x$.
If $H$ is ``small enough'' \todo{(precise!)}, the subset $x + 
O_i(H)$ does not depend on $i$.
\end{prop}

\begin{proof}
Let $i$ and $j$ be two indices such that $x$ belongs to $V_i$ and $V_j$. 
Set $x_i = \varphi_i(x) \in E_i$ and $H_i = \varphi'_i(x)(H)$. Going 
back to the definitions, we see that the equality $x + O_i(H) = x + 
O_j(H)$ is equivalent to:
$$\psi_{ij}(x_i + H_i) = \psi_{ij}(x_i) + \psi'_{ij}(x_i)(H_i)$$ 
where we recall that $\psi_{ij}$ is defined by Eq.~\eqref{eq:psiij}.
\todo{Complete the proof and write the correct condition on $H$!}
\end{proof}

Under the assumption of Proposition \ref{prop:independence}, we define
$x + O(H)$ as $x + O_i(H)$ for some (equivalently all) $i$ such that
$x \in V_i$.

\subsubsection*{Change of base point}

In order to restrict ourselves to elements $x$ lying in a dense 
computable subset, we need to compare $x_0 + O(H_0)$ with varying $x + 
O(H)$ when $x$ and $x_0$ are close enough.
Let us first examine the situation in a fixed given chart: we fix some 
index $i \in I$ and pick two elements $x_0$ and $x$ in $V_i$. We consider 
in addition a precision datum $H_0$ at $x_0$ and we want to produce a 
precision datum $H$ at $x$ such that $x_0 + O_i(H_0) = x + O_i(H)$. 
We remark that the tangent spaces $T_{x_0} V$ and $T_x V$ are both 
isomorphic to $E_i$ via the maps $\varphi'_i(x_0)$ and 
$\varphi'_i(x)$ respectively. A natural candidate for $H$ is then:
\begin{equation}
\label{eq:Hprime}
H = \left(\varphi'_i(x)^{-1} \circ \varphi'_i(x_0)\right) (H_0).
\end{equation}
And indeed, one can check that $x + O(H)$ is indeed equal to $x_0 + 
O(H_0)$ as soon as $x$ and $x_0$ are close enough in the following sense: 
the difference $\varphi_i(x) - \varphi_i(x_0)$ lies in the lattice $H_i = 
\varphi'_i(x_0)(H_0)$. We furthermore have a property of independence on 
$i$.

\begin{prop}
We assume that $H_0$ is ``small enough'' \todo{(precise!)} and that $x$ is close 
enough to $x_0$. Then
\begin{enumerate}[(i)]
\item the lattice $H$ defined by \eqref{eq:Hprime} does not depend 
on $i$,
\item the pairs $(x,H)$ and $(x_0, H_0)$ satisfy the assumptions of
Proposition \ref{prop:independence} and we have $x + O(H) = x_0 + O(H_0)$.
\end{enumerate}
\end{prop}

\begin{proof}
We first prove (i). For an index $i$ such that $x, x_0 \in V_i$, let us 
denote by $f_i : T_{x_0} V \to T_x V$ the composite $\varphi'_i(x)^{-1} 
\circ \varphi'_i(x_0)$. Given an extra index $j$ satisfying the same
assumption, we know by \todo{(find a reference)} that the difference
$f_i - f_j$ goes to $0$ when $x$ converges to $x_0$. Since $H_0$ is open
in $T_{x_0} V$, this implies that $(f_j - f_i)(H_0)$ contains $f_i(H_0)$ and 
$f_j(H_0)$ if $x$ and $x_0$ are close enough. Now, pick $w \in f_j(H_0)$ and 
write it $w = f_j(v)$ with $v \in H_0$. Then $w$ is equal to $f_i(v) + 
(f_j - f_i)(v)$ and then belongs to $f_i(H_0)$ because each summand does. 
Therefore $f_j(H_0) \subset f_i(H_0)$. The inverse inclusion can be proved 
in the same way.

We now prove (ii). \todo{Write the proof.}
\end{proof}

\subsection{Generalization of the main Lemma}

\todofor{David/Xavier}{Extend Lemma \ref{lem:main} to manifolds.}

\section{Differential of usual operations}
\label{ssec:differentials}

We continue to work over a fixed ultrametric field $K$, whose absolute 
value is denoted by $|\cdot|$. We recall that usual examples of such $K$ 
are the field of $p$-adic numbers $\Q_p$ and the field of Laurent series 
$k((X))$ over a base field $k$. The aim of this appendix is to compute 
the differential of many usual operations on numbers, polynomials and 
matrices. Surprisingly, we observe that all differentials we will 
consider are rather easy to compute even if the underlying operation is 
quite involved (\emph{e.g.} Gr\"obner basis).

In what follows, we use freely the ``method of physicists'' to compute 
differentials: given a function $f$ differentiable at some point $x$, we 
consider a small perturbation $dx$ of $x$ and write $f(x+dx) = y + dy$ 
by expanding LHS and neglecting terms of order $2$. The differential of 
$f$ at $x$ is then the linear mapping $dx \mapsto dy$.

\subsection{Numbers}

The most basic operations are, of course, sums, products and inverses of 
elements of $K$. Their differential are well-known and quite easy to 
compute: if $z = x + y$ (resp. $z = xy$, resp $z = \frac 1 x$), we have 
$dz = dx + dy$ (resp. $dz = x \cdot dy + dx \cdot y$, resp $dz = - 
\frac{dx}{x^2}$).

A little bit more interesting is the $n$-th power map $f$ from $K$ to 
itself. Its differential $f'(x)$ is obtained by differentiate the 
equality $y = x^n$: we obtain $dy = n x^{n-1} dx$.
Hence $f'(x)$ maps to ball $B_K(r)$ (where $r$ is some positive real 
number) to $B_K(|n|{\cdot} r)$. According to Lemma \ref{lem:main}, this 
means that the behaviour of the precision depends on the absolute value 
of $n$. By the ultrametric inegality, we always have $|n| \leq 1$ but 
this inegality might be strict. It happens for instance if the 
characteristic of $K$ is divisible by $p$, in which case $n$ vanishes in 
$K$. In that case $f'(x)$ also vanishes and Lemma \ref{lem:main} does 
not apply. 
Another situation (which is more interesting) is that of $p$-adic 
numbers: let us take $K = \Q_p$ and $n = p^k$ for some integer $k$. Then
$|n| = p^{-k}$ and Lemma \ref{lem:main} reflects the well-known fact that
raising a $p$-adic number to the $p^k$-th power increases the precision 
by $k$ extra digits.

\subsection{Univariate polynomials}

For any integer $d$, let us denote by $K_{< d}[X]$ the set of 
polynomials over $K$ of degree $< d$. It is a finite dimensional vector 
space of dimension $d+1$. The affine space $X^d + K_{< d}[X]$ is then 
the set of monic polynomials over $K$ of degree $d$. We denote it by 
$K_d[X]$.

\subsubsection*{Euclidean division}

Beyond sums and products (which can be treated as before), a basic 
operation involving polynomials is Euclidean division. Let us compute 
the differential of it. We start with two polynomials $A$ and $B$ with 
$B \neq 0$. The Euclidean division of $A$ by $B$ is written $A = BQ + R$ 
with $\deg R < \deg B$. Differentiating the above equality we find: 
$dA - dB \cdot Q = B \cdot dQ + dR$,
which implies that $dQ$ and $dR$ are respectively obtained as the 
quotient and the remainder of the Euclidean division of $dA - dB \cdot 
Q$ by $B$. This gives the differential. We note that the discussion 
above extends readily to convergent series (see also 
\cite{Caruso-Lubicz}).

\subsubsection*{Subresultants and B\'ezout coefficients}

\todo{To be written.}

\subsubsection*{Factorization}

Suppose that we are given a monic polynomial $P \in K[X]$ written as a 
product $P_0 = A_0 B_0$ where $A_0$ and $B_0$ are monic and coprime. 
Hensel's lemma implies that there exists a small neighborhood $\mathcal 
U$ of $P_0$ such that any monic polynomial $P \in \mathcal U$ factors 
uniquely as $P = A B$ with $A$ and $B$ monic and close enough to $A_0$ 
and $B_0$ respectively. Thus, we can consider the application $f : P 
\mapsto (A,B)$ defined on the subset of $\mathcal U$ consisting of monic 
polynomials. We want to differentiate $f$ at $P_0$. For this, we 
differentiate the equality $P = A B$ around $P_0$, obtaining
\begin{equation}
\label{eq:difffactor}
dP = A_0 \cdot dB + B_0 \cdot dA.
\end{equation}
where $dP$, $dA$ and $dB$ have degree less than $\deg P$, $\deg A$ and 
$\deg B$ respectively. If $A_0 U_0 + B_0 V_0 = 1$ is a Bezout relation
between $A_0$ and $B_0$, it follows from Eq.~\eqref{eq:difffactor} that
$dA$ (resp. $dB$) is the remainder in the Euclidean division of $V_0
{\cdot} dP$ by $A_0$ (resp. of $U_0 {\cdot} dP$ by $B_0$).

An important special case of the previous study occurs when $A_0$ has 
degree $1$, that is $A_0(X) = X - \alpha$ with some $\alpha \in K$. The 
application $P \mapsto A$ is then nothing but the mapping that follows 
the simple root $\alpha$. Of course, its differential around $P_0$ can 
be computed by the above method. Nevertheless there is a more direct way 
to obtain it in this particular case: we write $(P + dP)(\alpha + 
d\alpha) = 0$ and expand this. We obtain this way $P'(\alpha) d\alpha + 
dP(\alpha) = 0$ where $P'$ denotes the derivative of $P$. Since $\alpha$ 
is a simple root, $P'(\alpha)$ does not vanish and we recover $d \alpha 
= - \frac{dP(\alpha)}{P'(\alpha)}$.

\subsection{Multivariate polynomials}

We consider the ring $K[\XX] = K[X_1, \ldots, X_n]$ of polynomials in $n$
variables over $K$ and fix a monomial order on it. 

\subsubsection*{Division}

We then have a notion of division in $K[\XX]$: if $f, f_1, \ldots, f_s$ 
are polynomials in $K[\XX]$, there exists a writing:
$$f = q_1 f_1 + \cdots + q_s f_s + r$$
where $q_1, \ldots, q_s, r \in K[\XX]$ and no term of $r$ is divisible
by the leading term of some $f_i$ ($1 \leq i \leq s$). Moreover, assuming
that $(f_1, \ldots, f_s)$ is a Gr\"obner basis\footnote{Without this
assumption, there still exists a canonical choice of $r$. We do not know
if the computation of the differential that follows extends to a more
general setting.} (of the ideal generated by these polynomials), the 
polynomial $r$ is uniquely determined and called the \emph{remainder} of 
the division of $f$ by the family $(f_1, \ldots, f_s)$. The application 
$(f, f_1, \ldots, f_s) \mapsto r$ is then well defined and similarly to 
the case of univariate polynomials, we can compute its differential: we 
find that $dr$ is obtained as the remainder of the division of $f - (q_1 
\cdot d f_1 + \cdots + q_s \cdot d f_s)$ by $(f_1, \ldots, f_s)$.

\subsubsection*{Gr\"obner basis}

Recall that any 
ideal $I \subset K[\XX]$ admits a unique reduced Gr\"obner basis. We can 
then consider the application mapping a family $(f_1, \ldots, f_s)$ of 
\emph{homogeneous}\footnote{This restriction will be convenient for us 
mainly because we are using the article \cite{Vaccon}, which deals only 
with homogeneous polynomials. It is however probably not essential.} 
polynomials of fixed degrees to the reduced Gr\"obner basis $(g_1, 
\ldots, g_t)$. It follows from Theorem 1.1 of \cite{Vaccon} that this 
application at $(f_1, \ldots, f_s)$ is continuous provided that:
\begin{itemize}
\item the sequence $(f_1, \ldots, f_s)$ is regular
\item for all $i \in \{1, \ldots, s\}$, the ideal generated by
$f_1, \ldots, f_i$ is wearly-$w$ where $w$ denotes the fixed monomial
order (\emph{cf} \cite{Vaccon} for more precisions).
\end{itemize}
A similar argument proves that it is actually differentiable at such
points. We are now going to compute the differential. For this we
remark that since the families $(f_1, \ldots, f_s)$ and $(g_1, \ldots,
g_t)$ generate the same ideal, we have a relation:
\begin{equation}
\label{eq:prodgrobner}
(g_1, \ldots, g_t) = (f_1, \ldots, f_s) \cdot A
\end{equation}
where $A$ is a $(s \times t)$ matrix with coefficients in $K[\XX]$.
Moreover following Vaccon's construction, we see that the entries of $A$ 
can be chosen in such a way that they define differentiable functions.
Differentiating Eq.~\eqref{eq:prodgrobner}, we get:
$$(d g_1, \ldots, d g_t) =
(d f_1, \ldots, d f_s) \cdot A + (f_1, \ldots, f_s) \cdot dA.$$
Finally, using that we are working with \emph{reduced} Gr\"obner
basis, the above equality implies that $d g_i$ is the remainder in
the division of the $i$-th coefficient of the product
$(d f_1, \ldots, d f_s) \cdot A$ by the family $(g_1, \ldots, g_t)$.

\subsection{Matrices}

Differentiating ring operations (sum, products, inverse) over matrices 
is again straightforward; we just need to be careful that matrix 
algebras are (in general) noncommutative.

\subsubsection*{Determinants and characteristic polynomials}

Let us first compute the differential of the function $\det : M_n(K) \to 
K$. It is a standard computation. To begin with, we consider an 
invertible matrix $M$ and write:
\begin{eqnarray*}
\det(M + dM) &=& \det(M) \cdot \det(I + M^{-1} \cdot dM)  \\
&=& \det(M) \cdot \big(1 + \tr(M^{-1} \cdot dM)\big) \\
&=& \det(M) + \tr(\com(M) \cdot dM)
\end{eqnarray*}
where $\com(M)$ denotes the comatrix of $M$. The differential of 
$\det$ at $M$ is then $dM \mapsto \tr(\com(M) \cdot dM)$. It turns
out that this formula is still valid when $M$ is not invertible.
The same computation extends readily to characteristic polynomials,
since they are defined as determinants. More precisely, let us 
consider the function $\chi : M_n(K) \to K_{\leq n}[X]$ (where
the latter ring denotes the finite dimensional vector space of 
polynomials over $K$ of degree at most $n$) taking a matrix $M$
to its characteristic polynomial $\det(X-M)$.
Then $\chi$ is differentiable at each point $M \in M_n(K)$ and its 
differential is given by $dM \mapsto \tr(\com(X{-}M) \cdot dM)$.

\subsubsection*{LU factorization}

Let us agree to define a 
LU factorization of a square matrix $M \in M_n(K)$ as a writing $M = LU$ 
where $L$ is lower triangular and unipotent and $U$ is upper triangular. 
Such a writing exists and is unique provided that all principal minors 
of $M$ do not vanish. We can then consider the mapping $M \mapsto (L,U)$ 
defined over the Zariski-open set of matrices satisfying the above 
condition. In order to differentiate it, we differentiate the relation 
$M = LU$ and rewrite the result as follows:
$$L^{-1} dM \: U^{-1} = L^{-1} \cdot dL + dU \cdot U^{-1}.$$
We remark that in the right hand side of the above formula, the first
summand is lower triangular and all its diagonal entries vanish whereas 
the second summand is upper triangular. Hence in order to compute $dL$
and $dU$, one can proceed as follows: (1)~we compute the product $dX = 
L^{-1} dM \: U^{-1}$, (2)~we separate the lower and upper part of $dX$
obtaining this way $L^{-1} \cdot dL$ and $dU \cdot U^{-1}$ and (3)~we
recover $dL$ and $dU$ by multiplying the above matrices by $L$ on the 
left and $U$ on the right respectively. 
The above discussion extends almost \emph{verbatim} to LUP 
factorization; the only difference is that LUP factorizations are not 
unique but they are on a small neighborhood of $M$ if we fix the matrix 
$P$.

\subsubsection*{QR factorization}

For us, a QR factorization of a square matrix $M \in M_n(K)$ will be
a writing $M = QR$ where $R$ is unipotent upper triangular and $Q$ is
orthogonal in the sense that $\trans Q \cdot Q$ is diagonal. As before, such 
a writing exists and is unique on a Zariski-open subset of $M_n(K)$. The
mapping $f : M \mapsto (Q,R)$ is then well defined on this subset. We 
would like to emphasize at this point that the orthogonality condition
defines a sub-manifold of $M_n(K)$ which is a apparently \emph{not} a
vector space (it is defined by equations of degree $2$). The codomain
of $f$ is then also a manifold; this example then fits to the setting
of \S \ref{sec:manifold} but not to those of \S \ref{sec:philosophy}. 
The generalization made in \S \ref{sec:manifold} is then needed.
Anyway, we can differentiate $f$ by following the method we used for LU 
factorization: differentiating the relation $M = QR$, we obtain:
\begin{equation}
\label{eq:diffQR}
\trans Q \cdot dM \cdot R^{-1} = \trans Q \cdot dQ + \Delta \cdot dR 
\cdot R^{-1}
\end{equation}
where $\Delta = \trans Q \cdot Q$ is a diagonal matrix by definition.
Moreover by differentiating $\trans Q \cdot Q = \Delta$, we find that
$\trans Q \cdot dQ$ can be written as the sum of an antisymmetric 
matrix and a diagonal one. Since moreove $dR \cdot R^{-1}$ is upper
triangular with all diagonal entries equal to $0$, we see that 
Eq.~\eqref{eq:diffQR} is enough to compute $dQ$ and $dR$ from $Q$, $R$ 
and $dM$.

\subsection{Vector spaces}

Let $d$ and $n$ be two nonnegative integers such that $d \leq n$. The 
Grassmannian $\Grass(d,n)$ is the set of all sub-vector spaces of $K^n$ 
of dimension $d$. It defines an algebraic variety over $K$ and hence
\emph{a fortiori} a $K$-manifold in the sense of \S \ref{ssec:manifold}.
Concretely, a vector space $V \subset K^n$ of dimension $d$ is given by 
a rectangular matrix $M \in M_{d,n}(K)$ whose rows form a basis of $V$ 
and two such matrices $M$ and $M'$ define the same vector space if there 
exists $P \in \GL_d(K)$ such that $M = P M'$. Performing row echelon, we 
find that we can always choose the above matrix $M$ in the particular
shape:
$$M = \begin{pmatrix} I_d & M' \end{pmatrix} \cdot P$$
where $I_d$ denotes the $(d \times d)$ identity matrix, $M' \in M_{d, 
n-d}(K)$ and $P$ is a permutation matrix of size $n$. Moreover two 
such writings with the same $P$ necessarily coincide. Hence each 
permutation matrix $P$ defines a chart $U_P \subset \Grass(d,n)$ which
is canonically diffeomorphism to $M_{d, n-d}(K) \simeq K^{d(n-d)}$.

\subsubsection*{Left kernels}

We consider the open subset $\mathcal V_{n-d}$ of $M_{n,n-d}(K)$ 
consisting of matrices of full rank, \emph{i.e.} rank $n-d$. The left 
kernel defines a mapping $\text{LK} : M_{n,n-d}(K) \to \Grass(d,n)$. Let 
us prove that it is differentiable and compute its differential around 
some point $M \in M_{n,n-d}(K)$. Of course, there exists a neighborhood 
$\mathcal U$ of $M$ whose image is entirely contained in a given chart 
$U_P$. We assume for simplicity that $P$ is the identity. On $\mathcal 
U$, the map $\text{LK}$ corresponds in our chart to the application that 
takes $M$ to the unique matrix $N \in M_{d, n-d}(K)$ such that 
$\begin{pmatrix} I_d & N \end{pmatrix} \cdot M = 0$. The implicit 
function theorem then implies that $\text{LK}$ is differentiable. 
Furthermore, its differential satisfies the relation
$\begin{pmatrix} 0 & dN \end{pmatrix} \cdot M +
\begin{pmatrix} I_d & N \end{pmatrix} \cdot dM = 0$,
from which we can compute $dN$ by projecting on the $(n-d)$ last
columns.

We can develop further this example and study what happens on the 
closed subset of $M_{n,n-d}(K)$ where matrices have not full rank. On
this subspace, the left kernel has dimension $< d$ and then no longer
defines a point in $\Grass(n,d)$. Nevertheless, for all integer $r <
n-d$, we can consider the subset $\mathcal V_r \subset M_{n,n-d}(K)$
of matrices whose rank are exactly rank $r$. It is locally closed in
$M_{n,n-d}(K)$ with respect to the Zariski topology and hence defines
a $K$-manifold. Furthermore, we have a mapping $\text{LK}_r : \mathcal 
M_r \to \Grass(n,n-r)$ which is differentiable and whose differential 
can be computed as before.

The lesson from this study is the following. If we have to compute the 
kernel of a matrix $M$ over $K$ known with finite precision, \emph{a 
priori} we first need to determine what is the rank of $M$. However, the 
rank function is not continuous --- it is only lower semi-continuous --- 
and hence cannot be determined from sure from $M$ (expect if $M$ has 
full rank). We then need to make an extra hypothesis and the more natural 
one is arguably to assume that $M$ has actually the minimal rank that it 
can have. With this convention, for instance, the kernel of the zero 
matrix $M \in M_n(K)$ \emph{known with finite precision} is always the 
whole space although there exist invertible matrices in any 
neighborhood of $M$.

\subsubsection*{Intersections}

We pick $n$, $d_1$ and 
$d_2$ three nonnegative integers such that $d_1 \leq n$, $d_2 \leq n$ 
and $d_1 + d_2 \geq n$. Two subspaces of $K^n$ of dimension $d_1$ and 
$d_2$ respectively meet along a subspace of dimension at most $d_1 + d_2 
- n$. For all $d \leq d_1 + d_2 -n$, we can then define the subspace 
$\mathcal V_d$ of $\Grass(n,d_1) \times \Grass(n,d_2)$ consisting of 
pairs $(E_1, E_2)$ such that $\dim (E_1 \cap E_2) = d$ and consider the 
function $f_d : \mathcal V_d \to \Grass(n, d)$ which sends $(E_1, E_2)$ 
to $E_1 \cap E_2$. In charts, the function $f_d$ can be interpreted as
the kernel of a matrix simply because $E_1 \cap E_2$ appears as the
kernel of the canonical linear map $E_1 \oplus E_2 \to K^n$. We deduce
for this that $f_d$ is differentiable and that its differential can be
computed similarly to what we have done before.

\subsubsection*{Sums and images}

Finally, we note that similar results hold for images of matrices and,
consequently, sums of subspaces.

<<<<<<< HEAD
\subsection{The SOMOS 4 sequence}
\label{ssec:SOMOS-solution}

Recall (\emph{cf} \S \ref{ssec:stepbystep}) that a SOMOS 4 sequence is a 
four-term inductive sequence defined by $u_{n+4} = \frac{u_{n+2} u_{n+4} 
+ u_{n+3}^3}{u_n}$. We recall also that SOMOS sequences exhibit the 
Laurent phenomenon: its means that the four initial terms $u_0, u_1, 
u_2, u_3$ are some indeterminates, that each $u_n$ is a Laurent 
polynomial with coefficients in $\Z$ in these indeterminates (see \cite{Fomin-Zelevinsky}).

From now, we will always consider SOMOS sequences with values in $\Q_p$ 
(for some prime number $p$) and assume that $u_0, u_1, u_2, u_3$ are 
chosen such that all terms $u_n$'s are determined (\emph{i.e.} there is 
no division by zero). We assume also for simplicity that $u_0, u_1, u_2, 
u_3$ are all units in $\Z_p$. By Laurent phenomenon, this implies first 
that all $u_n$'s lie in $\Z_p$ and second that if $u_0, u_1, u_2, u_3$ 
are known with finite precision $O(p^N)$ then all terms $u_n$'s are 
known with the same precision. 

In this last paragraph, we illustrate the theory developed in this 
article by designing an algorithm for computing the $u_n$'s at precision 
$O(p^N)$. The algorithm is presented at the top of page 
\pageref{algo:SOMOS}. We now prove that it is correct.
%
\begin{algorithm}[t]
\SetKwInOut{Assumption}{Assumption}
\KwIn{$a,b,c,d$ --- four initial terms of a SOMOS 4 sequence $(u_n)_{n \geq 0}$}
\KwIn{$n, N$ --- two integers}
\Assumption{$a$, $b$, $c$ and $d$ lie in $\Z_p^\times$ and are known at 
precision $O(p^N)$}
\Assumption{None of $u_i$ $(0 \leq i \leq n$) is divisible by $p^N$}
\KwOut{$u_n$ at precision $O(p^N)$}
\BlankLine
$\prec$ $\leftarrow$ $N$\;
\For {$i$ from $1$ to $n-3$}{
  $\prec$ $\leftarrow$ $\prec + v_p(bd + c^2) - v_p(a)$\;
  {\bf lift} (arbitrary) 
    $b$, $c$ and $d$ to precision $O(p^{\prec})$\label{line:lift}\;
  $\prec$ $\leftarrow$ $\prec - v_p(a)$\;
  $e$ $\leftarrow$ $\frac{bd+c^2} a$; \hspace{1cm}
   \tcp{$e$ is known at precision $O(p^\prec)$}
  $a, b, c, d$ $\leftarrow$ $b + O(p^{\prec}), c + O(p^{\prec}), d + O(p^{\prec}), e$\;
}
\Return{$d + O(p^N)$}\;
\caption{\sc SOMOS$(a, b, c, d, n, N)$}\label{algo:SOMOS}
\end{algorithm}
%
We introduce the (partial) function $f : \Q_p^4 \to \Q_p^4$ defined by 
$f(a,b,c,d) = (b,c,d,\frac{bd+c^2}a)$, so that, for all $i$, we have 
$(u_i, u_{i+1}, u_{i+2}, u_{i+3}) = f_i(u_0, u_1, u_2, u_3)$ where $f_i
= f \circ \cdots \circ f$ ($i$ times). Clearly, $f$ is strictly 
differentiable on $\Q_p^\times \times \Q_p^3$ and its differential in the 
canonical basis is given by the matrix:
$$D(a,b,c,d) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-\frac{bd+c^2}{a^2} & \frac d a & \frac {2c} a & \frac b a
\end{pmatrix}$$
whose determinant is $\frac{bd+c^2}{a^2}$. Thus, if the $(i+4)$-th term 
of the SOMOS sequence is defined, the mapping $f_i$ is differentiable
at $(u_0, u_1, u_2, u_3)$ and its differential $\varphi_i = 
f'_i(u_0, u_1, u_2, u_3)$ is given by the matrix:
$$D_i = D(u_{i-1}, u_i, u_{i+1}, u_{i+2}) \cdots
D(u_1, u_2, u_3, u_4) \cdot D(u_0, u_1, u_2, u_3).$$
Moreover thanks to the Laurent phenomenon, we know that $D_i$ has
integral coefficients, \emph{i.e.} $\varphi_i$ stabilizes the lattice
$\Z_p^4$.
We are now going to prove by induction on $i$ that, at the end of the 
$i$-th iteration of the loop, we have $\prec = N + v_p(\det D_i)$ and
\begin{equation}
\label{eq:congrSOMOS}
(a, b, c, d) \equiv (u_i, u_{i+1}, u_{i+2}, u_{i+3}) \pmod
{p^N \varphi_i(\Z_p^4)}.
\end{equation}
The first point is easy. Indeed, from $D_i = D(u_{i-1}, u_i, u_{i+1}, 
u_{i+2}) D_{i-1}$, we deduce $\det D_i = \det D_{i-1} \cdot 
\frac{u_i}{u_{i-3}}$ and the assertion follows by taking determinants
and using the induction hypothesis. 
Let us now establish Eq.~\eqref{eq:congrSOMOS}. To avoid confusion, 
let us agree to denote $a'$, $b'$, $c'$, $d'$ and $\prec'$ the values of 
$a$, $b$, $c$, $d$ and $\prec$ respectively at the \emph{beginning} of 
the $i$-th iteration of the loop. By induction hypothesis (or by 
initialization if $i = 1$), we have:
\begin{equation}
\label{eq:congrSOMOS2}
(a', b', c', d') \equiv (u_{i-1}, u_i, u_{i+1}, u_{i+2}) \pmod
{p^N \varphi_{i-1}(\Z_p^4)}.
\end{equation}
Moreover, we know that the determinant of $\varphi_{i-1}$ has valuation
$\prec'$. Hence \eqref{eq:congrSOMOS2} remains true if $a'$, $b'$, $c'$
and $d'$ are replaced by other values which are congruent to them modulo
$p^{\prec'}$. In particular it holds if $a'$, $b'$, $c'$ and $d'$ denotes
the values of $a$, $b$, $c$ and $d$ after the execution of line
\ref{line:lift}. Now applying Lemma \ref{lem:main} and Proposition
\ref{prop:} to $\varphi_{i-1}$ and $\varphi_i$ (at the point $(u_0,
u_1, u_2, u_3)$), we get:
$$f\big((u_{i-1}, u_i, u_{i+1}, u_{i+2}) + p^N \varphi_{i-1}(\Z_p^4)\big)
= (u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4).$$
By the discussion above, this implies in particular that $f(a',b',c',d')$
belongs to $(u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4)$. We
conclude by remarking first that $(a,b,c,d) \equiv f(a',b',c',d') \pmod 
{p^\prec \Z_p^4}$ by construction and second that $p^\prec \Z_p^4 \subset
p^N \varphi_i(\Z_p^4)$.

Finally Eq.~\eqref{eq:congrSOMOS} applied with $i = n-3$ together with
the fact that $\varphi_i$ stabilizes $\Z_p^4$ imply that, when we exit
the loop, the value of $d$ is congruent to $u_n$ modulo $p^N$. Hence,
our algorithm returns the correct value.

\medskip

Let us conclude this subsection by remarking that Algorithm 
\ref{algo:SOMOS} performs computations at precision at most $O(p^{N+v})$ 
where $v$ is the sum of the valuations of five consequence terms among 
the first $n$ terms of the SOMOS sequence we are considering. 
Experiments show that the value of $v$ varies like $c \cdot \log n$ 
where $c$ is some constant. Assuming that we are using a FFT-like 
algorithm to compute products of integers, the complexity of Algorithm 
\ref{algo:SOMOS} is then expected to be $\tilde O(N n)$ where the 
notation $\tilde O$ means that we hide logarithmic factors.

We can compare this with the complexity of the more naive algorithm 
consisting of lifting the initial terms $u_0, u_1, u_2, u_3$ to enough 
precision and then do the computation using a naive step-by-step 
tracking of precision. In this setting, the required original precision 
is $O(p^{N+2v'})$ where $v'$ is the sum of the valuation of the $u_i$'s 
for $i$ varying between $0$ and $n$. Experiments show that $v'$ is about 
$c' \cdot n \log n$ (where $c'$ is a constant), which leads to a 
complexity in $\tilde O (N n + n^2)$. Our approach is then interesting
when $n$ is large compared to $N$: under this hypothesis, it saves
roughly a factor $n$.

%\section{Conclusions}

=======
>>>>>>> 11f6a7ebc868161119e0f9c428ecd733f3996e20
\begin{thebibliography}{10}

\bibitem{Bostan}
  A.~Bostan, L.~Gonz\'alez-Vega, H.~Perdry, \'E.~Schost,
  \emph{From Newton sums to coefficients: complexity issues in characteristic $p$},
  Proceedings MEGA'05 (2005).

\bibitem{Caruso}
  X.~Caruso,
  \emph{Random matrices over a DVR and LU factorization},
  preprint (2012)
  
\bibitem{Caruso-Lubicz}
  X.~Caruso, D.~Lubicz
  \emph{Linear Algebra over Zp[[u]] and related rings,} 
  to appear at LMS J. Comp. and Math. 

\bibitem{Fomin-Zelevinsky}
  S.~Fomin, A.~Zelevinsky
  \emph{The Laurent phenomenon,}   Advances in Applied Mathematics 28: 119–144 (2002).
  
\bibitem{Gaudry}
  P.~Gaudry, T.~Houtmann, A.~Weng, C.~Ritzenthaler, D.~Kohel,
  \emph{The $2$-adic CM method for genus $2$}, 
  Asiacrypt 2006, vol. 4284 of Lecture Notes in Comput. Sci. (2006), 114--129

\bibitem{Kedlaya}
  K.~Kedlaya,
  \emph{Counting points on hyperelliptic curves using Monsky--Washnitzer cohomology}, 
  J. Ramanujan Math. Soc. {\bf 16} (2001)

\bibitem{Lercier}
  R.~Lercier, T.~Sirvent,
  \emph{On Elkies subgroups of $\ell$-torsion points in elliptic curves defined over a finite field},
  J. Th\'eorie des Nombres de Bordeaux {\bf 20} (2008), 783--797

\bibitem{Schneider}
  P.~Schneider,
  \emph{$p$-adic Lie Groups}, Springer (2011)

\bibitem{Vaccon}
  T.~Vaccon,
  \emph{Matrix-F5 algorithms over finite-precision complete discrete 
  valuation fields},
  preprint (2014)
\end{thebibliography}

\end{document}
