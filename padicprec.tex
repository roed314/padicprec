%\documentclass{sig-alternate}
\documentclass{lms}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,french,linesnumbered]{algorithm2e}

\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
%\theoremstyle{definition}
\newtheorem{deftn}[theo]{Definition}
\newtheorem{rmk}[theo]{Remark}

\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\R}{\mathbb R}
\renewcommand{\O}{\mathcal O}
\newcommand{\XX}{\mathbf X}
\newcommand{\trans}{{}^{\text t}}

\newcommand{\val}{\text{\rm val}}
\newcommand{\tr}{\text{\rm Tr}}
\newcommand{\com}{\text{\rm Com}}
\renewcommand{\prec}{\text{\rm prec}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\title{$p$-adic precision}
\author{Xavier Caruso}
\author{David Roe}
\author{Tristan Vaccon}
\date\today

\maketitle
\begin{abstract}
We present a new method to handle $p$-adic precision (or more generally
precision over ultrametric fields) based on $p$-adic calculus.
We illustrate it by many examples and give a toy application to the
stable computation of the SOMOS 4 sequence.
\end{abstract}

\setcounter{tocdepth}{1}
\tableofcontents

\section{Introduction}

During the last two decades, $p$-adic methods have become a tremendous tool in computer algebra, since it allows computation not feasible on finite fields, can control the growth of the size of the coefficients when dealing with rationnal numbers, or is needed when performing algorithm of $p$-adic nature.

Yet, $p$-adic algorithms are necessarily numeric or symbolic-numeric: one can not avoid working with finite precision, and thus, keeping track of loss in precision during the computation.

Here are some examples of algorithm that rely on $p$-adic computations:
\begin{itemize}
\item Bostan \cite{Bostan}, on how polynomials in $\mathbb{F}_p$ can be computed from theirs Newton series lifted in $\mathbb{Z}_p$;
\item Gaudry \cite{Gaudry}, on generation of genus $2$ curves via $p$-adic lifting;
\item Kedlaya \cite{Kedlaya}, on point counting on hyperelliptic curves using Monsky--Washnitzer cohomology;
\item Lercier \cite{Lercier}, on computing isogenies between elliptic curves using $p$-adic differential equations.
\end{itemize}

Many usual softwares (\emph{e.g.} {\tt magma} and {\tt sage}) include 
primitives to handle $p$-adic numbers. Since $p$-adic numbers are by 
nature infinite, one needs to truncate them up to some finite level (we 
generally say \emph{finite precision}) in order to manipulate them on a 
computed. The usual technics to achieve this are inspired from case of 
real numbers: the computer works with ``truncated $p$-adic numbers'' 
represented as $x + O(p^n)$ and performs all basic operations (addition, 
product) on this representation. For instance, it computes: 
$$(x + O(p^n)) + (y + O(p^m)) = (x+y) + O(p^{\min(n,m)}).$$
However, in many important cases (\emph{e.g.} those discussed just 
above), this approach is too naive since it creates artificially 
important losses of precision (see \S\ref{ssec:stepbystep}). 
It is the so-called \emph{numerical instability} issue.

The aim of the current paper is to provide a systematical theorical 
study of $p$-adic precision (and more generally nonarchimedian 
precision) and to derive from this study new methods to handle 
precision, that turns out to give nearly optimal results in a quite 
large setting.

\section{Motivations and aims}

\todofor{David}{Add a general introduction to this section.}

\subsection{Separate precision and approximation}

\todofor{David}{
Explain here why it might be useful to work separately with 
approximation on the one hand and precision on the other hand.
}

\subsection{Step-by-step tracking is not sharp}
\label{ssec:stepbystep}

In this subsection, we illustrate by some examples that tracking
$p$-adic precision step by step may lead to important numerical
instability.

\subsubsection*{A first stupid example}

Let $f : \Qp^2 \to \Qp^2$ be the function mapping $(x,y)$ to $(x+y, 
x-y)$. We consider in addition two $p$-adic numbers $x$ and $y$, 
which are known up to precision $O(p^n)$ and $O(p^m)$ respectively.
The question we would like to address in this example is the
computation of $f \circ f(x,y)$.

Two approaches are possible. The first one consists in doing the 
computation step-by-step: we first compute $(z,t) = f(x,y)$ and we 
compute then $f(z,t)$ which gives the result. Proceeding this way, we 
see that $z$ and $t$ are both known up to precision $O(p^{\min(n,m)})$ 
are consequently the same holds for the coordinates of the result.
The second approach consists in remarking that $f \circ f$ maps
$(x,y)$ to $(2x, 2y)$. Computing $f \circ f(x,y)$ using the latter
formula, it is clear that the result we get is known up to precision
$(O(p^n), O(p^m))$ --- and even $(O(p^{n+1}), O(p^{m+1}))$ if $p = 2$.
As a conclusion, the second approach yields a more accurate result.
In other words, computing step-by-step creates ``artificial'' losses 
of precision.

\subsubsection*{The SOMOS 4 sequence}

The SOMOS 4 sequence is a four-term inductive sequence satisfying:
$$u_{n+4} = \frac{u_{n+1} u_{n+3} + u_{n+2}^2}{u_n}.$$
In this example, we shall consider the situation where the initial
terms $u_0$, $u_1$, $u_2$ and $u_3$ are \emph{invertible} $p$-adic 
integers, \emph{i.e.} lie in $\Zp^\times$.

Let us first examine what happens (regarding to the precision) when
we are computing the sequence $(u_n)$ naively. The computation of
$u_{n+4}$ involves a division by $u_n$ and hence, roughly speaking,
decreases the precision by a factor $p^{\val(u_n)}$ where $\val$
stands for the $p$-adic valuation. Hence, if we assume that $u_0$,
$u_1$, $u_2$ and $u_3$ are given with precision $O(p^N)$ then the
naive computation returns the value of $u_n$ with precision
\begin{equation}
\label{eq:SOMOS}
O(p^{N-v_n})
\quad \text{with}\quad
v_n = \val(u_0) + \cdots + \val(u_{n-4}).
\end{equation}

However, on the other hand, one can prove that the SOMOS 4 sequence 
exhibits the \emph{Laurent phenomenon}. This means that, for all integer 
$n$, there exists a polynomial $P_n$ in $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 
1}, T^{\pm 1}]$ such that $u_n = P_n(u_0, u_1, u_2, u_3)$.
From the latter formula, it follows directly that if $u_0$, $u_1$,
$u_2$ and $u_3$ are known up to precision $O(p^N)$ then all $u_n$'s
are also known with the same precision. Thus, the term $v_n$ that 
appears in \eqref{eq:SOMOS} does not reflect an intrinsic loss of
precision but some numerical instability related to the algorithm ---
which includes the model precision --- we used to compute to SOMOS 
sequence.

\begin{rmk}
From the above discussion, one can easily derive a numerically stable 
algorithm that computes the SOMOS 4 sequence: (1)~we compute the 
Laurent polynomials $P_n$ using the induction formula (and working in 
the ring $\Z[X^{\pm 1}, Y^{\pm 1}, Z^{\pm 1}, T^{\pm 1}]$) and (2)~we 
evaluate $P_n$ at the point $(u_0, u_1, u_2, u_3)$.
However this algorithm is not efficient at all since computing the 
$P_n$'s is very time-consuming for two reasons: first, it requires 
division in a polynomial ring with $4$ variables and, second, the
size of the coefficients of $P_n$ explodes as $n$ grows.

In \S \ref{ssec:SOMOS-solution}, we shall design an algorithm computing 
the SOMOS 4 sequence which turns out to be, at the same time, efficient 
and numerically stable.
\end{rmk}

\subsubsection*{LU factorization}

Let us first recall that a square matrix $M$ with coefficients in $\Qp$ 
admits a LU factorization if it can be written as a product $LU$ where 
$L$ and $U$ is lower triangular and upper triangular respectively. 
The computation of a LU factorization appears as an important tool to 
tackle many classical questions about matrices or linear systems.

We first note that a LU decomposition of a given matrix $M$ is unique
if you require further that the $L$-part of this decomposition is
unipotent (\emph{i.e.} all diagonal entries equal $1$). Let us write
$L(M) U(M)$ the LU factorization of $M$ satisfying the above extra
condition (when it exists). There exists Cramer-type formulas that
gives each individual entries of $L(M)$and $U(M)$ in a closed form
(see \cite{Caruso}).

If $M$ is random square matrix over $\Zp$ of side $d$ whose every
coefficient is known up to precision $O(p^N)$, the two following
results are proved in \cite{Caruso}:
\begin{itemize}
\item using usual Gauss elimination and tracking $p$-adic precision 
step-by-step, the smallest precision on an entry of $L(M)$ is about
$O(p^{N - \frac{2d}{p-1}})$ on average;
\item computing $L(M)$ by evaluating Cramer-type formulas yields a
result whose every entry is known up to precision $O(p^{N - 2 \log_p 
d})$.
\end{itemize}
If $d$ is large compared to $p$, the second precision is much more
accurate than the first one. This shows once again that ``artifical''
losses of precision appear when one performs Gauss elimination by
tracking precision step-by-step.

%\subsubsection*{Algorithms \emph{\`a la} Kedlaya}

\section{A new philosophy}
\label{sec:philosophy}

The aim of this section is to develop a new point of view on $p$-adic 
precision based on $p$-adic analysis. Actually, we shall generalize a 
bit the setting and work over a general ultrametric field (see definition
below).

\subsection{Ultrametric analysis}
\label{ssec:ultrametric}

In this section, we recall usual definitions and properties related to
ultrametric analysis. We refer to Schneider \cite{Schneider} for a 
complete exposition.

An \emph{absolute value} on a field $K$ is a multiplicative map $|\cdot| 
: K \to \R_+$ satisfying the triangle inequality and sending every 
nonzero element of $K$ to a strictly positive real number. It is said 
\emph{ultrametric} if it satisfies the following strong from of the 
triangle inequality:
$$|x+y| \leq \max(|x|, |y|).$$
Given an absolute value $|\cdot|$ on $K$, we note that $|K^\times|$ is a 
multiplicative subgroup of $\R_+^\times$. It is then either discrete or 
dense. We shall say that $|\cdot|$ is \emph{discrete} if $|K^\times|$ is
discrete. The valued field $(K, |\cdot|)$ is said \emph{complete} if all 
Cauchy sequences (with respect to the distance $d$ defined by $d(x,y) = 
|x-y|$) converge.

The formula $|x| = p^{-\val(x)}$ defines an ultrametric absolute value on 
the field $\Qp$ for which it is complete. More generally, the $p$-adic 
absolute value extends uniquely to any finite extension $K$ of $\Qp$ and 
turns $K$ into a complete ultrametric field. Another classical example of 
complete ultrametric field is the field of Laurent series over a finite
field, the absolute value being defined from the valuation of a series.

From now, we fix a field $K$ equipped with an ultrametric absolute value 
denoted by $|\cdot|$ and we assume that $K$ is complete. Given $v$ a 
$K$-vector space (of possibly infinite dimension), an \emph{ultrametric 
norm} on $V$ is a map $\Vert\cdot\Vert : V \to \R^+$ satisfying:
\begin{enumerate}[(i)]
\item $\Vert x\Vert = 0$ if and only if $x = 0$;
\item $\Vert \lambda x\Vert = |\lambda| \cdot \Vert x\Vert$;
\item $\Vert x+y\Vert \leq \max(\Vert x\Vert, \Vert y\Vert)$.
\end{enumerate}
A norm gives rise to a distance and a normed $K$-vector space $V$ is 
said \emph{complete} when any Cauchy sequence with values in $V$ 
converges. A normed $K$-vector space which is complete is called a 
\emph{$K$-Banach}.

Similarly to the real case, any finite dimensional normed $K$-vector 
space is complete and all norms over such a space are equivalent 
(\emph{i.e.} they define the same topology). Many usual Theorems 
concerning Banach spaces (\emph{e.g.} open mapping Theorem, closed graph 
Theorem) remain true in the ultrametric setting.

\begin{deftn}
Let $V, W$ be two normed $K$-vector spaces, let $U \subset V$ be an open 
subset of $V$ and let $f : U \rightarrow W$ be a map. Then $f$ is called 
\emph{differentiable} at $v_0 \in U$ if there exists a 
continuous linear map $f'(v_0) : U \rightarrow W$ such that for any 
$\varepsilon >0$, there exists an open neighborhood $U_\varepsilon 
\subset U$ containing $v_0$ with:
\[ 
\forall v, w \in U_\varepsilon, \quad
\Vert f(v)-f(w)-f'(v_0) \cdot \left( v-w \right) \Vert 
\leq \varepsilon \Vert v-w \Vert. 
\]
The linear map $f'(v_0)$ is called the \emph{differential} of $f$ at $v_0$.
\end{deftn}

One can derive all usual properties of the differential from this 
definition: unicity, chain rules, \emph{etc}.

\subsection{Lattices}

Let $E$ be a $K$-normed vector space. Given $r \in \R^+$, 
we denote by $B^-_E(r)$ (resp. by $B_E(r)$) the set of elements $x 
\in E$ such that $\Vert x \Vert < r$ (resp. $\Vert x \Vert \leq r$). 
We insist on the following maybe unusual fact: $B^-_E(r)$ and 
$B_E(r)$ are both open and closed for the norm topology on $E$.
Clearly, $B_K(0,1)$ is stable under multiplication. Moreover, the 
ultrametric triangle inequality ensures that $B_K(1)$ is stable under 
addition as well. Thus, $B_K(1)$ is a subring of $K$: it is often 
called the \emph{integer ring} of $K$.

\begin{deftn}
Let $E$ be a $K$-Banach.
A \emph{lattice} in $E$ is an open bounded sub-$B_K(1)$-module of $E$.
\end{deftn}

\begin{rmk}
Any lattice $H$ in $E$ is also closed since its complement is the
union of all cosets $a + H$ (with $a \not\in H$) which are all open.
\end{rmk}

The following Lemma shows that the notion of lattices behaves rather 
well under linear\footnote{We shall see in the sequel (\emph{cf} Lemma 
\ref{lem:main}) that, under some extra hypothesis, the linearity 
assumption can be drastically relaxed.} transformations.

\begin{lem}
\label{lem:morlat}
Let $E$ and $F$ be two $K$-Banach and $f : 
E \to F$ be a continuous \emph{surjective} linear map.
If $H$ is a lattice in $E$, then $f(H)$ is a lattice in $F$.
\end{lem}

\begin{proof}
It is clear that $f(H)$ is a module over $B_K(1)$ which is bounded.
The fact that it is open is a direct consequence of the open mapping
Theorem.
\end{proof}

To each lattice $H$ in $E$, one can attach a norm $\Vert \cdot \Vert_H$ 
on $E$ defined as follows. We pick $x \in E$, $x \neq 0$. We introduce
the set $I_x$ consisting of scalars $\lambda \in K$ such that $\lambda x 
\in H$. It follows from the fact that $H$ is open and bounded in $E$
that $I_x$ is nonempty and bounded in $K$. Therefore, the image $|I_x|$ 
of $I_x$ under the absolute value map is bounded and contains a positive
element. We can then define:
$$\Vert x \Vert_H = (\sup |I_x|)^{-1}.$$
It is an exercise to check that $\Vert \cdot \Vert_H$ is a norm over 
$E$.

\begin{lem} \label{lem:reductionball}
\begin{enumerate}
\item The norms $\Vert \cdot \Vert_H$ and $\Vert \cdot \Vert$ are 
equivalent.
\item Let $B^-_{E,H}(1)$ (resp. by $B_{E,H}(1)$) the set of elements $x 
\in E$ such that $\Vert x \Vert_H < 1$ (resp. $\Vert x \Vert_H \leq 1$).
Then:
$$B^-_{E,H}(1) \subset H \subset B_{E,H}(1)$$
and the last inclusion is an equality if the absolute value on $K$ is
discrete.
\end{enumerate}
\end{lem}

\begin{proof}
Left to the reader.
\end{proof}

The first part of Lemma \ref{lem:reductionball} asserts that the 
identity map $\iota_H : (E, \Vert \cdot \Vert_H) \to (E, \Vert \cdot 
\Vert)$ is an homeomorphism. Furthermore, by the second of the Lemma, 
$H$ is the image under $\iota_H$ of a lattice satisfying an extra 
assumption: it is ``between the open unit ball and the closed unit 
ball''.

\subsubsection*{Lattices and computers}

\todofor{Xavier}{Explain that if $E$ is finite dimensional and $K$ (and 
the absolute value is discrete), a lattice can be encoded by a finite 
amount of informations.}

\subsection{The main Lemma}

\begin{lem} \label{lem:main}
Let $E$ and $F$ be two $K$-Banach and $f : U 
\rightarrow F$ be a function defined on an open subset $U$ of $E$.
We assume that $f$ is differentiable at some point $v_0 \subset 
U$ and that the differential $f'(v_0)$ is surjective. 

Then, there exists a positive real number $\delta$ such that, for all $r 
\in (0, \delta)$, the equality
\begin{equation}
\label{eq:main}
f(v_0 + H) = f(v_0) + f'(v_0) \cdot H.
\end{equation}
holds for each lattice $H$ such that $B^-_E(r) \subset H \subset
B_E(r)$.
\end{lem}

\begin{rmk}
Given a lattice $H_0$ in $E$, the equality \eqref{eq:main} holds for $H 
= \lambda H_0$ with $\lambda$ small enough. To prove this, it is enough 
to apply Lemma \ref{lem:main} to the map $f \circ \iota_{H_0}$ where 
$\iota_{H_0} : (E, \Vert \cdot \Vert_{H_0}) \to (E, \Vert \cdot \Vert)$ 
is the ``identity map'' (which is an homeomorphism by Lemma 
\ref{lem:reductionball}).
\end{rmk}

\begin{proof}
We may assume that $v_0=0$ and $f(0)=0$. Since $f'(0)$ is surjective, by 
the open mapping Theorem, there exists $C>0$ such that $B_F(1) \subset 
C \cdot f'(0) \cdot B_E(1)$. Let $\varepsilon>0$ be such that 
$\varepsilon C < 1$. Let $U_\varepsilon \subset E$ be given by the fact 
that $f$ is differentiable at $0$. We may assume $U_\varepsilon = 
B_E(\delta)$ for some $\delta >0$.

Let $r \in (0, \delta)$.
Let us show that $f$ maps surjectively $H$ onto $f'(0) \cdot H$.
Let $x \in H$. We must show that $f(x) \in f'(0) \cdot H$. 
By differentiability at $0$, $\Vert f(x)-f'(0) \cdot x \Vert \leq 
\varepsilon \Vert x \Vert $. Therefore, there exists $y \in f'(0) \cdot 
H$ such that 
$\Vert y \Vert \leq \varepsilon r$. By the open mapping Theorem and the 
surjectivityy of $f'(0)$, there exists $x_1 \in E$ such that $f'(0) \cdot 
x_1 =y$ and $\Vert x_1 \Vert \leq \varepsilon C \cdot r < r$. Thus, $x_1 
\in B^-_E(r) \subset H$ and $f(x)= f'(0) \cdot (x-x_1) \in f'(0) 
\cdot H$.

We shall now prove surjectivity. Let $y \in f'(0) \cdot H$. 
Let $x_0 \in H$ be such that $y = f'(0) \cdot x_0$.
We define by induction two sequences $(x_n)$ and $(z_n)$ by
\begin{itemize}
\item $z_n \in E$ is such that
$f'(0) \cdot z_n = y - f(x_n)$ and
$\Vert z_n \Vert \leq C \cdot \Vert y - f(x_n) \Vert$,
\item $x_{n+1}=x_n+z_n$.
\end{itemize}
By definition, $\Vert z_0 \Vert < C \Vert y_0\Vert$ and $y_0 
= -f(x_0)+f'(0) \cdot x_0$. Therefore, $\Vert y_0 \Vert < \varepsilon 
\Vert x_0 \Vert \leq \varepsilon r$. Thus $\Vert z_0 \Vert \leq 
\varepsilon C \cdot r < r$. Thus, $x_1$ and $z_0$ lie in $B^-_E(r) 
\subset H$. An easy induction involving an analogous computation
shows that the sequences $(x_n)$ and $(z_n)$ are well defined and
take their values in $H$.

Now, we notice that
$y - f(x_{n+1}) = f(x_n) - f(x_{n+1}) + f'(0) \cdot z_n$.
Therefore, using differentiability, we get:
$$\Vert y  - f(x_{n+1}) \Vert 
\leq \varepsilon \Vert z_n \Vert \leq \varepsilon C \cdot \Vert 
y - f(x_n) \Vert,$$
from what we deduce that $\Vert y - f(x_{n+1}) \Vert = 
O(a^n)$ and $\Vert z_n \Vert = O(a^n)$ with $a = \varepsilon C < 1$.
This shows that $(x_n)$ is a Cauchy sequence and therefore converges 
(since $E$ is complete). If $x$ denotes the limit of $(x_n)$, we have
$x \in H$ and $y=f(x)$. The result is proven.
\end{proof}

\todofor{Tristan}{Improve the end of this Subsection}

Before discussing applications of Lemma \ref{lem:main} to ultrametric 
precision, we would like to describe a little bit more restrictive 
setting where the constant $\delta$ appearing in this Lemma can be 
computed explicitly.
We refer to Schneider \cite{Schneider}, \S 6, for a definition of a 
locally analytic function.

\begin{rmk}
Let $f$ be locally analytic function $K^n \rightarrow K^m$. Let $x \in K^n$ be such that $f'(x)$ is surjective. Then the radius of the ball needed can be read from the Newton polygon given by the norms of the Taylor series expansion of $f$ at $x$. We assume the $C$ given by the Banach-Schauder is given, $\varepsilon = \frac{1}{C}$ and now, we try to recover the link between $\varepsilon$ and $\delta$.
\end{rmk}

\begin{proof}
Let us assume that $x=0$, $f(0)=0$, and for $r>0$, $f(x)=f'(0) \cdot x +\sum_{n \geq 2} \frac{f^{(n)}(0)}{n!} \cdot (x, \dots,x)$ is the Taylor series expansion of $f$ at $0$, coinciding on $B_E (0,r)$.

We shall now find $l$ such that if $x \in B_{K^n}(0,l)$, then $\Vert \sum_{n \geq 2} \frac{f^{(n)}(0)}{n!}(x,\dots,x) \Vert \leq \varepsilon$.

By continuity and ultrametricity, it is enough for $l$ to be such that, if $x \in B_{K^n}(0,l)$, then  \[ \sup_{n \geq 2} \interleave f^{(n)}(0) \interleave \frac{\Vert x \Vert^n }{ \vert n! \vert } \leq \varepsilon \Vert x \Vert .\] 

Therefore, \[ \sup_{n \geq 2} \interleave f^{(n)}(0) \interleave \frac{l^{n-1} }{\vert n! \vert } \leq \varepsilon . \] 

In other words, $\forall n \geq 2$, $\interleave f^{(n)}(0) \interleave \frac{l^{n-1} }{\vert n! \vert } \leq \varepsilon.$

By passing to logarithm and valuation, we get for all $n \geq 2$: \[ \log_p (\interleave f^{(n)}(0) \interleave )-(n-1) \log_p l-val (n!)\geq - \log_p \varepsilon.\]

By Legendre formula, it is enough that for all $n \geq 2$:
\[
\log_p (\interleave f^{(n)}(0) \interleave )-(n-1) (\log_p l-(n-1)) \geq - \log_p \varepsilon.
\]

If we denote by $NP_F$ the Newton polygon application of the power series 
\[
F(t) = \sum_{k\geq 0}^{+\infty} \log_p (\interleave f^{(k+1)}(0) \interleave ) t^k
\]
then it is enough that:
\[
\sup_{ x \in \mathbb{R}_+} \left( NP_F(x) -x  \left( \log_p l - \frac{1}{p-1} \right) \right) \geq - \log_p \varepsilon.
\]

If $NP_F^*$ is the Legendre-Fanchel transform of $NP_F$, then it is enough that:
\[ NP_F^* \left( \log_p l - \frac{1}{p-1} \right) \leq \log_p \varepsilon.\]

Therefore, by definition of the Legendre-Fanchel transform, it is enough to:
\begin{itemize}
\item take $M=(0,\log_p \varepsilon)$,
\item draw the line passing by $M$ tangent to the graph of $NP_F$,
\item if we denote by $\mu$ the slope of this line, then take $l$ such that $\mu + \frac{1}{p-1}=\log_p l$.
\end{itemize}
\end{proof}

\subsection{Automatic sharp track of precision}

\todofor{Xavier}{
Explain here how to build a software that tracks precision automatically 
and sharply.
}

\subsubsection*{Working precision}

\section{Generalization to manifolds}
\label{sec:manifold}

Many natural $p$-adic objects do not lie in vector spaces:
points in projective spaces or elliptic curves,
subspaces of a fixed vector space (which lie in Grassmannians),
classes of isomorphism of certain curves (which lie in various moduli spaces),
\emph{etc.}  In this section we extend the formalism 
developed in Section \ref{sec:philosophy} to a more general setting:
we consider the quite general case of strictly
differentiable manifolds locally modeled on ultrametric Banach spaces.
This covers all the 
aforementioned examples.

In what follows, the letter $K$ always refers to an ultrametric field 
(\emph{cf} \S \ref{ssec:ultrametric}). The absolute value over $K$ is 
denoted by $|\cdot|$.

\subsection{Strictly differentiable $K$-manifolds}
\label{ssec:manifold}

The theory of finite dimensional $K$-manifolds is presented for example 
in \cite{Schneider}, parts 8 and 9. In this Section, we shall work with
a slightly different notion of manifolds which allows also Banach vector 
spaces of infinite dimension.
More precisely, for us, a \emph{strictly differentiable $K$-manifold} 
(or just \emph{$K$-manifold} for short) is the data of a topological 
space $V$ together with an open covering $V = \bigcup_{i \in I} V_i$ 
(where $I$ is some set) and, for all $i \in I$, an homeomorphism 
$\varphi_i : V_i \to U_i$ where $U_i$ is an open subset of a $K$-Banach 
$E_i$ such that for all $i,j \in I$ for which $V_i \cap V_j$ is 
nonempty, the composite map
\begin{equation}
\label{eq:psiij}
\psi_{ij} : 
\varphi_i(V_{ij}) \stackrel{\varphi_i^{-1}}{\longrightarrow} 
V_{ij} \stackrel{\varphi_j}{\longrightarrow} \varphi_j(V_{ij})
\quad \text{(with } V_{ij} = V_i \cap V_j \text{)}
\end{equation}
is strictly differentiable at all points in its domain. We recall that
the mappings $\varphi_i$ above are the so-called \emph{charts} and that
their collection defines an \emph{atlas} of $V$. In the sequel, we shall
assume further that the open covering $V = \bigcup_{i \in I} V_i$ is
locally finite, which means that every point $x \in V$ lies only in a
finite number of $V_i$'s. Trivial examples of $K$-manifolds are 
$K$-Banach themselves.

If $V$ is a $K$-manifold and $x$ is a point of $V$, we define the 
\emph{tangent space} $T_x V$ of $V$ at $x$ as the space $E_i$ for some 
$i$ such that $x \in V_i$. We note that if $x$ belongs to $V_i$ and 
$V_j$, the linear map $\psi'_{ij}(\varphi_i(x))$ defines an isomorphism 
between $E_i$ and $E_j$. Furthermore these isomorphisms are compatible 
in an obvious way. This implies that the definition of $T_x V$ given 
above does not depend (up to some canonical isomorphism) on the index 
$i$ such that $x \in V_i$ and then makes sense.

As usual, we can define the notion of strict differentiability (at some 
point) for a continuous mapping between two $K$-manifolds by viewing it 
through the charts. A strictly differentiable map $f : V \to V'$ induces 
a linear map on tangent spaces $f'(x) : T_x V \to T_{f(x)} V'$ for all 
$x$ in the domain $V$. It is called the \emph{differential} of $f$ at
$x$.

\subsection{Precision data}

\todofor{David/Xavier}{Complete this Subsection}

Going back to our problem of precision, given $V$ a $K$-manifold as 
above, we would like to be able to deal with ``approximations up to some 
precision'' of elements in $V$, \emph{i.e.} expressions of the form $x + 
O(H)$ where $x$ belongs to a dense \emph{computable} subset of $V$ and 
$H$ is a ``precision data''.
The aim of this paragraph is to give a precise meaning to the expression 
$x + O(H)$ and to prove several basic properties of it.
For now, we fix a $K$-manifold $V$ and we use freely the notations $I$, 
$V_i$, $\varphi_i$, \emph{etc.} introduced in \S \ref{ssec:manifold}.

\begin{deftn}
Let $x \in V$. 
A \emph{precision datum} at $x$ is a lattice in the tangent space 
$T_x V$.
\end{deftn}

\begin{deftn}
Let $x \in V$ and $H$ be a precision datum at $x$.
For all $i \in I$ such that $x \in V_i$, we set:
$$x + O_i(H) = \varphi_i^{-1}\big(\varphi_i(x) + \varphi_i'(x)(H)\big) 
\subset V.$$
\end{deftn}

We emphasize that if $x$ belongs to $V_i$ and $V_j$ at the same time,
the subsets $x + O_i(H)$ and $x + O_j(H)$ differ in general. It is 
however not the case if $H$ is small enough.

\begin{prop}
\label{prop:independence}
Let $x \in V$ and $H$ be a precision data at $x$.
If $H$ is ``small enough'' \todo{(precise!)}, the subset $x + 
O_i(H)$ does not depend on $i$.
\end{prop}

\begin{proof}
Let $i$ and $j$ be two indices such that $x$ belongs to $V_i$ and $V_j$. 
Set $x_i = \varphi_i(x) \in E_i$ and $H_i = \varphi'_i(x)(H)$. Going 
back to the definitions, we see that the equality $x + O_i(H) = x + 
O_j(H)$ is equivalent to:
$$\psi_{ij}(x_i + H_i) = \psi_{ij}(x_i) + \psi'_{ij}(x_i)(H_i)$$ 
where we recall that $\psi_{ij}$ is defined by Eq.~\eqref{eq:psiij}.
\todo{Complete the proof and write the correct condition on $H$!}
\end{proof}

Under the assumption of Proposition \ref{prop:independence}, we define
$x + O(H)$ as $x + O_i(H)$ for some (equivalently all) $i$ such that
$x \in V_i$.

\subsubsection*{Change of base point}

In order to restrict ourselves to elements $x$ lying in a dense 
computable subset, we need to compare $x_0 + O(H_0)$ with varying $x + 
O(H)$ when $x$ and $x_0$ are close enough.
Let us first examine the situation in a fixed given chart: we fix some 
index $i \in I$ and pick two elements $x_0$ and $x$ in $V_i$. We consider 
in addition a precision datum $H_0$ at $x_0$ and we want to produce a 
precision datum $H$ at $x$ such that $x_0 + O_i(H_0) = x + O_i(H)$. 
We remark that the tangent spaces $T_{x_0} V$ and $T_x V$ are both 
isomorphic to $E_i$ via the maps $\varphi'_i(x_0)$ and 
$\varphi'_i(x)$ respectively. A natural candidate for $H$ is then:
\begin{equation}
\label{eq:Hprime}
H = \left(\varphi'_i(x)^{-1} \circ \varphi'_i(x_0)\right) (H_0).
\end{equation}
And indeed, one can check that $x + O(H)$ is indeed equal to $x_0 + 
O(H_0)$ as soon as $x$ and $x_0$ are close enough in the following sense: 
the difference $\varphi_i(x) - \varphi_i(x_0)$ lies in the lattice $H_i = 
\varphi'_i(x_0)(H_0)$. We furthermore have a property of independence on 
$i$.

\begin{prop}
We assume that $H_0$ is ``small enough'' \todo{(precise!)} and that $x$ is close 
enough to $x_0$. Then
\begin{enumerate}[(i)]
\item the lattice $H$ defined by \eqref{eq:Hprime} does not depend 
on $i$,
\item the pairs $(x,H)$ and $(x_0, H_0)$ satisfy the assumptions of
Proposition \ref{prop:independence} and we have $x + O(H) = x_0 + O(H_0)$.
\end{enumerate}
\end{prop}

\begin{proof}
We first prove (i). For an index $i$ such that $x, x_0 \in V_i$, let us 
denote by $f_i : T_{x_0} V \to T_x V$ the composite $\varphi'_i(x)^{-1} 
\circ \varphi'_i(x_0)$. Given an extra index $j$ satisfying the same
assumption, we know by \todo{(find a reference)} that the difference
$f_i - f_j$ goes to $0$ when $x$ converges to $x_0$. Since $H_0$ is open
in $T_{x_0} V$, this implies that $(f_j - f_i)(H_0)$ contains $f_i(H_0)$ and 
$f_j(H_0)$ if $x$ and $x_0$ are close enough. Now, pick $w \in f_j(H_0)$ and 
write it $w = f_j(v)$ with $v \in H_0$. Then $w$ is equal to $f_i(v) + 
(f_j - f_i)(v)$ and then belongs to $f_i(H_0)$ because each summand does. 
Therefore $f_j(H_0) \subset f_i(H_0)$. The inverse inclusion can be proved 
in the same way.

We now prove (ii). \todo{Write the proof.}
\end{proof}

\subsection{Generalization of the main Lemma}

\todofor{David/Xavier}{Extend Lemma \ref{lem:main} to manifolds.}

\section{Examples and applications}

In this Section, we illustrate the theory developed in this paper by 
giving examples and applications. In \S \ref{ssec:differentials}, we 
compute the differential of a bunch of usual operations involving 
$p$-adic objects of different nature. Very often, we will observe that 
this computation is easy even if the underlying operation is 
sophisticated (\emph{e.g.} Gr\"obner basis).
In \S \ref{ssec:SOMOS-solution}, we study the SOMOS 4 sequence 
introduced in \S \ref{ssec:stepbystep} and, making a crucial use of 
Lemma \ref{lem:main} and Proposition \ref{prop:}, we design a quite 
\emph{stable} algorithm for computing it.

\subsection{Differential of usual operations}
\label{ssec:differentials}

As in the previous Sections, we always work over a fixed ultrametric 
field $K$, whose absolute value is denoted by $|\cdot|$. We recall that 
usual examples are the field of $p$-adic numbers $\Q_p$ and the field of 
Laurent series $k((X))$ over a base field $k$.

In what follows, we use freely the ``method of physicists'' to compute 
differentials: given a function $f$ differentiable at some point $x$, we 
consider a small perturbation $dx$ of $x$ and write $f(x+dx) = y + dy$ 
by expanding LHS and neglecting terms of order $2$. The differential of 
$f$ at $x$ is then the linear mapping $dx \mapsto dy$.

\subsubsection*{Numbers}

The most basic operations are, of course, sums, products and inverses of 
elements of $K$. Their differential are well-known and quite easy to 
compute: if $z = x + y$ (resp. $z = xy$, resp $z = \frac 1 x$), we have 
$dz = dx + dy$ (resp. $dz = x \cdot dy + dx \cdot y$, resp $dz = - 
\frac{dx}{x^2}$).

A little bit more interesting is the $n$-th power map $f$ from $K$ to 
itself. Its differential $f'(x)$ is obtained by differentiate the 
equality $y = x^n$: we obtain $dy = n x^{n-1} dx$.
Hence $f'(x)$ maps to ball $B_K(r)$ (where $r$ is some positive real 
number) to $B_K(|n|{\cdot} r)$. According to Lemma \ref{lem:main}, this 
means that the behaviour of the precision depends on the absolute value 
of $n$. By the ultrametric inegality, we always have $|n| \leq 1$ but 
this inegality might be strict. It happens for instance if the 
characteristic of $K$ is divisible by $p$, in which case $n$ vanishes in 
$K$. In that case $f'(x)$ also vanishes and Lemma \ref{lem:main} does 
not apply. 
Another situation (which is more interesting) is that of $p$-adic 
numbers: let us take $K = \Q_p$ and $n = p^k$ for some integer $k$. Then
$|n| = p^{-k}$ and Lemma \ref{lem:main} reflects the well-known fact that
raising a $p$-adic number to the $p^k$-th power increases the precision 
by $k$ extra digits.

\subsubsection*{Univariate polynomials}

Beyond sums and products (which can be treated as before), a basic 
operation involving polynomials is Euclidean division. Let us compute 
the differential of it. We start with two polynomials $A$ and $B$ with 
$B \neq 0$. The Euclidean division of $A$ by $B$ is written $A = BQ + R$ 
with $\deg R < \deg B$. Differentiating the above equality we find: 
$dA - dB \cdot Q = B \cdot dQ + dR$,
which implies that $dQ$ and $dR$ are respectively obtained as the 
quotient and the remainder of the Euclidean division of $dA - dB \cdot 
Q$ by $B$. This gives the differential. We note that the discussion 
above extends readily to convergent series (see also 
\cite{caruso-lubicz}).

Another usual operations on polynomials is the factorization. Suppose 
that we are given a monic polynomial $P \in K[X]$ written as a product 
$P_0 = A_0 B_0$ where $A_0$ and $B_0$ are monic and coprime. Hensel's 
lemma implies that there exists a small neighbourhood $\mathcal U$ of 
$P_0$ such that any monic polynomial $P \in \mathcal U$ factors uniquely 
as $P = A B$ with $A$ and $B$ monic and close enough to $A_0$ and $B_0$ 
respectively. Thus, we can consider the application $f : P \mapsto (A,B)$ 
defined on the subset of $\mathcal U$ consisting of monic polynomials.
We want to differentiate $f$ at $P_0$. For this, we differentiate the 
equality $P = A B$ around $P_0$, obtaining 
\begin{equation}
\label{eq:difffactor}
dP = A_0 \cdot dB + B_0 \cdot dA.
\end{equation}
where $dP$, $dA$ and $dB$ have degree less than $\deg P$, $\deg A$ and 
$\deg B$ respectively. If $A_0 U_0 + B_0 V_0 = 1$ is a Bezout relation
between $A_0$ and $B_0$, it follows from Eq.~\eqref{eq:difffactor} that
$dA$ (resp. $dB$) is the remainder in the Euclidean division of $V_0
{\cdot} dP$ by $A_0$ (resp. of $U_0 {\cdot} dP$ by $B_0$).

An important special case of the previous study occurs when $A_0$ has 
degree $1$, that is $A_0(X) = X - \alpha$ with some $\alpha \in K$. The 
application $P \mapsto A$ is then nothing but the mapping that follows 
the simple root $\alpha$. Of course, its differential around $P_0$ can 
be computed by the above method. Nevertheless there is a more direct way 
to obtain it in this particular case: we write $(P + dP)(\alpha + 
d\alpha) = 0$ and expand this. We obtain this way $P'(\alpha) d\alpha + 
dP(\alpha) = 0$ where $P'$ denotes the derivative of $P$. Since $\alpha$ 
is a simple root, $P'(\alpha)$ does not vanish and we recover $d \alpha 
= - \frac{dP(\alpha)}{P'(\alpha)}$.

\subsubsection*{Multivariate polynomials}

We consider the ring $K[\XX] = K[X_1, \ldots, X_n]$ of polynomials in $n$
variables over $K$ and fix a monomial order on it. We then have a 
notion of division in $K[\XX]$: if $f, f_1, \ldots, f_s$ are polynomials
in $K[\XX]$, there exists a writing:
$$f = q_1 f_1 + \cdots + q_s f_s + r$$
where $q_1, \ldots, q_s, r \in K[\XX]$ and no term of $r$ is divisible
by the leading term of some $f_i$ ($1 \leq i \leq s$). Moreover, assuming
that $(f_1, \ldots, f_s)$ is a Gr\"obner basis\footnote{Without this
assumption, there still exists a canonical choice of $r$. We do not know
if the computation of the differential that follows extends to a more
general setting.} (of the ideal generated by these polynomials), the 
polynomial $r$ is uniquely determined and called the \emph{remainder} of 
the division of $f$ by the family $(f_1, \ldots, f_s)$. The application 
$(f, f_1, \ldots, f_s) \mapsto r$ is then well defined and similarly to 
the case of univariate polynomials, we can compute its differential: we 
find that $dr$ is obtained as the remainder of the division of $f - (q_1 
\cdot d f_1 + \cdots + q_s \cdot d f_s)$ by $(f_1, \ldots, f_s)$.

We now focus on the computation of Gr\"obner basis. Recall that any 
ideal $I \subset K[\XX]$ admits a unique reduced Gr\"obner basis. We can 
then consider the application mapping a family $(f_1, \ldots, f_s)$ of 
\emph{homogeneous}\footnote{This restriction will be convenient for us 
mainly because we are using the article \cite{Vaccon}, which deals only 
with homogeneous polynomials. It is however probably not essential.} 
polynomials of fixed degrees to the reduced Gr\"obner basis $(g_1, 
\ldots, g_t)$. It follows from Theorem 1.1 of \cite{Vaccon} that this 
application at $(f_1, \ldots, f_s)$ is continuous provided that:
\begin{itemize}
\item the sequence $(f_1, \ldots, f_s)$ is regular
\item for all $i \in \{1, \ldots, s\}$, the ideal generated by
$f_1, \ldots, f_i$ is wearly-$w$ where $w$ denotes the fixed monomial
order (\emph{cf} \cite{Vaccon} for more precisions).
\end{itemize}
A similar argument proves that it is actually differentiable at such
points. We are now going to compute the differential. For this we
remark that since the families $(f_1, \ldots, f_s)$ and $(g_1, \ldots,
g_t)$ generate the same ideal, we have a relation:
\begin{equation}
\label{eq:prodgrobner}
(g_1, \ldots, g_t) = (f_1, \ldots, f_s) \cdot A
\end{equation}
where $A$ is a $(s \times t)$ matrix with coefficients in $K[\XX]$.
Moreover following Vaccon's construction, we see that the entries of $A$ 
can be chosen in such a way that they define differentiable functions.
Differentiating Eq.~\eqref{eq:prodgrobner}, we get:
$$(d f_1, \ldots, d f_s) \cdot A = - (f_1, \ldots, f_s) \cdot dA + 
(d g_1, \ldots, d g_t).$$
Finally, using that we are working with \emph{reduced} Gr\"obner
basis, the above equality implies that $d f_i$ is the remainder in
the division of the $i$-th coefficient of the product
$(d f_1, \ldots, d f_s) \cdot A$ by the family $(f_1, \ldots, f_s)$.

\subsubsection*{Matrices}

Differentiating ring operations (sum, products, inverse) over matrices 
is again straightforward; we just need to be careful that matrix 
algebras are (in general) noncommutative.

We now move to determinants, \emph{i.e.} we compute the differential
of the function $\det : M_n(K) \to K$. It is a standard computation.
To begin with, we consider an invertible matrix $M$ and write:
\begin{eqnarray*}
\det(M + dM) &=& \det(M) \cdot \det(I + M^{-1} \cdot dM)  \\
&=& \det(M) \cdot \big(1 + \tr(M^{-1} \cdot dM)\big) \\
&=& \det(M) + \tr(\com(M) \cdot dM)
\end{eqnarray*}
where $\com(M)$ denotes the comatrix of $M$. The differential of 
$\det$ at $M$ is then $dM \mapsto \tr(\com(M) \cdot dM)$. It turns
out that this formula is still valid when $M$ is not invertible.
The same computation extends readily to characteristic polynomials,
since they are defined as determinants. More precisely, let us 
consider the function $\chi : M_n(K) \to K_{\leq n}[X]$ (where
the latter ring denotes the finite dimensional vector space of 
polynomials over $K$ of degree at most $n$) taking a matrix $M$
to its characteristic polynomial $\det(X-M)$.
Then $\chi$ is differentiable at each point $M \in M_n(K)$ and its 
differential is given by $dM \mapsto \tr(\com(X{-}M) \cdot dM)$.

Other important tools related to matrices are usual factorizations as, 
for example, LU factorization. In what follows, let us agree to define a 
LU factorization of a square matrix $M \in M_n(K)$ as a writing $M = LU$ 
where $L$ is lower triangular and unipotent and $U$ is upper triangular. 
Such a writing exists and is unique provided that all principal minors 
of $M$ do not vanish. We can then consider the mapping $M \mapsto (L,U)$ 
defined over the Zariski-open set of matrices satisfying the above 
condition. In order to differentiate it, we differentiate the relation 
$M = LU$ and rewrite the result as follows:
$$L^{-1} dM \: U^{-1} = L^{-1} \cdot dL + dU \cdot U^{-1}.$$
We remark that in the right hand side of the above formula, the first
summand is lower triangular and all its diagonal entries vanish whereas 
the second summand is upper triangular. Hence in order to compute $dL$
and $dU$, one can proceed as follows: (1)~we compute the product $dX = 
L^{-1} dM \: U^{-1}$, (2)~we separate the lower and upper part of $dX$
obtaining this way $L^{-1} \cdot dL$ and $dU \cdot U^{-1}$ and (3)~we
recover $dL$ and $dU$ by multiplying the above matrices by $L$ on the 
left and $U$ on the right respectively.

A similar argument works for QR factorization.
For us, a QR factorization of a square matrix $M \in M_n(K)$ will be
a writing $M = QR$ where $R$ is unipotent upper triangular and $Q$ is
orthogonal in the sense that $\trans Q \cdot Q$ is diagonal. As before, such 
a writing exists and is unique on a Zariski-open subset of $M_n(K)$. The
mapping $f : M \mapsto (Q,R)$ is then well defined on this subset. We 
would like to emphasize at this point that the orthogonality condition
defines a sub-manifold of $M_n(K)$ which is a apparently \emph{not} a
vector space (it is defined by equations of degree $2$). The codomain
of $f$ is then also a manifold; this example then fits to the setting
of \S \ref{sec:manifold} but not to those of \S \ref{sec:philosophy}. 
The generalization made in \S \ref{sec:manifold} is then needed.
Anyway, we can differentiate $f$ by following the method we used for LU 
factorization: differentiating the relation $M = QR$, we obtain:
\begin{equation}
\label{eq:diffQR}
\trans Q \cdot dM \cdot R^{-1} = \trans Q \cdot dQ + \Delta \cdot dR 
\cdot R^{-1}
\end{equation}
where $\Delta = \trans Q \cdot Q$ is a diagonal matrix by definition.
Moreover by differentiating $\trans Q \cdot Q = \Delta$, we find that
$\trans Q \cdot dQ$ can be written as the sum of an antisymmetric 
matrix and a diagonal one. Since moreove $dR \cdot R^{-1}$ is upper
triangular with all diagonal entries equal to $0$, we see that 
Eq.~\eqref{eq:diffQR} is enough to compute $dQ$ and $dR$ from $Q$, $R$ 
and $dM$.

\subsubsection*{Vector spaces}

\todo{Kernels, images, intersection, sum}

\subsection{The SOMOS 4 sequence}
\label{ssec:SOMOS-solution}

Recall (\emph{cf} \S \ref{ssec:stepbystep}) that a SOMOS 4 sequence is a 
four-term inductive sequence defined by $u_{n+4} = \frac{u_{n+2} u_{n+4} 
+ u_{n+3}^3}{u_n}$. We recall also that SOMOS sequences exhibit the 
Laurent phenomenon: its means that the four initial terms $u_0, u_1, 
u_2, u_3$ are some indeterminates, that each $u_n$ is a Laurent 
polynomial with coefficients in $\Z$ in these indeterminates.

From now, we will always consider SOMOS sequences with values in $\Q_p$ 
(for some prime number $p$) and assume that $u_0, u_1, u_2, u_3$ are 
chosen such that all terms $u_n$'s are determined (\emph{i.e.} there is 
no division by zero). We assume also for simplicity that $u_0, u_1, u_2, 
u_3$ are all units in $\Z_p$. By Laurent phenomenon, this implies first 
that all $u_n$'s lie in $\Z_p$ and second that if $u_0, u_1, u_2, u_3$ 
are known with finite precision $O(p^N)$ then all terms $u_n$'s are 
known with the same precision. 

In this last paragraph, we illustrate the theory developed in this 
article by designing an algorithm for computing the $u_n$'s at precision 
$O(p^N)$. The algorithm is presented at the top of page 
\pageref{algo:SOMOS}. We now prove that it is correct.
%
\begin{algorithm}[t]
\SetKwInOut{Assumption}{Assumption}
\KwIn{$a,b,c,d$ --- four initial terms of a SOMOS 4 sequence $(u_n)_{n \geq 0}$}
\KwIn{$n, N$ --- two integers}
\Assumption{$a$, $b$, $c$ and $d$ lie in $\Z_p^\times$ and are known at 
precision $O(p^N)$}
\Assumption{None of $u_i$ $(0 \leq i \leq n$) is divisible by $p^N$}
\KwOut{$u_n$ at precision $O(p^N)$}
\BlankLine
$\prec$ $\leftarrow$ $N$\;
\For {$i$ from $1$ to $n-3$}{
  $\prec$ $\leftarrow$ $\prec + v_p(bd + c^2) - v_p(a)$\;
  {\bf lift} (arbitrary) 
    $b$, $c$ and $d$ to precision $O(p^{\prec})$\label{line:lift}\;
  $\prec$ $\leftarrow$ $\prec - v_p(a)$\;
  $e$ $\leftarrow$ $\frac{bd+c^2} a$; \hspace{1cm}
   \tcp{$e$ is known at precision $O(p^\prec)$}
  $a, b, c, d$ $\leftarrow$ $b + O(p^{\prec}), c + O(p^{\prec}), d + O(p^{\prec}), e$\;
}
\Return{$d + O(p^N)$}\;
\caption{\sc SOMOS$(a, b, c, d, n, N)$}\label{algo:SOMOS}
\end{algorithm}
%
We introduce the (partial) function $f : \Q_p^4 \to \Q_p^4$ defined by 
$f(a,b,c,d) = (b,c,d,\frac{bd+c^2}a)$, so that, for all $i$, we have 
$(u_i, u_{i+1}, u_{i+2}, u_{i+3}) = f_i(u_0, u_1, u_2, u_3)$ where $f_i
= f \circ \cdots \circ f$ ($i$ times). Clearly, $f$ is strictly 
differentiable on $\Q_p^\times \times \Q_p^3$ and its differential in the 
canonical basis is given by the matrix:
$$D(a,b,c,d) = \begin{pmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
-\frac{bd+c^2}{a^2} & \frac d a & \frac {2c} a & \frac b a
\end{pmatrix}$$
whose determinant is $\frac{bd+c^2}{a^2}$. Thus, if the $(i+4)$-th term 
of the SOMOS sequence is defined, the mapping $f_i$ is differentiable
at $(u_0, u_1, u_2, u_3)$ and its differential $\varphi_i = 
f'_i(u_0, u_1, u_2, u_3)$ is given by the matrix:
$$D_i = D(u_{i-1}, u_i, u_{i+1}, u_{i+2}) \cdots
D(u_1, u_2, u_3, u_4) \cdot D(u_0, u_1, u_2, u_3).$$
Moreover thanks to the Laurent phenomenon, we know that $D_i$ has
integral coefficients, \emph{i.e.} $\varphi_i$ stabilizes the lattice
$\Z_p^4$.
We are now going to prove by induction on $i$ that, at the end of the 
$i$-th iteration of the loop, we have $\prec = N + v_p(\det D_i)$ and
\begin{equation}
\label{eq:congrSOMOS}
(a, b, c, d) \equiv (u_i, u_{i+1}, u_{i+2}, u_{i+3}) \pmod
{p^N \varphi_i(\Z_p^4)}.
\end{equation}
The first point is easy. Indeed, from $D_i = D(u_{i-1}, u_i, u_{i+1}, 
u_{i+2}) D_{i-1}$, we deduce $\det D_i = \det D_{i-1} \cdot 
\frac{u_i}{u_{i-3}}$ and the assertion follows by taking determinants
and using the induction hypothesis. 
Let us now establish Eq.~\eqref{eq:congrSOMOS}. To avoid confusion, 
let us agree to denote $a'$, $b'$, $c'$, $d'$ and $\prec'$ the values of 
$a$, $b$, $c$, $d$ and $\prec$ respectively at the \emph{beginning} of 
the $i$-th iteration of the loop. By induction hypothesis (or by 
initialization if $i = 1$), we have:
\begin{equation}
\label{eq:congrSOMOS2}
(a', b', c', d') \equiv (u_{i-1}, u_i, u_{i+1}, u_{i+2}) \pmod
{p^N \varphi_{i-1}(\Z_p^4)}.
\end{equation}
Moreover, we know that the determinant of $\varphi_{i-1}$ has valuation
$\prec'$. Hence \eqref{eq:congrSOMOS2} remains true if $a'$, $b'$, $c'$
and $d'$ are replaced by other values which are congruent to them modulo
$p^{\prec'}$. In particular it holds if $a'$, $b'$, $c'$ and $d'$ denotes
the values of $a$, $b$, $c$ and $d$ after the execution of line
\ref{line:lift}. Now applying Lemma \ref{lem:main} and Proposition
\ref{prop:} to $\varphi_{i-1}$ and $\varphi_i$ (at the point $(u_0,
u_1, u_2, u_3)$), we get:
$$f\big((u_{i-1}, u_i, u_{i+1}, u_{i+2}) + p^N \varphi_{i-1}(\Z_p^4)\big)
= (u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4).$$
By the discussion above, this implies in particular that $f(a',b',c',d')$
belongs to $(u_i, u_{i+1}, u_{i+2}, u_{i+3}) + p^N \varphi_i(\Z_p^4)$. We
conclude by remarking first that $(a,b,c,d) \equiv f(a',b',c',d') \pmod 
{p^\prec \Z_p^4}$ by construction and second that $p^\prec \Z_p^4 \subset
p^N \varphi_i(\Z_p^4)$.

Finally Eq.~\eqref{eq:congrSOMOS} applied with $i = n-3$ together with
the fact that $\varphi_i$ stabilizes $\Z_p^4$ imply that, when we exit
the loop, the value of $d$ is congruent to $u_n$ modulo $p^N$. Hence,
our algorithm returns the correct value.

\medskip

Let us conclude this subsection by remarking that Algorithm 
\ref{algo:SOMOS} performs computations at precision at most $O(p^{N+v})$ 
where $v$ is the sum of the valuations of five consequence terms among 
the first $n$ terms of the SOMOS sequence we are considering. 
Experiments show that the value of $v$ varies like $c \cdot \log n$ 
where $c$ is some constant. Assuming that we are using a FFT-like 
algorithm to compute products of integers, the complexity of Algorithm 
\ref{algo:SOMOS} is then expected to be $\tilde O(N n)$ where the 
notation $\tilde O$ means that we hide logarithmic factors.

We can compare this with the complexity of the more naive algorithm 
consisting of lifting the initial terms $u_0, u_1, u_2, u_3$ to enough 
precision and then do the computation using a naive step-by-step 
tracking of precision. In this setting, the required original precision 
is $O(p^{N+2v'})$ where $v'$ is the sum of the valuation of the $u_i$'s 
for $i$ varying between $0$ and $n$. Experiments show that $v'$ is about 
$c' \cdot n \log n$ (where $c'$ is a constant), which leads to a 
complexity in $\tilde O (N n + n^2)$. Our approach is then interesting
when $n$ is large compared to $N$: under this hypothesis, it saves
roughly a factor $n$.

%\section{Conclusions}

\begin{thebibliography}{10}

\bibitem{Bostan}
  A.~Bostan, L.~Gonz\'alez-Vega, H.~Perdry, \'E.~Schost,
  \emph{From Newton sums to coefficients: complexity issues in characteristic $p$},
  Proceedings MEGA'05 (2005).

\bibitem{Caruso}
  X.~Caruso,
  \emph{Random matrices over a DVR and LU factorization},
  preprint (2012)

\bibitem{Gaudry}
  P.~Gaudry, T.~Houtmann, A.~Weng, C.~Ritzenthaler, D.~Kohel,
  \emph{The $2$-adic CM method for genus $2$}, 
  Asiacrypt 2006, vol. 4284 of Lecture Notes in Comput. Sci. (2006), 114--129

\bibitem{Kedlaya}
  K.~Kedlaya,
  \emph{Counting points on hyperelliptic curves using Monsky--Washnitzer cohomology}, 
  J. Ramanujan Math. Soc. {\bf 16} (2001)

\bibitem{Lercier}
  R.~Lercier, T.~Sirvent,
  \emph{On Elkies subgroups of $\ell$-torsion points in elliptic curves defined over a finite field},
  J. Th\'eorie des Nombres de Bordeaux {\bf 20} (2008), 783--797

\bibitem{Schneider}
  P.~Schneider,
  \emph{$p$-adic Lie Groups}, Springer (2011)

\bibitem{Vaccon}
  T.~Vaccon,
  \emph{Matrix-F5 algorithms over finite-precision complete discrete 
  valuation fields},
  preprint (2014)
\end{thebibliography}

\end{document}
