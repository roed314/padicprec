\documentclass[sigconf]{acmart}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\hyphenation{regarding}

\usepackage{amsmath,amssymb}
%\usepackage{amsthmnoproof}
\usepackage{amsthm}
\usepackage{mathrsfs}
%\let\bibsection\relax
%\usepackage{amsrefs}
%\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
%\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
%\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
\usepackage{xspace}
\usepackage{tabularx,multicol}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\adj}{Adj}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\rec}{rec}
\DeclareMathOperator{\cond}{cond}
\DeclareMathOperator{\disc}{Disc}
\DeclareMathOperator{\row}{row}
\DeclareMathOperator{\col}{col}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\Fq}{\mathbb{F}_q}
\newcommand{\R}{\mathbb R}
\newcommand{\OK}{\mathcal{O}_K}

\newcommand{\calV}{\mathcal{V}}
\newcommand{\ttv}{\texttt{v}\xspace}
\newcommand{\ttw}{\texttt{w}\xspace}
\newcommand{\tttmp}{\texttt{tmp}\xspace}

\newcommand{\rem}{\,\%\,}

\newcommand{\sage}{\textsc{SageMath}\xspace}
\newcommand{\ZpL}{\texttt{ZpL}\xspace}
\newcommand{\ZpLC}{\texttt{ZpLC}\xspace}
\newcommand{\ZpLF}{\texttt{ZpLF}\xspace}

\newcommand{\card}{\text{Card}\:}
\newcommand{\ind}{\text{ind}}
\newcommand{\coind}{\text{coind}}

\newcommand{\famN}{\mathcal{N}}

\newcommand{\llb}{[\mkern-2.5mu[}
\newcommand{\rrb}{]\mkern-2.5mu]}
\newcommand{\llp}{(\mkern-2.5mu(}
\newcommand{\rrp}{)\mkern-2.5mu)}

\newcommand{\calU}{\mathcal{U}}

\newcommand{\softO}{O\tilde{~}}

\newcommand{\inv}{\text{\rm inv}}
\newcommand{\app}{\text{\rm app}}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\newcommand{\done}[1]{\textcolor{blue}{#1}}
\newcommand{\tdo}[1]{\textcolor{red}{#1}}
\definecolor{answer}{rgb}{0,0.5,0.2}
\newcommand{\xavier}[1]{\textcolor{answer}{{\bf Xavier:} #1}}
\newcommand{\tristan}[1]{\textcolor{answer}{{\bf Tristan:} #1}}
\newcommand{\david}[1]{\textcolor{answer}{{\bf David:} #1}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}

\clubpenalty=10000
\widowpenalty = 10000

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rmq}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\fancyhead{}

\begin{document}

\title{\ZpL: a p-adic precision package}

\author{Xavier Caruso}
  \affiliation{Universit\'e Rennes 1; \\
  \institution{IRMAR}
  \city{Rennes, France}
  \postcode{35042}
}
\email{xavier.caruso@normalesup.org}
\author{David Roe}
  \affiliation{University of Pittsburg; \\
  \institution{Department of Mathematics}
  \city{Pittsburgh, PA, USA}
  \postcode{15260}
}
\email{roed@pitt.edu}
\author{Tristan Vaccon}
  \affiliation{Universit\'e de Limoges; \\
  \institution{CNRS, XLIM UMR 7252}
  \city{Limoges, France}  
  \postcode{87060}  
  }
\email{tristan.vaccon@unilim.fr}

\ccsdesc[500]{Computing methodologies~Algebraic algorithms}

\keywords{Algorithms, $p$-adic precision, characteristic polynomial,
eigenvalue}

\begin{abstract}
\end{abstract}

\maketitle

\section{Introduction}

\begin{itemize}
\item Interval arithmetic, floating point arithmetic
\item short introduction of the precision lemma and lattices
\item goal: tracking precision using this theory
\item short presentation of the package
\item plan of the article
\end{itemize}

\todo{Convention: in matrices, vectors are represented by row-vector.}

\section{Examples}

\subsection{Elementary arithmetic}

\begin{tabular}{|l|l|}
\verb?R = Zp(3,5)? & \verb?R = ZpL(3,5)? \\
\verb?x = R(143,5); x? & \verb?x = R(143,5); x? \\
\hfill\verb?...12022? & \hfill\verb?...12022? \\
\verb?3*x? & \verb?3*x? \\
\hfill\verb?...120220? & \hfill\verb?...120220? \\
\verb?x + x + x? & \verb?x + x + x? \\
\hfill\verb?...20220? & \hfill\verb?...120220? \\
\verb?x^3? & \verb?x^3? \\
\hfill\verb?...020222? & \hfill\verb?...020222? \\
\verb?x * x * x? & \verb?x * x * x? \\
\hfill\verb?...20222? & \hfill\verb?...020222? \\
\end{tabular}

\begin{tabular}{|l|l|}
\verb?R = Zp(2,10)? & \verb?R = ZpL(2,10)? \\
\verb?x = R(987,10); x? & \verb?x = R(987,10); x? \\
\hfill\verb?...1111011011? & \hfill\verb?...1111011011? \\
\verb?y = R(21,5); y? & \verb?y = R(21,5); y? \\
\hfill\verb?...10101? & \hfill\verb?...10101? \\
\verb?u = x + y; u? & \verb?u = x + y; u? \\
\hfill\verb?...10000? & \hfill\verb?...10000? \\
\verb?v = x - y; v? & \verb?v = x - y; v? \\
\hfill\verb?...00110? & \hfill\verb?...00110? \\
\verb?u + v? & \verb?u + v? \\
\hfill\verb?...10110? & \hfill\verb?...11110110110? \\
\end{tabular}

\subsection{Linear algebra}

\begin{itemize}
\item determinant
\item characteristic polynomial
\end{itemize}

\subsection{Commutative algebra}

\begin{itemize}
\item gcd of polynomials
\end{itemize}

\section{Presentation of the package}

In this Section, we explain how our package \ZpL works and analyze
its performances.
The main theoretical result on which our package is based in the 
ultrametric precision theory developed in \cite{padicprec}, which 
suggests to track precision \emph{via} lattices and differential 
computations. For this reason, our approach is very inspired by 
Automatic Differentiation techniques \cite{} and our implementation 
follows the usual Operator Overloading strategy \cite{}.

We will introduce two versions of our package, namely \ZpLC and \ZpLF: 
this former is safer while the latter is faster. They are presented in 
\S \ref{ssec:ZpLC} and \S \ref{ssec:ZpLF} respectively. As for \S 
\ref{ssec:preclemma} below, it serves as a short survey on the result of 
\cite{padicprec} on which our developments are built.

\smallskip

\noindent \textit{Remark about the naming.}
%
The letter \texttt{L}, which appears in the name of the package, 
comes from ``lattices''. The letters \texttt{C} (in \ZpLC) and 
\texttt{F} (in \ZpLF) stand for ``cap'' and ``float'' respectively.

\subsection{The precision Lemma}
\label{ssec:preclemma}

In \cite{caruso-roe-vaccon:14a}, we have suggested the 
use of lattices to represent the precision of elements in 
$K$-vector spaces.  It contrasts with the
\emph{coordinate-wise method} (of \textit{e.g.}
 \verb?Zp(5)?) that is used traditionally in \sage and
where the precision of an element is specified by giving the precision
of each coordinate separately and is updated after each basic
operation.

Consider a finite 
dimensional normed vector space $E$ 
defined over $K$. 
We use the notation $\Vert \cdot \Vert_E$ for the norm 
on $E$ and $B^-_E(r)$ (resp. $B^{\phantom -}_E(r)$) for the open (resp. 
closed) ball of radius $r$ centered at the origin. A \emph{lattice} $L \subset 
E$ is a sub-$\O_K$-module which generates $E$ over $K$. 
Because of ultrametricity, the balls $B^{\phantom 
-}_E(r)$ and $B^-_E(r)$ are examples of lattices. 
Lattices can be thought of as special neighborhoods of $0,$
 and therefore are good 
candidates to model precision data. Moreover, as revealed in 
\cite{caruso-roe-vaccon:14a}, they behave quite well under (strictly) 
differentiable maps:

\begin{prop}
\label{prop:precision}
Let $E$ and $F$ be two finite dimensional normed vector spaces over $K$ 
and $f : U \rightarrow F$ be a function defined on an open subset $U$ of 
$E$. We assume that $f$ is differentiable at some point $v_0 \in U$ and 
that the differential $df_{v_0}$ is surjective.
Then, for all $\rho \in (0, 1]$, there exists a positive real
number $\delta$ such that, for all $r \in (0, \delta)$, any lattice
$H$ such that $B^-_E(\rho r) \subset H \subset B^{\phantom -}_E(r)$ 
satisfies:
\begin{equation}
\label{eq:firstorder}
f(v_0 + H) = f(v_0) + df_{v_0} (H).
\end{equation}
\end{prop}

This proposition enables the \emph{lattice method} of tracking precision,
where the precision of the input is specified as a lattice $H$ and precision
is tracked via differentials of the steps within a given algorithm.
The equality sign in Eq.~\eqref{eq:firstorder} shows that this method
yields the optimum possible precision. 
We refer to \cite[\S 4.1]{caruso-roe-vaccon:14a} for a more complete 
exposition.
We emphasize that polynomials, rational fractions (outside of their poles)
and converging power series are strictly differentiable.

The lemma is more precise in the context of so-called
integral polynomial functions.
A function $f : E \to F$ is said to be
\emph{integral polynomial} if it is given by multivariate polynomial 
functions with coefficients in $\O_K$ in any (equivalently all) system 
of coordinates associated to a $\OK$-basis of $B_E(1)$.

\begin{prop}
\label{prop:precision2}
We keep the notation of Proposition \ref{prop:precision} and assume 
in addition that $f$ is integral polynomial. Let $C$ be a positive real
number such that $B_F(1) \subset df_{v_0}(B_E(C))$. 
Then Proposition \ref{prop:precision} holds with $\delta = C \cdot
\rho^{-1}$.
\end{prop}

We will use the following definition to measure the gap
between lattice and coordinate-wise methods. 
Suppose $E$ is equipped with a basis $(e_1, \ldots, e_n)$ and write $\pi_i : E \to Ke_i$ for the projections.

\begin{deftn} \label{def:diffused}
Let $H \subset E$ be a lattice. The number of 
\emph{diffused digits of precision} of $H$ is the length of
$H_0/H$ where $H_0 = \pi_1(H) \oplus \cdots \oplus \pi_n(H)$.
\end{deftn}
If $H$ represents the actual precision on some object, then
$H_0$ is the smallest diagonal lattice containing $H$.
Coordinate-wise methods, with one precision datum per coordinate,
corresponds to diagonal lattices.
Hence, they cannot yield a precision better than $H_0$, and
then $k$ provides a lower bound on the number of $p$-adic digits
gained by lattice methods over standard methods.

\subsection{\ZpLC}
\label{ssec:ZpLC}

\todo{Maybe, add a short intro.}

\medskip

\noindent \textbf{Tracking precision.}
%
In what follows, it will be convenient to use a notion of discrete time 
represented by the letter $t$. Rigorously, it is defined as follows: 
$t=0$ when the $p$-adic ring $\ZpLC(\:\cdots)$ is created and increases by 
$1$ each time a variable is created, deleted\footnote{The deletion can
be explicit (through a call to the \texttt{del} operator) or implicit
(handled by the garbage collector).} or updated.

Let $\calV_t$ be the set of alive variables at time $t$. Set $E_t = 
\Qp^{\calV_t}$; it is a finite dimensional vector space over $\Qp$ which 
should be thought as the set of all possible values that can be taken by 
the variables in $\calV_t$. For $\ttv \in \calV_t$, let $e_\ttv \in
\Qp^{\calV_t}$ be the vector whose all coordinates vanished except 
that at position $\ttv$ which takes the value $1$. The family 
$(e_\ttv)_{\ttv \in \calV_t}$ is obviously a basis of $E_t$; we will 
refer to it as the \emph{canonical basis}.

The package \ZpLC follows precision by keeping track of a lattice 
$H_t$ in $E_t$, which is a unique object whose purpose is to model the 
precision on all the variables in $\calV_t$ together.
Concretely, this lattice is represented by a matrix $M_t$ in row-echelon 
form whose lines form a set of generators.
Below, we explain how the matrices $M_t$ are updated each time
$t$ increases.

\smallskip

\noindent \textit{Creating a variable.}
%
This happens when we encounter an instruction having one of the
two following forms:

\medskip

\noindent \hspace{5mm} \makebox[2.5cm]{[Computation]\hfill\null}
\verb?w = ?$f$\verb?(v_1,? \ldots\verb?, v_n)?

\smallskip

\noindent \hspace{5mm} \makebox[2.5cm]{[New value]\hfill\null}
\verb?w = ?\verb?R(?\text{value}\verb?, ?\text{prec}\verb?)?

\medskip

\noindent
In both cases, $\ttw$ is the newly created variable. 
The $\ttv_i$'s stand for already defined variables and $f$ is some 
$n$-ary builtin function (in most cases it is just addition, 
subtraction, multiplication or division). On the contrary, the terms 
``value'' and ``prec'' refer to user-specified constants or integral 
values which was computed earlier.

\smallskip

Let us first examine the first construction [Computation].
With our conventions, if $t$ is the time just before the execution of 
the instruction we are interested in, the $\ttv_i$'s lie in $\calV_t$ 
while $\ttw$ does not. Moreover $\calV_{t+1} = \calV_t \sqcup \{\ttw\}$, 
so that $E_{t+1} = E_t \oplus \Qp e_\ttw$.
The mapping taking the values of variables at time $t$ to that at 
time $t{+}1$ is:
$$\begin{array}{rcl}
F: \quad E_t & \longrightarrow & E_{t+1} \smallskip \\
\underline x & \mapsto & \underline x \:\oplus f(x_1, \ldots, x_n)
\end{array}$$
where $x_i$ is the $\ttv_i$-th coordinate of the vector $\underline x$.
The Jacobian matrix of $F$ at $\underline x$ is easily computed; it 
is the block matrix $J_{\underline x}(F) = \big( \begin{matrix} I & L
\end{matrix} \big)$ where $I$ is the identity matrix of size $\text{Card}
\:\calV_t$ and $L$ is the column vector whose $\ttv$-th entry is
$\frac{\partial f}{\partial \ttv} (\underline x)$ if $\ttv$ is one of 
the $\ttv_i$'s and $0$ otherwise.
Therefore, the image of $H_t$ under $dF_{\underline x}$ is represented 
by the matrix $J_{\underline x}(F) \cdot M_t = \big( \begin{matrix} M_t & C
\end{matrix} \big)$ where $C$ is the column vector:
$$C = \sum_{i=1}^n \frac{\partial f}{\partial{\ttv_i}} (\underline x)
\cdot C_i$$
where $C_i$ is the column vector of $M_t$ corresponding to the
variable $\ttv_i$.
Observe that the matrix $J_{\underline x}(F) \cdot M_t$ is no longer a 
square matrix; it has one extra column. This reflects the fact that 
$\dim E_{t+1} = \dim E_t + 1$. Rephasing this in a different language, 
the image of $H_t$ under $dF_{\underline x}$ is no longer a lattice in 
$E_{t+1}$ but is included in an hyperplane.

The package \ZpLC tackles this issue by introducing a cap: we do not 
work with $dF_{\underline x}(H_t)$ but instead define the lattice: 
$$H_{t+1} = dF_{\underline x}(H_t) \oplus p^{N_{t+1}} \Zp e_\ttw$$
where $N_{t+1}$ is an integer, the so-called \emph{cap}.
Alternatively, one may introduce the application:
$$\begin{array}{rcl}
\tilde F: \quad E_t \oplus \Qp & \longrightarrow & E_{t+1} \smallskip \\
\underline x \oplus c & \mapsto & \underline x \:\oplus 
\big(f(x_1, \ldots, x_n) + c\big).
\end{array}$$
The lattice $H_{t+1}$ is then the image of $H_t \oplus p^{N_{t+1}}
\Zp$ under $d\tilde F_{(\underline x, \star)}$ for any value of $\star$.
The choice of the 
cap is of course a sensitive question. \ZpLC proceeds as follows. When a 
ring is created, it comes with two constants (which can be specified by 
the user): a relative cap \textsc{relcap} and an absolute cap 
\textsc{abscap}. With these predefined values, the chosen cap is:
$$N_{t+1} = 
\min\big(\textsc{abscap},\, \textsc{relcap} + v_p(y)\big)$$
with $y = f(x_1, \ldots, x_n)$.

In concrete terms, the lattice $H_{t+1}$ is represented by the block 
matrix:
$$\left(\begin{matrix}
M_t & C \smallskip \\ 0 & p^{N_{t+1}}
\end{matrix}\right).$$
Performing row operations, we see then the entries of $C$ can be
reduced modulo $p^{N_{t+1}}$ without changing the lattice. In order
to optimize the size of the objects, we perform this reduction and
define $M_{t+1}$ by:
$$M_{t+1} = \left(\begin{matrix}
M_t & C \text{ mod } p^{N_{t+1}} \smallskip \\ 0 & p^{N_{t+1}}
\end{matrix}\right).$$
We observe in particular that $M_{t+1}$ is still in row-echelon form.

\smallskip

Finally, we need to precise which value is set to the newly created 
variable $\ttw$. We observe that it cannot be exactly $f(x_1, \ldots, 
x_n)$ because the latter is \emph{a priori} a $p$-adic number which 
cannot be computed exactly. For this reason, we have to truncate it at 
some finite precision. Again we choose the precision $O(p^{N_{t+1}})$,
\emph{i.e.} we define $x_\ttw$ as $f(x_1, \ldots, x_n) \text{ mod } 
p^{N_{t+1}}$. This makes sense because:
$$\bar x \oplus f(x_1, \ldots, x_n) \equiv
\bar x \oplus x_\ttw \pmod{H_{t+1}}$$
thanks to the extra generator we have added.

\medskip

\noindent \textit{Deleting a variable.}
%
Let us now examine the case where a variable $\ttw$ is deleted (or 
collected by the garbage collector). Just after the deletion, at
time $t{+}1$, we then have $\calV_{t+1} = \calV_t \backslash \{\ttw\}$.
Thus $E_t = E_{t+1} \oplus \Qp e_\ttw$. Moreover, the deletion of $\ttw$
is modeled by the canonical projection $f : E_t \to E_{t+1}$. Since $f$
is linear, it is its own differential (at each point) and we set 
$H_{t+1} = f(H_t)$.

A matrix representing $H_{t+1}$ is deduced from $M_t$ by erasing the
column corresponding to $\ttw$. However the matrix we get this way
has the following shape:
$$\left(\raisebox{-0.5\height}{\begin{tikzpicture}[scale=0.3]
\fill[black!20] (0,0)--(0,-1)--(1,-1)--(1,-2)--(2,-2)--(2,-3)
    --(3,-3)--(3,-5)--(4,-5)--(4,-6)--(5,-6)--(5,-7)--(6,-7)--(6,0)--cycle;
\draw (0,0)--(0,-1)--(1,-1)--(1,-2)--(2,-2)--(2,-3)
    --(3,-3)--(3,-5)--(4,-5)--(4,-6)--(5,-6)--(5,-7)--(6,-7);
\draw[thin] (6,-7)--(6,0)--(0,0);
\end{tikzpicture}}\right)$$
where a cell is colored when it can contain a non-vanishing entry.
At this point, we encounter an issue: the above matrix is no longer
echelonized (except the $\ttw$ corresponded to the last column, 
\emph{i.e.} was the latest created variable which was still alive). 
We then need to re-echelonize it before proceeding to the next
instruction. However, we notice that only the left bottom corner
needs to be echelonized. We will come back to this observation later
when we will study complexity.

\medskip

\noindent \textit{Updating a variable.}
%
Just like for creation, this happens when the program reaches an
affectation \verb?w = ...? where the variable \ttw is already 
defined.

This situation can actually be decomposed into three steps: first, we 
compute the right-hand-side and store it in a temporary variable \tttmp; 
second, we delete \ttw; third, we rename \tttmp in \ttw. The first step 
corresponds to the creation of a variable and was then already studied. 
Similarly, the second step is a variable deletion and again has already 
been discussed. As for the third step, it just changes the name of an
element of $\calV_t$ but does not affect the lattice $H_t$, nor the
matrix $M_t$.

\medskip

\noindent \textbf{Complexity.}
%
We now briefly discuss the cost of the above operations. For this,
it is convenient to introduce a total order on $\calV_t$: for $\ttv, 
\ttw \in \calV_t$, we convene that $\ttv <_t \ttw$ is \ttv was created 
before~\ttw. By construction, the columns of the matrix $M_t$ are 
ordered with respect to $<_t$.
For $\ttv \in \calV_t$, we define the \emph{index} of \ttv, denoted by 
$\ind_t(\ttv)$ as the number of elements of $\calV_t$ which are not 
greater than \ttv. If we sort the elements of $\calV_t$ by increasing 
order, \ttv then appears in $\ind_t(\ttv)$-th position.
We also define the \emph{coindex} of \ttv by
$\coind_t(\ttv) = \card \calV_t - \ind_t(\ttv)$.


\medskip

\noindent \textit{Creating a variable.}

$$O\bigg(\sum_{i=1}^n \ind_t(\ttv_i)\bigg) \subset 
O\big(n \cdot \card \calV_t\big)$$

\medskip

\noindent \textit{Deleting a variable.}

$$O\big(\coind_t(\ttw)^2\big)$$

\medskip

\noindent \textit{Temporal locality.}

\medskip

\noindent \textbf{Correctness.}


\subsection{\ZpLF}
\label{ssec:ZpLF}

we work with $\Zp$-submodules of possible positive codimension
and represent them using matrices of floating point $p$-adics.

\subsubsection*{Tracking precision}

\subsubsection*{Complexity}

\subsection{More examples}

\begin{tabular}{|l|l|}
\verb?R = Zp(2)? & \verb?R = ZpL(2)? \\
\multicolumn{2}{|l|}{\tt x, y, z, t = R(1,15), R(1,15), R(1,15), R(3,15)} \\
\multicolumn{2}{|l|}{\tt for \_ in range(20):} \\
\multicolumn{2}{|l|}{\tt \ \ \ \ x, y, z, t = y, z, t, (y*t + z*z)/x} \\
\multicolumn{2}{|l|}{\tt \ \ \ \ print t} \\
\hfill\verb?...000000000000100? & \hfill\verb?...000000000000100? \\
\hfill\verb?...000000000001101? & \hfill\verb?...000000000001101? \\
\hfill\verb?...000000000110111? & \hfill\verb?...000000000110111? \\
\hfill\verb?...101010111010111? & \hfill\verb?...101010111010111? \\
\hfill\verb?...1100111101111? & \hfill\verb?...101100111101111? \\
\hfill\verb?...1110000010010? & \hfill\verb?...111110000010010? \\
\hfill\verb?...0001000111001? & \hfill\verb?...100001000111001? \\
\hfill\verb?...0000011111101? & \hfill\verb?...100000011111101? \\
\hfill\verb?...1000000110101? & \hfill\verb?...001000000110101? \\
\hfill\verb?...101101010011? & \hfill\verb?...010101101010011? \\
\hfill\verb?...110000000000? & \hfill\verb?...001110000000000? \\
\hfill\verb?...000101011101? & \hfill\verb?...111000101011101? \\
\hfill\verb?...001001101011? & \hfill\verb?...111001001101011? \\
\hfill\verb?...000011110011? & \hfill\verb?...111000011110011? \\
\hfill\verb?...11? & \hfill\verb?...0000000111? \\
\hfill\verb?...10? & \hfill\verb?...1010001110? \\
\hfill\verb?...01? & \hfill\verb?...1110110001? \\
\hfill\verb?...01? & \hfill\verb?...011110111001? \\
\hfill\verb?...01? & \hfill\verb?...100101101001001? \\
\hfill\verb?...1? & \hfill\verb?...11111000011? \\
\end{tabular}

\begin{itemize}
\item SOMOS sequence
\item many multiplications of matrices
\item Dodgson algorithm
\item subresultants
\item Grobner basis
\item $p$-adic differential equations
\end{itemize}

\subsubsection{$p$-adic differential equations}


In \cite{LV16}, the behaviour of the precision when solving
$p$-adic differential equations with separation of 
variables has been studied.
The authors have investigated the gap when that appears
when applying a Newton-method solver between
the theoretic loss in precision and the 
actual loss in precision for a naive implemenationin \verb?Zp(p)?.
We can reach this theoretical loss in precision using \verb?ZpL?.
We use a generic \verb?Newton_Iteration_Solver(g,h,N)?
that applies \verb?N? steps of the Newton method for 
$y'=g \times h(y)$ as described in \citep{LV16}. 

\begin{tabular}{|l|l|}
\verb?R = Qp(2,250)? & \verb?R = QpL(2,250)? \\
\verb?S = Zp(2,250)? & \verb?S = ZpL(2,250)? \\
\multicolumn{2}{|l|}{\tt A.<x> = PowerSeriesRing(R,32) } \\
\multicolumn{2}{|l|}{\tt B.<x> = PowerSeriesRing(S,32)) } \\
\multicolumn{2}{|l|}{\tt h,y =1+x+x\^{}3 , x+x\^{}2*A(B.random\_element()) } \\
\multicolumn{2}{|l|}{\tt g=y.derivative()/h(y) } \\
\multicolumn{2}{|l|}{\tt s=Newton\_Iteration\_Solver(g,h,5)} \\
\multicolumn{2}{|l|}{\tt print 250-(s[31].lift()-y[31].lift()).valuation(2)} \\
\hfill\verb?9? & \hfill\verb?4? \\
\end{tabular}
The last line provides the loss in precision between the
original solution of the differential equation and its approximation
obtained through computation.
The theorical loss in precision for the coefficient of $t^{31}$
is $\lfloor log_2 (31)\rfloor=4$ and is then reached, whereas the
loss in precision for a naive implementation is clearly more severe.


\section{Conclusion}

\begin{itemize}
\item In which case should we use this package? Mainly for exploration
\item example of $p$-adic differential equations (especially for $p=2$)
\end{itemize}

\subsection{Exploration}

The purpose of this package is that of exploration
of the $p$-adic precision.
We conclude with a last example coming
from the field of the computation of
isogenies between elliptic curves over finite fields.
Techniques to compute these isogenies
through the solving of $p$-adic differential equations
have a long history, and a first
analysis of the behaviour of precision using
lattices has been investigated in \cite{LV16}.

In that article, the authors were interested in
solving $y'^2=g \times h(y)$ with
$g,h,y \in \mathbb{Z}_p \llbracket x \rrbracket$
such that $g(0)=h(0)=1$ and $y(0)=0.$

The main result was that the intrinsic loss in precision
when computing the coefficient $x^n$ of $y$
from $g$ and $h$ was in $\log_p(n)$
even though a naive analysis of
the Newton method for solving the equation
yield a loss in $\log_p(n)^2.$

However, this does not encompass the case of 
elliptic curves defined and isogeneous 
over $\mathbb{Q},$ which can yield a differential 
equation with a solution $y$ whose coefficients
have negative valuation.

For instance, the curves 
$E: \: y^2=x^3+1$
and $E': \: y^2=+ (m^2-1)x^2/4 + m^6$ are isogeneous
over $\mathbb{Q}$ if  $m$ is an odd integer.
We take $m=67.$ The corresponding isogeny has coefficients with possibly
negative $3$-adic valuation.

We can solve numerically 
the corresponding differential equation
using the algorithm of \cite{Lercier-Sirvent:08}
over $\mathbb{Q}_3$ with lattice precision.
We use an initial precision of 50 digits.
Any generic implementation of the Newton
method of \cite{Lercier-Sirvent:08} is enough.

\begin{tabular}{|l|}
\verb?    R = QpL(3,50)? \\
\verb?    A.<x> = PolynomialRing(R)?\\
\verb?    m=67?\\
\verb?    g=1+(1/4)*x^2+x^6?\\
\verb?    h=1+((m^2)/4)*x^2+m^6* x^6?\\
\verb?    y=Lercier_Sirvent(g,h,64)?\\
\verb?    y[31]?\\
\verb?(467995903278061284942799/3^14 + O(3^36))*x^31?\\
\end{tabular}

We can then see that up to $63,$ the behaviour
of the loss in relative precision is the same
as for the coefficient in $x^{31}$: \textbf{no loss
occured}.
Thus, those simple manipulations
hint at a different behaviour for the loss 
in precision in at least some special cases of
of isogenies defined over $\mathbb{Q}$
but not necessarily over $\mathbb{Z}_p.$ 

\bibliographystyle{plain}
\bibliography{ZpL}


\end{document}

