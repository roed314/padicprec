\documentclass{sig-alternate-2013}

\setlength{\paperheight}{11in}
\setlength{\paperwidth}{8.5in}

\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb}
\usepackage{amsthmnoproof}
\usepackage{amsrefs}
\usepackage[usenames,dvipsnames]{color}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage[algoruled,vlined,english,linesnumbered]{algorithm2e}
\usepackage[pdfpagelabels,colorlinks=true,citecolor=blue]{hyperref}
\usepackage{comment}
\usepackage{multirow}
\usepackage{tikz}

\newcommand{\noopsort}[1]{}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\HP}{HP}
\DeclareMathOperator{\PP}{PP}
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\val}{val}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\com}{Com}
\DeclareMathOperator{\Grass}{Grass}
\DeclareMathOperator{\Lat}{Lat}
\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\rank}{rank}

\newcommand{\slp}{\text{\rm slp}}
\newcommand{\RS}{\text{\rm RS}}

\newcommand{\N}{\mathbb N}
\newcommand{\Z}{\mathbb Z}
\newcommand{\Zp}{\Z_p}
\newcommand{\Q}{\mathbb Q}
\newcommand{\Qp}{\Q_p}
\newcommand{\Fp}{\mathbb{F}_p}
\newcommand{\R}{\mathbb R}
\renewcommand{\O}{\mathcal O}
\newcommand{\OK}{\mathcal{O}_K}
\newcommand{\XX}{\mathbf X}
\newcommand{\trans}{{}^{\text t}}
\newcommand{\T}{\mathcal{T}}

\renewcommand{\prec}{\text{\rm prec}}
\newcommand{\NF}{\text{\rm NF}}

\newcommand{\id}{\textrm{id}}
\newcommand{\Epi}{\textrm{Epi}}
\renewcommand{\c}{\text{\rm c}}

\newcommand{\detp}{\det{'}}
\newcommand{\low}{\text{\rm low}}
\newcommand{\up}{\text{\rm up}}
\newcommand{\DI}{\text{\rm DI}}
\newcommand{\II}{\text{\rm II}}
\DeclareMathOperator{\charpoly}{char}
\newcommand{\charp}{\charpoly'}

\newcommand{\lb}{\ensuremath{\llbracket}}
\newcommand{\rb}{\ensuremath{\rrbracket}}
\newcommand{\lp}{(\!(}
\newcommand{\rp}{)\!)}
\newcommand{\col}{\: : \:}

\def\todo#1{\ \!\!{\color{red} #1}}
\definecolor{purple}{rgb}{0.6,0,0.6}
\def\todofor#1#2{\ \!\!{\color{purple} {\bf #1}: #2}}

\def\binom#1#2{\Big(\begin{array}{cc} #1 \\ #2 \end{array}\Big)}


\permission{%
}



\begin{document}

\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{quest}[theo]{Question}
\newtheorem{conj}[theo]{Conjecture}
\theoremstyle{definition}
\newtheorem{rem}[theo]{Remark}
\newtheorem{ex}[theo]{Example}
\newtheorem{deftn}[theo]{Definition}

\title{Multiplication, division and factorization\\of p-adic polynomials}

\numberofauthors{3}
\author{
\alignauthor Xavier Caruso\\
  \affaddr{Universit\'e Rennes 1}\\
  \affaddr{\textsf{xavier.caruso@normalesup.org}}
\alignauthor David Roe \\
  \affaddr{University of Pittsburgh}\\
  \affaddr{\textsf{roed.math@gmail.com}}
\alignauthor Tristan Vaccon\\
  \affaddr{Rikkyo University}\\
  \affaddr{\textsf{vaccon@rikkyo.ac.jp}}
}

\maketitle

\begin{abstract}
\end{abstract}

\category{I.1.2}{Computing Methodologies}{Symbolic and Algebraic Manipulation -- \emph{Algebraic Algorithms}}
\terms{Algorithms, Theory}
\keywords{}

%\vspace{1mm}
% \noindent
% {\bf Categories and Subject Descripto\RS:} \\
%\noindent I.1.2 [{\bf Computing Methodologies}]:{~} Symbolic and Algebraic
%  Manipulation -- \emph{Algebraic Algorithms}
%
% \vspace{1mm}
% \noindent
% {\bf General Terms:} Algorithms, Theory
%
% \vspace{1mm}
% \noindent
% {\bf Keywords:} $p$-adic precision, linear algebra, ultrametric analysis
%\medskip

\section{Introduction}

\noindent
{\bf Notations.}
Throughout this paper, we fix a complete discrete valuation field $K$; 
we denote by $\val : K \to \Z \cup \{+\infty\}$ the valuation on it and 
by $W$ its ring of integers (\emph{i.e.} the set of elements with 
nonnegative valuation). We assume that $\val$ is normalized so that it
is surjective and denote by $\pi$ a uniformizer of $K$, that is an 
element of valuation $1$. Denoting by $S \subset W$ a fixed set of 
representatives of the classes modulo $\pi$ and assuming $0 \in S$, 
one can prove that each element in $x \in K$ can be represented 
uniquely as a convergent series:
\begin{equation}
\label{eq:CDVFseries}
x = \sum_{i=\val(x)}^{+\infty} a_i \pi^i
\quad \text{with} \quad a_i \in S.
\end{equation}
The two most important examples are the field of $p$-adic numbers $K = 
\Qp$ and the field of Laurent series $K = k((t))$ over a field $k$. The 
valuation on them are the $p$-adic valuation and the usual valuation of 
a Laurent series respectively. Their ring of integers are therefore 
$\Zp$ and $k[[t]]$ respectively. A distinguished uniformizer is $p$ 
and $t$ whereas a possible set $S$ is $\{0, \ldots, p-1\}$ and $k$
respectively.
The reader who is not familiar with complete discrete valuation fields
may assume (without sacrifying too much to the generality) that $K$ is
one of the two aforementioned examples.

In what follows, the notation $K[X]$ refers to the ring of univariate 
polynomials with coefficients in $K$. The subspace of polynomials of 
degree at most $n$ (resp. exactly $n$) is denoted by $K_{\leq n}[X]$ 
(resp. $K_{=n}[X]$).

\section{Precision data}

Elements in $K$ (and \emph{a fortiori} in $K[X]$) carry an infinite 
amount of information. They thus cannot be stored entirely in the
memory of a computer and have to be truncated. Elements of $K$ are
usually represented by truncating Eq.\eqref{eq:CDVFseries} as
follows:
\begin{equation}
\label{eq:CDVFseriesO}
x = \sum_{i=v}^{N-1} a_i \pi^i + O(\pi^N)
\end{equation}
where $N$ is an integer called the \emph{absolute precision} and 
the notation $O(\pi^N)$ means that the coefficients $a_i$ for $i
\geq N$ are discarded. If $N > v$ and $a_v \neq 0$, the integer $v$ 
is the valuation of $x$ and the difference $N-v$ is called the
\emph{relative precision}.
Alternatively, one may thing that the writing~\eqref{eq:CDVFseriesO}
represents a subset of $K$ which consists of all elements in $K$ for
which the $a_i$'s in the range $[v,N-1]$ are those specified. From the
metric point of view, this is a ball (centered at any point inside it).

It worths be noting that tracking precision using this representation is 
rather easy. For example, if $x$ and $y$ are known with absolute (resp. 
relative) precision $N_x$ and $N_y$ respectively, one can compute the 
sum $x+y$ (resp. the product $xy$) at absolute (resp. relative) 
precision $\min(N_x,N_y)$. Computations with $p$-adic and Laurent
series are often handle this way on symbolic computation softwares.

\subsection{Precision for polynomials}

The situation is much more subtle when we are working with a collection 
of elements of $K$ (\emph{e.g.} a polynomial) and not just a single one.
Indeed, several precision data may be considered and, as we shall see
later, each of them has is own interest. Below we detail three of them
for the special case of polynomials.

\medskip

\noindent
{\bf Jagged precision.}
The most naive precision we may consider is the so-called \emph{jagged 
precision}. It consists in attaching an individual precision (of the
shape $O(\pi^{N_i})$) to each coefficient separatedly.

\medskip

\noindent
{\bf Newton precision.} 
We now move to \emph{Newton precision} data. They can be actually seen 
as particular instances of jagged precision but there exist for them 
better representations and better algorithms.

\begin{deftn}
A \emph{Newton function of degree $n$} is a convex function 
$\varphi : [0,n] \to \R$ which is piecewise affine and whose epigraph
$\Epi(\varphi)$ have integral extremal points.
\end{deftn}

\begin{rem}
The datum of $\varphi$ is equivalent to that of $\Epi(\varphi)$ and they 
can easily represented and manipulated on a computer.
\end{rem}

We recall that one can attach a Newton function to each polynomial.
If $P(X) = \sum_{i=0}^n a_n X^n \in K_n[X]$, we define its Newton
polygon $\NP(P)$ as the convex hull of the points $(i,\val(a_i))$ 
($1 \leq i \leq n$) together with the point at infinity $(0,+\infty)$
and then its Newton function $\NF(P) : [0,n] \to \R$ as the unique
function whose epigraph is $\NP(P)$. It is well known \todo{(Give a
reference?)} that:
$$\begin{array}{r@{\,\,}c@{\,\,}l}
\NP(P+Q) & \subset & \text{Conv}\big(\NP(P) \cup \NP(Q)\big) \smallskip \\
\NP(PQ) & = & \NP(P) + \NP(Q)
\end{array}$$
where $\text{Conv}$ denotes the convex hull and the plus sign stands 
for the Minkowski sum. This translates to:
$$\begin{array}{r@{\,\,}c@{\,\,}l}
\NF(P+Q) \geq \NF(P) \oplus \NF(Q) \smallskip \\
\NF(PQ) = \NF(P) \odot \NF(Q)
\end{array}$$
where the operations $\oplus$ and $\odot$ are defined accordingly.
It turns out that there exist efficient algorithms for computing these 
operations.

In a similar fashion, Newton functions can be used to model precision: 
given a Newton function $\varphi$ of degree $n$, we agree that a polynomial 
of degree at most $n$ is given at precision $O(\varphi)$ when, for all $i$,
its $i$-th coefficient is given at precision $O\big(\pi^{\lceil \varphi(i)
\rceil}\big)$ (where $\lceil \cdot \rceil$ is the ceiling function).
In the sequel, we shall write
$O(\varphi) = \sum_{i=0}^n O\big(\pi^{\lceil \varphi(i) \rceil}\big) \cdot X^i$
and use the notation $\sum_{i=0}^n a_i X^i + O(\varphi)$ (where the
coefficients $a_i$ are given by truncated series) to refer to a 
polynomial given at precision $O(\varphi)$.

It is easily checked that if $P$ and $Q$ are two polynomials known at 
precision $O(\varphi_P)$ and $O(\varphi_Q)$ respectively, then $P+Q$ is 
known at precision $O(\varphi_P \oplus \varphi_Q)$ and $PQ$ is known at 
precision $O\big(\varphi_P \odot \NF(Q) \oplus \NF(P) \odot 
\varphi_Q\big)$.

\medskip

\noindent
{\bf Lattice precision.}
The notion of \emph{lattice precision} was developed in 
\cite{padicprec}. It encompasses the two previous models and has
the decisive advantage of optimality. As a counterpart, it might be
very space-consuming and time-consuming for polynomials of large
degree.

\begin{deftn}
Let $V$ be a finite dimensional vector space over $K$. A lattice
in $V$ is a sub-$W$-module of $V$ generated by a $K$-basis of
$V$.
\end{deftn}

\noindent
We fix an integer $n$. A lattice precision datum for a polynomial of 
degree $n$ is a lattice $H$ lying in the vector space $K_{\leq n}[X]$. 
We shall sometimes denote it $O(H)$ in order to emphasize that it should 
be considered as a precision datum. The notation $P(X) + O(H)$ then 
refers to any polynomial in the $W$-affine space $P(X) + H$. Tracking
lattice precision can be done using differentials as shown in
\cite[Lemma~3.4 and Proposition~3.12]{padicprec}: if $f : K_{\leq n}[X] 
\to K_{\leq m}[X]$ denotes any strictly differentiable function with
surjective differential, under mild assumption on $H$, we have:
$$f(P(X)+H) = f(P(X)) + f'(P(X))(H)$$
where $f'(P(X))$ denotes the differential of $f$ at $P(X)$. The
equality sign reflets the optimality of the method.

As already mentioned, the jagged precision model is a particular case
of the lattice precision. Indeed, a precision of the shape
$\sum_{i=0}^n O(\pi^{N_i}) X^i$ corresponds to the lattice
generated by the elements $\pi^{N_i} X^i$ ($0 \leq i \leq n$).

\section{Euclidean division}

\begin{theo} \label{theo:EDivisionNP}
Here should be the theorem for the Newton polygons during a Euclidean division.
\end{theo}

\section{Modular multiplication}

\section{Slope factorization}
\subsection{Main theorem}

\begin{theo} \label{theo:slope-factor}
Under some assumptions... $\textrm{coeff}(P,d)=1,$ $\NP (A_0)' <0$ (???)

\begin{align*}
A_0 &= \textrm{trunc}(P,d), \\
B_{0} &= P \div A_{0}, \\
V_{0} &= 1.
\end{align*}


\begin{align*}
A_{i+1} &= A_i + (V_i P \% A_i), \\
B_{i+1} &= P \div A_{i+1}, \\
V_{i+1} &= (2 V_i -V_i^2 B_{i+1} ) \% A_{i+1}.
\end{align*}
$A_i$ convergences to $A_\infty$ such that $\NP (A_\infty)= \NP (A_0)$ and $A_\infty$ divides $P.$
\end{theo}

\begin{rem}
Using $P \mapsto P^*=X^{deg(P)}P \left( \frac{1}{X} \right), $ the assumptions in Theorem \ref{theo:slope-factor} do not induce any loss in generality.
\end{rem}

\begin{rem}
Non necessity to have $A_0$ as the first slope. It is possible to use minorations.

\begin{tikzpicture}
\draw[->] (0,-1) -- (0,2);


\draw[->] (-0.05,0) -- (4,0);

\node[left] at (0,0) {$0$};

\draw (1.5,-.1) -- (1.5,.1);
\node[above] at (1.5,0) {$d$};


\draw (0,1)-- (1.5,0) -- (3,-.375)-- (3.5,.625);
\draw[green] (0,1.5)-- (1,0.33)-- (1.5,0) --(2,-.125)-- (3,0)-- (3.5,1);

\node[below] at (.75,-0.2) {$\lambda_0$};
\end{tikzpicture}
\end{rem}

\subsection{Useful formulae}

\begin{lem} \label{lem:formulae}
Some definitions :
\begin{align}
R_i &= A_{i+1} -A_i, \label{eqdef:Ri}\\
S_i &= P \% A_i, \label{eqdef:Si} \\
Q_i &= V_i P \div A_i. \label{eqdef:Qi}. 
\end{align}
Some formulae :
\begin{align}
B_i-B_{i+1} &= R_i B_{i+1} \div A_i, \label{eqdef:Biminus} \\
S_i-S_{i+1} &= R_i B_{i+1} \div A_i,  \label{eqdef:Siminus} \\
S_{i} &= (B_i R_i + (1-V_i B_i) S_{i-1}+(1-V_i B_i)(S_i-S_{i-1})) \% A_{i}, \label{eqdef:Si2}  \\
V_{i+1}-V_i &= (V_i (1-V_i B_i)+V_i^2(B_i-B_{i+1})) \% A_{i}, \label{eqdef:Viminus}\\
1-Q_i &= (1-V_i B_i)+((R_i-V_i S_i) \div A_i), \label{eqdef:Qiminus}\\
R_{i+1} &= ((V_{i+1}-V_i)S_{i+1}+(1-Q_i)R_i) \% A_{i+1}, \label{eq:Riplus}\\
1-V_{i+1}B_{i+1} &= ((1-V_i B_i)+V_i (B_i-B_{i+1}))^2. \label{eq:ViBiplus}
\end{align}
\end{lem}
\begin{proof}
We first remark that by definition,
\begin{align*}
P &= A_i B_i + S_i, \\
P V_i&= A_i Q_i + R_i.
\end{align*}
Since $P= A_i B_i + S_i= A_{i+1} B_{i+1} + S_{i+1},$ we have \[R_i B_{i+1}= (B_i -B_{i+1})A_i + (S_i+S_{i+1}).\]
Hence, by consideration of degree, what we have written is the Euclidean division of $R_i B_{i+1}$ by $A_i$ and we obtain \eqref{eqdef:Biminus} and \eqref{eqdef:Siminus}.

From $V_i P =  A_i Q_i + R_i=V_i (A_i B_i + S_i),$ we get \begin{equation}(V_i B_i -Q_i) A_i=R_i-V_i S_i. \label{eq:ViBiQi} \end{equation} Thus, \[ R_i=V_i S_i \% A_i.\]
Hence, $B_i R_i =S_i (V_iB_i-1)+S_i \% A_i,$ and $S_i=B_i R_i-S_i (V_iB_i-1) \% A_i.$ \eqref{eqdef:Si2} follows directly.

By definition of $V_i,$ we get $V_{i+1}-V_i=V_i (1-V_i B_{i+1}) \% A_{i+1}.$ \eqref{eqdef:Viminus} follows immediately.

We can write $1-Q_i=(1-V_i B_i)+(V_i B_i - Q_i).$ Using \eqref{eq:ViBiQi}, we get \eqref{eqdef:Qiminus}.

We have $V_i P=A_i Q_i+R_i=(A_{i+1}-R_i)Q_i+R_i,$ and $V_{i+1} P=A_{i+1} Q_{i+1}+R_{i+1}.$ As a consequence, 
\begin{align*}
R_{i+1} &= (V_{i+1}-V_i)P+(1-Q_i)R_i \\
 &= (V_{i+1}-V_i)S_{i+1}+(1-Q_i)R_i) \% A_{i+1},
\end{align*} and \eqref{eq:Riplus} is proved.

Finally, \begin{align*}
1-V_{i+1}B_{i+1} &= 1-2V_i B_{i+1}+V_i^2 B_{i+1}^2 \% A_{i+1}, \\
&= (1-V_i B_{i+1})^2 \% A_{i+1}, \\
&= ((1-V_i B_i)+V_i (B_i-B_{i+1}))^2 \% A_{i+1},
\end{align*} which concludes the proof.
\end{proof}

\begin{deftn}
Notation:
\[P=-d \lambda_0+d \slp(\lambda_0) + l_1 \slp (\lambda_1)+\RS \]
($\RS$ for remaining slopes)

\begin{tikzpicture}
\draw[->] (0,-1) -- (0,2);


\draw[->] (-0.05,0) -- (4,0);

\node[left] at (0,0) {$0$};

\draw (1.5,-.1) -- (1.5,.1);
\node[above] at (1.5,0) {$d$};

\draw (3,-.1) -- (3,.1);
\node[above] at (2.8,0) {$d+l_1$};

\node[left] at (0,1.5) {$-d \lambda_0$};

\draw (0,1.5)-- (1.5,0) -- (3,-.375)-- (3.5,.625);
\node[below] at (.75,0.5) {$\lambda_0$};
\node[below] at (2.25,-0.25) {$\lambda_1$};
\node[right] at (3.5,0.25) {$\RS$};
\end{tikzpicture}
\end{deftn}

\subsection{Proof of the Main Theorem}

\subsubsection{Initial Newton polygons}

\begin{lem}
$\NP(R_0)=\NP(S_0)$:
\[ \NP (R_0)=\NP(S_0)=(\lambda_1-\lambda_0)- d \lambda_0+(d-1)\slp( \lambda_0). \]

\begin{tikzpicture}
\draw[->] (0,-.5) -- (0,2.5);


\draw[->] (-0.05,0) -- (2,0);

\draw[->] (-0.05,1.75) -- (.05,1.75);
\node[left] at (0,1.75) {$(\lambda_1-\lambda_0)$};
\node[left] at (-.4,1.4) {$-d \lambda_0$};



\node[left] at (0,0) {$0$};


\draw (1,-.1) -- (1,.1);
\node[below, right] at (1,-.2) {$d-1$};


\draw (0,1.75) -- (1,.75) -- (1,2.5);
\node[below] at (.5,1.3) {$\lambda_0$};
\end{tikzpicture}

$\NP (1-V_0B_0 \% A_0)$:
\begin{align*}
\NP ((1-V_0B_0)\% A_0) &\geq (\lambda_1-\lambda_0)+1\slp( \lambda_0)+(d-1) \slp(\lambda_1), \\
&\geq (\lambda_1-\lambda_0)+d\slp( \lambda_0). \\
\end{align*} 
\begin{tikzpicture}
\draw[->] (0,-1) -- (0,1);


\draw[->] (-0.05,0) -- (2.5,0);

\node[left] at (0,0) {$0$};


\node[above] at (1.5,0) {$d-1$};

\draw (.5,-.1) -- (.5,.1);
\node[above] at (.5,0) {$1$};

\draw (-.1,.25) -- (.1,.25);
\node[left] at (0,.25) {$(\lambda_1-\lambda_0)$};

\draw (0,.25) -- (.5,-.25) -- (1,-.375)-- (1,1.5);
\draw (0,.25) -- (.5,-.25) -- (1,-.75)-- (1,1.5);



\node[below] at (.25,-0.1) {$\lambda_0$};
\node[below] at (.75,0.1) {$\lambda_1$};
\end{tikzpicture}


\label{lem:initial_NP}
\end{lem}
\begin{proof}
Clear by Theorem \ref{theo:EDivisionNP}, and the fact that $coeff(P,d)=1.$
\end{proof}


\subsubsection{Unchanged Newton polygons}




\begin{lem} \label{lem_unchanged_NP}
$\NP (V_i)$ for $i>0$ ($V_0=1$):
\[\NP(V_i)=0+(d-1)\slp( \lambda_1). \]

\begin{tikzpicture}
\draw[->] (0,-1) -- (0,1);


\draw[->] (-0.05,0) -- (2,0);

\node[left] at (0,0) {$0$};

\node[below, right] at (1,-.2) {$d-1$};


\draw (0,0) -- (1,-.25) -- (1,1);
\node[below] at (.5,-0.25) {$\lambda_1$};
\end{tikzpicture}

$\NP (B_i)$:
\[\NP(B_i)=0+(l_1)\slp( \lambda_1)+\RS. \]

\begin{tikzpicture}
\draw[->] (0,-1) -- (0,1);


\draw[->] (-0.05,0) -- (2.5,0);

\node[left] at (0,0) {$0$};

\draw (1.5,-.1) -- (1.5,.1);
\node[above] at (1.5,0) {$l_1$};


\draw (0,0) -- (1.5,-.375) -- (2,.625);
\node[below] at (.5,-0.25) {$\lambda_1$};
\node[below] at (2,-0.25) {$\lambda_2$};
\end{tikzpicture}

$\NP (A_i)$:
\[\NP(A_i)=-d \lambda_0+d \slp( \lambda_0). \]

\begin{tikzpicture}
\draw[->] (0,-1) -- (0,2);


\draw[->] (-0.05,0) -- (2,0);

\node[left] at (0,0) {$0$};

\draw (1.5,-.1) -- (1.5,.1);
\node[above] at (1.5,0) {$d$};


\node[left] at (0,1.5) {$-d \lambda_0$};

\draw (0,1.5)-- (1.5,0) ;
\node[below] at (.75,0.5) {$\lambda_0$};
\end{tikzpicture}
\end{lem}

\subsubsection{Newton polygons inside the loop}

\begin{lem} \label{lem:iteration_inside_the_loop}
\begin{align*}
\NP (R_i), \NP(S_i) & \geq -\lambda_0+2^i(\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0), \\
\NP (1-V_i B_i) & \geq 2^i (\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0),\\
\end{align*}
\end{lem}

We prove the last two lemmas by induction.

\begin{proof}
The case $i=0$ is clear thanks to Theorem \ref{theo:EDivisionNP} and Lemma \ref{lem:initial_NP}.

Now, we assume that for some $i \geq 0$ the formulae defined in the Lemmae \ref{lem_unchanged_NP} and \ref{lem:iteration_inside_the_loop} are satisfied. 
We prove them for $i+1$.

Firstly, thanks to the minoration on $\NP (R_i)$ and the fact that $R_i=A_{i+1}-A_i$, we have $\NP (A_{i+1}= \NP (A_i).$
Since $B_{i+1} = P \div A_{i+1},$ we have $\NP (B_{i+1}) = \NP (B_i).$
We apply \eqref{eqdef:Biminus} to obtain 
\[\NP (B_i-B_{i+1})  \geq 2^i (\lambda_1-\lambda_0)+(\lambda_1-\lambda_0)+(l_1-1) \slp (\lambda_1)+\RS. \]
Then, we can apply \eqref{eqdef:Siminus} to obtain 
\[\NP (S_i-S_{i+1})  \geq -\lambda_0 d+2^i (\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0).\]

As a consequence, we get that $\NP (S_{i+1}) \geq \NP (S_i)$ (pointwise).

Now, we can apply \eqref{eqdef:Viminus} to obtain that 
\[\NP (V_i-V_{i+1}) \geq 2^i (\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0).\]
As a consequence, we get that $\NP(V_i)=\NP(V_{i+1}).$
Similarly, we use \eqref{eqdef:Qiminus} to obtain that
\[\NP (1-Q_i)  \geq  2^i (\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0).\]

This is enough to conclude on the Newton polygon of $R_{i+1}.$
Indeed, thanks to \eqref{eq:Riplus}, we obtain that 
\[\NP (R_i) \geq -\lambda_0+2^i(\lambda_1-\lambda_0)+(d-1) \slp (\lambda_0). \]
We can proceed similarly for $1-V_{i+1} B_{i+1}$ using \eqref{eq:ViBiplus}.

Finally, we can apply \eqref{eqdef:Si2} for $i+1,$
\[S_{i+1} = (B_{i+1} R_{i+1} + (1-V_{i+1} B_{i+1}) S_{i}+(1-V_{i+1} B_{i+1})(S_{i+1}-S_{i})) \% A_{i+1},\] to obtain the desired minoration on $\NP (S_{i+1}).$ This concludes the proof.
\end{proof}

\end{document}
